"use strict";(self.webpackChunkdocusaurus_template_openapi_docs=self.webpackChunkdocusaurus_template_openapi_docs||[]).push([[1007],{28453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>l});var i=t(96540);const a={},r=i.createContext(a);function s(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),i.createElement(r.Provider,{value:n},e.children)}},64911:(e,n,t)=>{t.d(n,{A:()=>m});var i=t(96540),a=t(34164),r=t(65627),s=t(77448),l=t(9136);const o={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var c=t(74848);function d({className:e,block:n,selectedValue:t,selectValue:i,tabValues:s}){const l=[],{blockElementScrollPositionUntilNextRender:d}=(0,r.a_)(),u=e=>{const n=e.currentTarget,a=l.indexOf(n),r=s[a].value;r!==t&&(d(n),i(r))},h=e=>{let n=null;switch(e.key){case"Enter":u(e);break;case"ArrowRight":{const t=l.indexOf(e.currentTarget)+1;n=l[t]??l[0];break}case"ArrowLeft":{const t=l.indexOf(e.currentTarget)-1;n=l[t]??l[l.length-1];break}}n?.focus()};return(0,c.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,a.A)("tabs",{"tabs--block":n},e),children:s.map(({value:e,label:n,attributes:i})=>(0,c.jsx)("li",{role:"tab",tabIndex:t===e?0:-1,"aria-selected":t===e,ref:e=>{l.push(e)},onKeyDown:h,onClick:u,...i,className:(0,a.A)("tabs__item",o.tabItem,i?.className,{"tabs__item--active":t===e}),children:n??e},e))})}function u({lazy:e,children:n,selectedValue:t}){const r=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=r.find(e=>e.props.value===t);return e?(0,i.cloneElement)(e,{className:(0,a.A)("margin-top--md",e.props.className)}):null}return(0,c.jsx)("div",{className:"margin-top--md",children:r.map((e,n)=>(0,i.cloneElement)(e,{key:n,hidden:e.props.value!==t}))})}function h(e){const n=(0,s.u)(e);return(0,c.jsxs)("div",{className:(0,a.A)("tabs-container",o.tabList),children:[(0,c.jsx)(d,{...n,...e}),(0,c.jsx)(u,{...n,...e})]})}function m(e){const n=(0,l.default)();return(0,c.jsx)(h,{...e,children:(0,s.v)(e.children)},String(n))}},77448:(e,n,t)=>{t.d(n,{u:()=>m,v:()=>c});var i=t(96540),a=t(56347),r=t(50372),s=t(30604),l=t(78749),o=t(11861);function c(e){return i.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,i.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function d(e){const{values:n,children:t}=e;return(0,i.useMemo)(()=>{const e=n??function(e){return c(e).map(({props:{value:e,label:n,attributes:t,default:i}})=>({value:e,label:n,attributes:t,default:i}))}(t);return function(e){const n=(0,o.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,t])}function u({value:e,tabValues:n}){return n.some(n=>n.value===e)}function h({queryString:e=!1,groupId:n}){const t=(0,a.W6)(),r=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,s.aZ)(r),(0,i.useCallback)(e=>{if(!r)return;const n=new URLSearchParams(t.location.search);n.set(r,e),t.replace({...t.location,search:n.toString()})},[r,t])]}function m(e){const{defaultValue:n,queryString:t=!1,groupId:a}=e,s=d(e),[o,c]=(0,i.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!u({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const t=n.find(e=>e.default)??n[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:n,tabValues:s})),[m,p]=h({queryString:t,groupId:a}),[g,f]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[t,a]=(0,l.Dv)(n);return[t,(0,i.useCallback)(e=>{n&&a.set(e)},[n,a])]}({groupId:a}),b=(()=>{const e=m??g;return u({value:e,tabValues:s})?e:null})();(0,r.A)(()=>{b&&c(b)},[b]);return{selectedValue:o,selectValue:(0,i.useCallback)(e=>{if(!u({value:e,tabValues:s}))throw new Error(`Can't select invalid tab value=${e}`);c(e),p(e),f(e)},[p,f,s]),tabValues:s}}},79329:(e,n,t)=>{t.r(n),t.d(n,{default:()=>s});t(96540);var i=t(34164);const a={tabItem:"tabItem_Ymn6"};var r=t(74848);function s({children:e,hidden:n,className:t}){return(0,r.jsx)("div",{role:"tabpanel",className:(0,i.A)(a.tabItem,t),hidden:n,children:e})}},89844:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>c,default:()=>m,frontMatter:()=>o,metadata:()=>i,toc:()=>u});const i=JSON.parse('{"id":"distributions/building-distro","title":"Build Your Own Distribution","description":"Step-by-step guide to create custom Llama Stack distributions with your choice of API providers","source":"@site/docs/distributions/building-distro.mdx","sourceDirName":"distributions","slug":"/distributions/building-distro","permalink":"/llama-stack/docs/distributions/building-distro","draft":false,"unlisted":false,"editUrl":"https://github.com/meta-llama/llama-stack/tree/main/docs/docs/distributions/building-distro.mdx","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Build Your Own Distribution","description":"Step-by-step guide to create custom Llama Stack distributions with your choice of API providers","sidebar_label":"Building Custom Distributions","sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Available Distributions","permalink":"/llama-stack/docs/distributions/list-of-distributions"},"next":{"title":"Customizing run.yaml","permalink":"/llama-stack/docs/distributions/customizing-run-yaml"}}');var a=t(74848),r=t(28453),s=t(64911),l=t(79329);const o={title:"Build Your Own Distribution",description:"Step-by-step guide to create custom Llama Stack distributions with your choice of API providers",sidebar_label:"Building Custom Distributions",sidebar_position:3},c="Build Your Own Distribution",d={},u=[{value:"Setting Your Log Level",id:"setting-your-log-level",level:2},{value:"Llama Stack Build",id:"llama-stack-build",level:2},{value:"Build Methods",id:"build-methods",level:2},{value:"Running Your Stack Server",id:"running-your-stack-server",level:2},{value:"Managing Distributions",id:"managing-distributions",level:2},{value:"Listing Distributions",id:"listing-distributions",level:3},{value:"Removing a Distribution",id:"removing-a-distribution",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2}];function h(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"build-your-own-distribution",children:"Build Your Own Distribution"})}),"\n",(0,a.jsx)(n.p,{children:"This guide will walk you through the steps to get started with building a Llama Stack distribution from scratch with your choice of API providers."}),"\n",(0,a.jsx)(n.h2,{id:"setting-your-log-level",children:"Setting Your Log Level"}),"\n",(0,a.jsxs)(n.p,{children:["In order to specify the proper logging level users can apply the following environment variable ",(0,a.jsx)(n.code,{children:"LLAMA_STACK_LOGGING"})," with the following format:"]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.code,{children:"LLAMA_STACK_LOGGING=server=debug;core=info"})}),"\n",(0,a.jsx)(n.p,{children:"Where each category in the following list:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"all"}),"\n",(0,a.jsx)(n.li,{children:"core"}),"\n",(0,a.jsx)(n.li,{children:"server"}),"\n",(0,a.jsx)(n.li,{children:"router"}),"\n",(0,a.jsx)(n.li,{children:"inference"}),"\n",(0,a.jsx)(n.li,{children:"agents"}),"\n",(0,a.jsx)(n.li,{children:"safety"}),"\n",(0,a.jsx)(n.li,{children:"eval"}),"\n",(0,a.jsx)(n.li,{children:"tools"}),"\n",(0,a.jsx)(n.li,{children:"client"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Can be set to any of the following log levels:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"debug"}),"\n",(0,a.jsx)(n.li,{children:"info"}),"\n",(0,a.jsx)(n.li,{children:"warning"}),"\n",(0,a.jsx)(n.li,{children:"error"}),"\n",(0,a.jsx)(n.li,{children:"critical"}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["The default global log level is ",(0,a.jsx)(n.code,{children:"info"}),". ",(0,a.jsx)(n.code,{children:"all"})," sets the log level for all components."]}),"\n",(0,a.jsxs)(n.p,{children:["A user can also set ",(0,a.jsx)(n.code,{children:"LLAMA_STACK_LOG_FILE"})," which will pipe the logs to the specified path as well as to the terminal. An example would be: ",(0,a.jsx)(n.code,{children:"export LLAMA_STACK_LOG_FILE=server.log"})]}),"\n",(0,a.jsx)(n.h2,{id:"llama-stack-build",children:"Llama Stack Build"}),"\n",(0,a.jsxs)(n.p,{children:["In order to build your own distribution, we recommend you clone the ",(0,a.jsx)(n.code,{children:"llama-stack"})," repository."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"git clone git@github.com:meta-llama/llama-stack.git\ncd llama-stack\npip install -e .\n"})}),"\n",(0,a.jsx)(n.p,{children:"Use the CLI to build your distribution. The main points to consider are:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Image Type"})," - Do you want a venv environment or a Container (eg. Docker)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Template"})," - Do you want to use a template to build your distribution? or start from scratch ?"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Config"})," - Do you want to use a pre-existing config file to build your distribution?"]}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"llama stack build -h\nusage: llama stack build [-h] [--config CONFIG] [--template TEMPLATE] [--distro DISTRIBUTION] [--list-distros] [--image-type {container,venv}] [--image-name IMAGE_NAME] [--print-deps-only]\n                         [--run] [--providers PROVIDERS]\n\nBuild a Llama stack container\n\noptions:\n  -h, --help            show this help message and exit\n  --config CONFIG       Path to a config file to use for the build. You can find example configs in llama_stack.cores/**/build.yaml. If this argument is not provided, you will be prompted to\n                        enter information interactively (default: None)\n  --template TEMPLATE   (deprecated) Name of the example template config to use for build. You may use `llama stack build --list-distros` to check out the available distributions (default:\n                        None)\n  --distro DISTRIBUTION, --distribution DISTRIBUTION\n                        Name of the distribution to use for build. You may use `llama stack build --list-distros` to check out the available distributions (default: None)\n  --list-distros, --list-distributions\n                        Show the available distributions for building a Llama Stack distribution (default: False)\n  --image-type {container,venv}\n                        Image Type to use for the build. If not specified, will use the image type from the template config. (default: None)\n  --image-name IMAGE_NAME\n                        [for image-type=container|venv] Name of the virtual environment to use for the build. If not specified, currently active environment will be used if found. (default:\n                        None)\n  --print-deps-only     Print the dependencies for the stack only, without building the stack (default: False)\n  --run                 Run the stack after building using the same image type, name, and other applicable arguments (default: False)\n  --providers PROVIDERS\n                        Build a config for a list of providers and only those providers. This list is formatted like: api1=provider1,api2=provider2. Where there can be multiple providers per\n                        API. (default: None)\n"})}),"\n",(0,a.jsxs)(n.p,{children:["After this step is complete, a file named ",(0,a.jsx)(n.code,{children:"<name>-build.yaml"})," and template file ",(0,a.jsx)(n.code,{children:"<name>-run.yaml"})," will be generated and saved at the output file path specified at the end of the command."]}),"\n",(0,a.jsx)(n.h2,{id:"build-methods",children:"Build Methods"}),"\n",(0,a.jsxs)(s.A,{children:[(0,a.jsxs)(l.default,{value:"template",label:"Building from a template",children:[(0,a.jsx)(n.p,{children:"To build from alternative API providers, we provide distribution templates for users to get started building a distribution backed by different providers."}),(0,a.jsx)(n.p,{children:"The following command will allow you to see the available templates and their corresponding providers."}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"llama stack build --list-templates\n"})}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"------------------------------+-----------------------------------------------------------------------------+\n| Template Name                | Description                                                                 |\n+------------------------------+-----------------------------------------------------------------------------+\n| watsonx                      | Use watsonx for running LLM inference                                       |\n+------------------------------+-----------------------------------------------------------------------------+\n| vllm-gpu                     | Use a built-in vLLM engine for running LLM inference                        |\n+------------------------------+-----------------------------------------------------------------------------+\n| together                     | Use Together.AI for running LLM inference                                   |\n+------------------------------+-----------------------------------------------------------------------------+\n| tgi                          | Use (an external) TGI server for running LLM inference                      |\n+------------------------------+-----------------------------------------------------------------------------+\n| starter                      | Quick start template for running Llama Stack with several popular providers |\n+------------------------------+-----------------------------------------------------------------------------+\n| sambanova                    | Use SambaNova for running LLM inference and safety                          |\n+------------------------------+-----------------------------------------------------------------------------+\n| remote-vllm                  | Use (an external) vLLM server for running LLM inference                     |\n+------------------------------+-----------------------------------------------------------------------------+\n| postgres-demo                | Quick start template for running Llama Stack with several popular providers |\n+------------------------------+-----------------------------------------------------------------------------+\n| passthrough                  | Use Passthrough hosted llama-stack endpoint for LLM inference               |\n+------------------------------+-----------------------------------------------------------------------------+\n| open-benchmark               | Distribution for running open benchmarks                                    |\n+------------------------------+-----------------------------------------------------------------------------+\n| ollama                       | Use (an external) Ollama server for running LLM inference                   |\n+------------------------------+-----------------------------------------------------------------------------+\n| nvidia                       | Use NVIDIA NIM for running LLM inference, evaluation and safety             |\n+------------------------------+-----------------------------------------------------------------------------+\n| meta-reference-gpu           | Use Meta Reference for running LLM inference                                |\n+------------------------------+-----------------------------------------------------------------------------+\n| llama_api                    | Distribution for running e2e tests in CI                                    |\n+------------------------------+-----------------------------------------------------------------------------+\n| hf-serverless                | Use (an external) Hugging Face Inference Endpoint for running LLM inference |\n+------------------------------+-----------------------------------------------------------------------------+\n| hf-endpoint                  | Use (an external) Hugging Face Inference Endpoint for running LLM inference |\n+------------------------------+-----------------------------------------------------------------------------+\n| groq                         | Use Groq for running LLM inference                                          |\n+------------------------------+-----------------------------------------------------------------------------+\n| fireworks                    | Use Fireworks.AI for running LLM inference                                  |\n+------------------------------+-----------------------------------------------------------------------------+\n| experimental-post-training   | Experimental template for post training                                     |\n+------------------------------+-----------------------------------------------------------------------------+\n| dell                         | Dell's distribution of Llama Stack. TGI inference via Dell's custom         |\n|                              | container                                                                   |\n+------------------------------+-----------------------------------------------------------------------------+\n| ci-tests                     | Distribution for running e2e tests in CI                                    |\n+------------------------------+-----------------------------------------------------------------------------+\n| cerebras                     | Use Cerebras for running LLM inference                                      |\n+------------------------------+-----------------------------------------------------------------------------+\n| bedrock                      | Use AWS Bedrock for running LLM inference and safety                        |\n+------------------------------+-----------------------------------------------------------------------------+\n"})}),(0,a.jsx)(n.p,{children:"You may then pick a template to build your distribution with providers fitted to your liking."}),(0,a.jsx)(n.p,{children:"For example, to build a distribution with TGI as the inference provider, you can run:"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"$ llama stack build --distro starter\n...\nYou can now edit ~/.llama/distributions/llamastack-starter/starter-run.yaml and run `llama stack run ~/.llama/distributions/llamastack-starter/starter-run.yaml`\n"})}),(0,a.jsx)(n.admonition,{type:"tip",children:(0,a.jsxs)(n.p,{children:["The generated ",(0,a.jsx)(n.code,{children:"run.yaml"})," file is a starting point for your configuration. For comprehensive guidance on customizing it for your specific needs, infrastructure, and deployment scenarios, see ",(0,a.jsx)(n.a,{href:"./customizing-run-yaml",children:"Customizing Your run.yaml Configuration"}),"."]})})]}),(0,a.jsxs)(l.default,{value:"scratch",label:"Building from Scratch",children:[(0,a.jsxs)(n.p,{children:["If the provided templates do not fit your use case, you could start off with running ",(0,a.jsx)(n.code,{children:"llama stack build"})," which will allow you to a interactively enter wizard where you will be prompted to enter build configurations."]}),(0,a.jsx)(n.p,{children:"It would be best to start with a template and understand the structure of the config file and the various concepts ( APIS, providers, resources, etc.) before starting from scratch."}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"llama stack build\n\n> Enter a name for your Llama Stack (e.g. my-local-stack): my-stack\n> Enter the image type you want your Llama Stack to be built as (container or venv): venv\n\nLlama Stack is composed of several APIs working together. Let's select\nthe provider types (implementations) you want to use for these APIs.\n\nTip: use <TAB> to see options for the providers.\n\n> Enter provider for API inference: inline::meta-reference\n> Enter provider for API safety: inline::llama-guard\n> Enter provider for API agents: inline::meta-reference\n> Enter provider for API memory: inline::faiss\n> Enter provider for API datasetio: inline::meta-reference\n> Enter provider for API scoring: inline::meta-reference\n> Enter provider for API eval: inline::meta-reference\n> Enter provider for API telemetry: inline::meta-reference\n\n > (Optional) Enter a short description for your Llama Stack:\n\nYou can now edit ~/.llama/distributions/llamastack-my-local-stack/my-local-stack-run.yaml and run `llama stack run ~/.llama/distributions/llamastack-my-local-stack/my-local-stack-run.yaml`\n"})})]}),(0,a.jsxs)(l.default,{value:"config",label:"Building from a pre-existing build config file",children:[(0,a.jsx)(n.p,{children:"In addition to templates, you may customize the build to your liking through editing config files and build from config files with the following command."}),(0,a.jsxs)(n.p,{children:["The config file will be of contents like the ones in ",(0,a.jsx)(n.code,{children:"llama_stack/distributions/*build.yaml"}),"."]}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"llama stack build --config llama_stack/distributions/starter/build.yaml\n"})})]}),(0,a.jsxs)(l.default,{value:"external",label:"Building with External Providers",children:[(0,a.jsx)(n.p,{children:"Llama Stack supports external providers that live outside of the main codebase. This allows you to create and maintain your own providers independently or use community-provided providers."}),(0,a.jsx)(n.p,{children:"To build a distribution with external providers, you need to:"}),(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["Configure the ",(0,a.jsx)(n.code,{children:"external_providers_dir"})," in your build configuration file:"]}),"\n"]}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:"# Example my-external-stack.yaml with external providers\nversion: '2'\ndistribution_spec:\n  description: Custom distro for CI tests\n  providers:\n    inference:\n    - remote::custom_ollama\n# Add more providers as needed\nimage_type: container\nimage_name: ci-test\n# Path to external provider implementations\nexternal_providers_dir: ~/.llama/providers.d\n"})}),(0,a.jsx)(n.p,{children:"Here's an example for a custom Ollama provider:"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:"adapter:\n  adapter_type: custom_ollama\n  pip_packages:\n  - ollama\n  - aiohttp\n  - llama-stack-provider-ollama # This is the provider package\n  config_class: llama_stack_ollama_provider.config.OllamaImplConfig\n  module: llama_stack_ollama_provider\napi_dependencies: []\noptional_api_dependencies: []\n"})}),(0,a.jsxs)(n.p,{children:["The ",(0,a.jsx)(n.code,{children:"pip_packages"})," section lists the Python packages required by the provider, as well as the\nprovider package itself. The package must be available on PyPI or can be provided from a local\ndirectory or a git repository (git must be installed on the build environment)."]}),(0,a.jsxs)(n.ol,{start:"2",children:["\n",(0,a.jsx)(n.li,{children:"Build your distribution using the config file:"}),"\n"]}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"llama stack build --config my-external-stack.yaml\n"})}),(0,a.jsxs)(n.p,{children:["For more information on external providers, including directory structure, provider types, and implementation requirements, see the ",(0,a.jsx)(n.a,{href:"/docs/providers/external/external-providers-guide",children:"External Providers documentation"}),"."]})]}),(0,a.jsxs)(l.default,{value:"container",label:"Building Container",children:[(0,a.jsx)(n.admonition,{title:"Podman Alternative",type:"tip",children:(0,a.jsxs)(n.p,{children:["Podman is supported as an alternative to Docker. Set ",(0,a.jsx)(n.code,{children:"CONTAINER_BINARY"})," to ",(0,a.jsx)(n.code,{children:"podman"})," in your environment to use Podman."]})}),(0,a.jsxs)(n.p,{children:["To build a container image, you may start off from a template and use the ",(0,a.jsx)(n.code,{children:"--image-type container"})," flag to specify ",(0,a.jsx)(n.code,{children:"container"})," as the build image type."]}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"llama stack build --distro starter --image-type container\n"})}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"$ llama stack build --distro starter --image-type container\n...\nContainerfile created successfully in /tmp/tmp.viA3a3Rdsg/ContainerfileFROM python:3.10-slim\n...\n"})}),(0,a.jsxs)(n.p,{children:["You can now edit ~/meta-llama/llama-stack/tmp/configs/ollama-run.yaml and run ",(0,a.jsx)(n.code,{children:"llama stack run ~/meta-llama/llama-stack/tmp/configs/ollama-run.yaml"})]}),(0,a.jsx)(n.p,{children:"Now set some environment variables for the inference model ID and Llama Stack Port and create a local directory to mount into the container's file system."}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'export INFERENCE_MODEL="llama3.2:3b"\nexport LLAMA_STACK_PORT=8321\nmkdir -p ~/.llama\n'})}),(0,a.jsx)(n.p,{children:"After this step is successful, you should be able to find the built container image and test it with the below Docker command:"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"docker run -d \\\n  -p $LLAMA_STACK_PORT:$LLAMA_STACK_PORT \\\n  -v ~/.llama:/root/.llama \\\n  localhost/distribution-ollama:dev \\\n  --port $LLAMA_STACK_PORT \\\n  --env INFERENCE_MODEL=$INFERENCE_MODEL \\\n  --env OLLAMA_URL=http://host.docker.internal:11434\n"})}),(0,a.jsx)(n.p,{children:"Here are the docker flags and their uses:"}),(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"-d"}),": Runs the container in the detached mode as a background process"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"-p $LLAMA_STACK_PORT:$LLAMA_STACK_PORT"}),": Maps the container port to the host port for accessing the server"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"-v ~/.llama:/root/.llama"}),": Mounts the local .llama directory to persist configurations and data"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"localhost/distribution-ollama:dev"}),": The name and tag of the container image to run"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"--port $LLAMA_STACK_PORT"}),": Port number for the server to listen on"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"--env INFERENCE_MODEL=$INFERENCE_MODEL"}),": Sets the model to use for inference"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"--env OLLAMA_URL=http://host.docker.internal:11434"}),": Configures the URL for the Ollama service"]}),"\n"]})]})]}),"\n",(0,a.jsx)(n.h2,{id:"running-your-stack-server",children:"Running Your Stack Server"}),"\n",(0,a.jsxs)(n.p,{children:["Now, let's start the Llama Stack Distribution Server. You will need the YAML configuration file which was written out at the end by the ",(0,a.jsx)(n.code,{children:"llama stack build"})," step."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"llama stack run -h\nusage: llama stack run [-h] [--port PORT] [--image-name IMAGE_NAME] [--env KEY=VALUE]\n                       [--image-type {venv}] [--enable-ui]\n                       [config | template]\n\nStart the server for a Llama Stack Distribution. You should have already built (or downloaded) and configured the distribution.\n\npositional arguments:\n  config | template     Path to config file to use for the run or name of known template (`llama stack list` for a list). (default: None)\n\noptions:\n  -h, --help            show this help message and exit\n  --port PORT           Port to run the server on. It can also be passed via the env var LLAMA_STACK_PORT. (default: 8321)\n  --image-name IMAGE_NAME\n                        Name of the image to run. Defaults to the current environment (default: None)\n  --env KEY=VALUE       Environment variables to pass to the server in KEY=VALUE format. Can be specified multiple times. (default: None)\n  --image-type {venv}\n                        Image Type used during the build. This should be venv. (default: None)\n  --enable-ui           Start the UI server (default: False)\n"})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Note:"})," Container images built with ",(0,a.jsx)(n.code,{children:"llama stack build --image-type container"})," cannot be run using ",(0,a.jsx)(n.code,{children:"llama stack run"}),". Instead, they must be run directly using Docker or Podman commands as shown in the container building section above."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Start using template name\nllama stack run tgi\n\n# Start using config file\nllama stack run ~/.llama/distributions/llamastack-my-local-stack/my-local-stack-run.yaml\n\n# Start using a venv\nllama stack run --image-type venv ~/.llama/distributions/llamastack-my-local-stack/my-local-stack-run.yaml\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"$ llama stack run ~/.llama/distributions/llamastack-my-local-stack/my-local-stack-run.yaml\n\nServing API inspect\n GET /health\n GET /providers/list\n GET /routes/list\nServing API inference\n POST /inference/chat_completion\n POST /inference/completion\n POST /inference/embeddings\n...\nServing API agents\n POST /agents/create\n POST /agents/session/create\n POST /agents/turn/create\n POST /agents/delete\n POST /agents/session/delete\n POST /agents/session/get\n POST /agents/step/get\n POST /agents/turn/get\n\nListening on ['::', '0.0.0.0']:8321\nINFO:     Started server process [2935911]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://['::', '0.0.0.0']:8321 (Press CTRL+C to quit)\nINFO:     2401:db00:35c:2d2b:face:0:c9:0:54678 - \"GET /models/list HTTP/1.1\" 200 OK\n"})}),"\n",(0,a.jsx)(n.h2,{id:"managing-distributions",children:"Managing Distributions"}),"\n",(0,a.jsx)(n.h3,{id:"listing-distributions",children:"Listing Distributions"}),"\n",(0,a.jsx)(n.p,{children:"Using the list command, you can view all existing Llama Stack distributions, including stacks built from templates, from scratch, or using custom configuration files."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"llama stack list -h\nusage: llama stack list [-h]\n\nlist the build stacks\n\noptions:\n  -h, --help  show this help message and exit\n"})}),"\n",(0,a.jsx)(n.p,{children:"Example Usage:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"llama stack list\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"------------------------------+-----------------------------------------------------------------+--------------+------------+\n| Stack Name                  | Path                                                            | Build Config | Run Config |\n+------------------------------+-----------------------------------------------------------------------------+--------------+\n| together                    | ~/.llama/distributions/together                                 | Yes          | No         |\n+------------------------------+-----------------------------------------------------------------------------+--------------+\n| bedrock                     | ~/.llama/distributions/bedrock                                  | Yes          | No         |\n+------------------------------+-----------------------------------------------------------------------------+--------------+\n| starter                     | ~/.llama/distributions/starter                                  | Yes          | Yes        |\n+------------------------------+-----------------------------------------------------------------------------+--------------+\n| remote-vllm                 | ~/.llama/distributions/remote-vllm                              | Yes          | Yes        |\n+------------------------------+-----------------------------------------------------------------------------+--------------+\n"})}),"\n",(0,a.jsx)(n.h3,{id:"removing-a-distribution",children:"Removing a Distribution"}),"\n",(0,a.jsx)(n.p,{children:"Use the remove command to delete a distribution you've previously built."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"llama stack rm -h\nusage: llama stack rm [-h] [--all] [name]\n\nRemove the build stack\n\npositional arguments:\n  name        Name of the stack to delete (default: None)\n\noptions:\n  -h, --help  show this help message and exit\n  --all, -a   Delete all stacks (use with caution) (default: False)\n"})}),"\n",(0,a.jsx)(n.p,{children:"Example:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"llama stack rm llamastack-test\n"})}),"\n",(0,a.jsxs)(n.p,{children:["To keep your environment organized and avoid clutter, consider using ",(0,a.jsx)(n.code,{children:"llama stack list"})," to review old or unused distributions and ",(0,a.jsx)(n.code,{children:"llama stack rm <name>"})," to delete them when they're no longer needed."]}),"\n",(0,a.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,a.jsxs)(n.p,{children:["If you encounter any issues, ask questions in our discord or search through our ",(0,a.jsx)(n.a,{href:"https://github.com/meta-llama/llama-stack/issues",children:"GitHub Issues"}),", or file an new issue."]})]})}function m(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(h,{...e})}):h(e)}}}]);