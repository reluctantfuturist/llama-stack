"use strict";(self.webpackChunkdocusaurus_template_openapi_docs=self.webpackChunkdocusaurus_template_openapi_docs||[]).push([[7951],{28453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>l});var s=i(96540);const a={},r=s.createContext(a);function t(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:t(e.components),s.createElement(r.Provider,{value:n},e.children)}},32172:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>u,frontMatter:()=>t,metadata:()=>s,toc:()=>o});const s=JSON.parse('{"id":"advanced-apis/scoring","title":"Scoring","description":"The Scoring API in Llama Stack allows you to evaluate outputs of your GenAI system using various scoring functions and metrics. This section covers all available scoring providers and their configuration.","source":"@site/docs/advanced-apis/scoring.mdx","sourceDirName":"advanced-apis","slug":"/advanced-apis/scoring","permalink":"/llama-stack/docs/advanced-apis/scoring","draft":false,"unlisted":false,"editUrl":"https://github.com/meta-llama/llama-stack/tree/main/docs/docs/advanced-apis/scoring.mdx","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Evaluation","permalink":"/llama-stack/docs/advanced-apis/evaluation"},"next":{"title":"Overview","permalink":"/llama-stack/docs/deploying/"}}');var a=i(74848),r=i(28453);const t={},l="Scoring",c={},o=[{value:"Overview",id:"overview",level:2},{value:"Basic Scoring",id:"basic-scoring",level:2},{value:"Configuration",id:"configuration",level:3},{value:"Features",id:"features",level:3},{value:"Use Cases",id:"use-cases",level:3},{value:"Braintrust",id:"braintrust",level:2},{value:"Configuration",id:"configuration-1",level:3},{value:"Sample Configuration",id:"sample-configuration",level:3},{value:"Features",id:"features-1",level:3},{value:"Use Cases",id:"use-cases-1",level:3},{value:"LLM-as-Judge",id:"llm-as-judge",level:2},{value:"Configuration",id:"configuration-2",level:3},{value:"Features",id:"features-2",level:3},{value:"Use Cases",id:"use-cases-2",level:3},{value:"Usage Examples",id:"usage-examples",level:2},{value:"Basic Scoring Example",id:"basic-scoring-example",level:3},{value:"LLM-as-Judge Example",id:"llm-as-judge-example",level:3},{value:"Braintrust Integration Example",id:"braintrust-integration-example",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Integration with Evaluation",id:"integration-with-evaluation",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"scoring",children:"Scoring"})}),"\n",(0,a.jsx)(n.p,{children:"The Scoring API in Llama Stack allows you to evaluate outputs of your GenAI system using various scoring functions and metrics. This section covers all available scoring providers and their configuration."}),"\n",(0,a.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsx)(n.p,{children:"Llama Stack provides multiple scoring providers:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Basic"})," (",(0,a.jsx)(n.code,{children:"inline::basic"}),") - Simple evaluation metrics and scoring functions"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Braintrust"})," (",(0,a.jsx)(n.code,{children:"inline::braintrust"}),") - Advanced evaluation using the Braintrust platform"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"LLM-as-Judge"})," (",(0,a.jsx)(n.code,{children:"inline::llm-as-judge"}),") - Uses language models to evaluate responses"]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["The Scoring API is associated with ",(0,a.jsx)(n.code,{children:"ScoringFunction"})," resources and provides a suite of out-of-the-box scoring functions. You can also add custom evaluators to meet specific evaluation needs."]}),"\n",(0,a.jsx)(n.h2,{id:"basic-scoring",children:"Basic Scoring"}),"\n",(0,a.jsx)(n.p,{children:"Basic scoring provider for simple evaluation metrics and scoring functions. This provider offers fundamental scoring capabilities without external dependencies."}),"\n",(0,a.jsx)(n.h3,{id:"configuration",children:"Configuration"}),"\n",(0,a.jsx)(n.p,{children:"No configuration required - this provider works out of the box."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:"{}\n"})}),"\n",(0,a.jsx)(n.h3,{id:"features",children:"Features"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Simple evaluation metrics (accuracy, precision, recall, F1-score)"}),"\n",(0,a.jsx)(n.li,{children:"String matching and similarity metrics"}),"\n",(0,a.jsx)(n.li,{children:"Basic statistical scoring functions"}),"\n",(0,a.jsx)(n.li,{children:"No external dependencies required"}),"\n",(0,a.jsx)(n.li,{children:"Fast execution for standard metrics"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"use-cases",children:"Use Cases"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Quick evaluation of basic accuracy metrics"}),"\n",(0,a.jsx)(n.li,{children:"String similarity comparisons"}),"\n",(0,a.jsx)(n.li,{children:"Statistical analysis of model outputs"}),"\n",(0,a.jsx)(n.li,{children:"Development and testing scenarios"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"braintrust",children:"Braintrust"}),"\n",(0,a.jsxs)(n.p,{children:["Braintrust scoring provider for evaluation and scoring using the ",(0,a.jsx)(n.a,{href:"https://braintrustdata.com/",children:"Braintrust platform"}),". Braintrust provides advanced evaluation capabilities and experiment tracking."]}),"\n",(0,a.jsx)(n.h3,{id:"configuration-1",children:"Configuration"}),"\n",(0,a.jsxs)(n.table,{children:[(0,a.jsx)(n.thead,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.th,{children:"Field"}),(0,a.jsx)(n.th,{children:"Type"}),(0,a.jsx)(n.th,{children:"Required"}),(0,a.jsx)(n.th,{children:"Default"}),(0,a.jsx)(n.th,{children:"Description"})]})}),(0,a.jsx)(n.tbody,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"openai_api_key"})}),(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"str | None"})}),(0,a.jsx)(n.td,{children:"No"}),(0,a.jsx)(n.td,{}),(0,a.jsx)(n.td,{children:"The OpenAI API Key for LLM-powered evaluations"})]})})]}),"\n",(0,a.jsx)(n.h3,{id:"sample-configuration",children:"Sample Configuration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:"openai_api_key: ${env.OPENAI_API_KEY:=}\n"})}),"\n",(0,a.jsx)(n.h3,{id:"features-1",children:"Features"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Advanced evaluation metrics"}),"\n",(0,a.jsx)(n.li,{children:"Experiment tracking and comparison"}),"\n",(0,a.jsx)(n.li,{children:"LLM-powered evaluation functions"}),"\n",(0,a.jsx)(n.li,{children:"Integration with Braintrust's evaluation suite"}),"\n",(0,a.jsx)(n.li,{children:"Detailed scoring analytics and insights"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"use-cases-1",children:"Use Cases"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Production evaluation pipelines"}),"\n",(0,a.jsx)(n.li,{children:"A/B testing of model versions"}),"\n",(0,a.jsx)(n.li,{children:"Advanced scoring with custom metrics"}),"\n",(0,a.jsx)(n.li,{children:"Detailed evaluation reporting and analysis"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"llm-as-judge",children:"LLM-as-Judge"}),"\n",(0,a.jsx)(n.p,{children:"LLM-as-judge scoring provider that uses language models to evaluate and score responses. This approach leverages the reasoning capabilities of large language models to assess quality, relevance, and other subjective metrics."}),"\n",(0,a.jsx)(n.h3,{id:"configuration-2",children:"Configuration"}),"\n",(0,a.jsx)(n.p,{children:"No configuration required - this provider works out of the box."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:"{}\n"})}),"\n",(0,a.jsx)(n.h3,{id:"features-2",children:"Features"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Subjective quality evaluation using LLMs"}),"\n",(0,a.jsx)(n.li,{children:"Flexible evaluation criteria definition"}),"\n",(0,a.jsx)(n.li,{children:"Natural language evaluation explanations"}),"\n",(0,a.jsx)(n.li,{children:"Support for complex evaluation scenarios"}),"\n",(0,a.jsx)(n.li,{children:"Contextual understanding of responses"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"use-cases-2",children:"Use Cases"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Evaluating response quality and relevance"}),"\n",(0,a.jsx)(n.li,{children:"Assessing creativity and coherence"}),"\n",(0,a.jsx)(n.li,{children:"Subjective metric evaluation"}),"\n",(0,a.jsx)(n.li,{children:"Human-like judgment for complex tasks"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"usage-examples",children:"Usage Examples"}),"\n",(0,a.jsx)(n.h3,{id:"basic-scoring-example",children:"Basic Scoring Example"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from llama_stack_client import LlamaStackClient\n\nclient = LlamaStackClient(base_url="http://localhost:8321")\n\n# Register a basic accuracy scoring function\nclient.scoring_functions.register(\n    scoring_function_id="basic_accuracy",\n    provider_id="basic",\n    provider_scoring_function_id="accuracy"\n)\n\n# Use the scoring function\nresult = client.scoring.score(\n    input_rows=[\n        {"expected": "Paris", "actual": "Paris"},\n        {"expected": "London", "actual": "Paris"}\n    ],\n    scoring_function_id="basic_accuracy"\n)\nprint(f"Accuracy: {result.results[0].score}")\n'})}),"\n",(0,a.jsx)(n.h3,{id:"llm-as-judge-example",children:"LLM-as-Judge Example"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Register an LLM-as-judge scoring function\nclient.scoring_functions.register(\n    scoring_function_id="quality_judge",\n    provider_id="llm_judge",\n    provider_scoring_function_id="response_quality",\n    params={\n        "criteria": "Evaluate response quality, relevance, and helpfulness",\n        "scale": "1-10"\n    }\n)\n\n# Score responses using LLM judgment\nresult = client.scoring.score(\n    input_rows=[{\n        "query": "What is machine learning?",\n        "response": "Machine learning is a subset of AI that enables computers to learn patterns from data..."\n    }],\n    scoring_function_id="quality_judge"\n)\n'})}),"\n",(0,a.jsx)(n.h3,{id:"braintrust-integration-example",children:"Braintrust Integration Example"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Register a Braintrust scoring function\nclient.scoring_functions.register(\n    scoring_function_id="braintrust_eval",\n    provider_id="braintrust",\n    provider_scoring_function_id="semantic_similarity"\n)\n\n# Run evaluation with Braintrust\nresult = client.scoring.score(\n    input_rows=[{\n        "reference": "The capital of France is Paris",\n        "candidate": "Paris is the capital city of France"\n    }],\n    scoring_function_id="braintrust_eval"\n)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Choose appropriate providers"}),": Use Basic for simple metrics, Braintrust for advanced analytics, LLM-as-Judge for subjective evaluation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Define clear criteria"}),": When using LLM-as-Judge, provide specific evaluation criteria and scales"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Validate scoring functions"}),": Test your scoring functions with known examples before production use"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Monitor performance"}),": Track scoring performance and adjust thresholds based on results"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Combine multiple metrics"}),": Use different scoring providers together for comprehensive evaluation"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"integration-with-evaluation",children:"Integration with Evaluation"}),"\n",(0,a.jsxs)(n.p,{children:["The Scoring API works closely with the ",(0,a.jsx)(n.a,{href:"/llama-stack/docs/advanced-apis/evaluation",children:"Evaluation"})," API to provide comprehensive evaluation workflows:"]}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Datasets"})," are loaded via the DatasetIO API"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Evaluation"})," generates model outputs using the Eval API"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Scoring"})," evaluates the quality of outputs using various scoring functions"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Results"})," are aggregated and reported for analysis"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Check out the ",(0,a.jsx)(n.a,{href:"/llama-stack/docs/advanced-apis/evaluation",children:"Evaluation"})," guide for running complete evaluations"]}),"\n",(0,a.jsxs)(n.li,{children:["See the ",(0,a.jsx)(n.a,{href:"/llama-stack/docs/building-applications/evals",children:"Building Applications - Evaluation"})," guide for application examples"]}),"\n",(0,a.jsxs)(n.li,{children:["Review the ",(0,a.jsx)(n.a,{href:"/llama-stack/docs/references/evals-reference",children:"Evaluation Reference"})," for comprehensive scoring function usage"]}),"\n",(0,a.jsxs)(n.li,{children:["Explore the ",(0,a.jsx)(n.a,{href:"/llama-stack/docs/concepts/evaluation-concepts",children:"Evaluation Concepts"})," for detailed conceptual information"]}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}}}]);