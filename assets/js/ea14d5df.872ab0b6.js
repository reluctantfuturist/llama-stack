"use strict";(self.webpackChunkdocusaurus_template_openapi_docs=self.webpackChunkdocusaurus_template_openapi_docs||[]).push([[7465],{28453:(e,n,a)=>{a.d(n,{R:()=>l,x:()=>r});var i=a(96540);const t={},s=i.createContext(t);function l(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:l(e.components),i.createElement(s.Provider,{value:n},e.children)}},40478:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>o,contentTitle:()=>r,default:()=>h,frontMatter:()=>l,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"advanced-apis/evaluation","title":"Evaluation","description":"The Evaluation API in Llama Stack allows you to run evaluation tasks on your GenAI applications and datasets. This section covers all available evaluation providers and their configuration.","source":"@site/docs/advanced-apis/evaluation.mdx","sourceDirName":"advanced-apis","slug":"/advanced-apis/evaluation","permalink":"/llama-stack/docs/advanced-apis/evaluation","draft":false,"unlisted":false,"editUrl":"https://github.com/meta-llama/llama-stack/tree/main/docs/docs/advanced-apis/evaluation.mdx","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Post-Training","permalink":"/llama-stack/docs/advanced-apis/post-training"},"next":{"title":"Scoring","permalink":"/llama-stack/docs/advanced-apis/scoring"}}');var t=a(74848),s=a(28453);const l={},r="Evaluation",o={},c=[{value:"Overview",id:"overview",level:2},{value:"Meta Reference",id:"meta-reference",level:2},{value:"Configuration",id:"configuration",level:3},{value:"Sample Configuration",id:"sample-configuration",level:3},{value:"Features",id:"features",level:3},{value:"NVIDIA",id:"nvidia",level:2},{value:"Configuration",id:"configuration-1",level:3},{value:"Sample Configuration",id:"sample-configuration-1",level:3},{value:"Features",id:"features-1",level:3},{value:"Usage Example",id:"usage-example",level:2},{value:"Supported Benchmarks",id:"supported-benchmarks",level:2},{value:"Best Practices",id:"best-practices",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"evaluation",children:"Evaluation"})}),"\n",(0,t.jsx)(n.p,{children:"The Evaluation API in Llama Stack allows you to run evaluation tasks on your GenAI applications and datasets. This section covers all available evaluation providers and their configuration."}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(n.p,{children:"Llama Stack provides multiple evaluation providers:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Meta Reference"})," (",(0,t.jsx)(n.code,{children:"inline::meta-reference"}),") - Meta's reference implementation with multi-language support"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"NVIDIA"})," (",(0,t.jsx)(n.code,{children:"remote::nvidia"}),") - NVIDIA's evaluation platform integration"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"The Evaluation API works with several related APIs to provide comprehensive evaluation capabilities:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"/datasetio"})," + ",(0,t.jsx)(n.code,{children:"/datasets"})," API - Interface with datasets and data loaders"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"/scoring"})," + ",(0,t.jsx)(n.code,{children:"/scoring_functions"})," API - Evaluate outputs of the system"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"/eval"})," + ",(0,t.jsx)(n.code,{children:"/benchmarks"})," API - Generate outputs and perform scoring"]}),"\n"]}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsxs)(n.p,{children:["For conceptual information about evaluations, see our ",(0,t.jsx)(n.a,{href:"/llama-stack/docs/concepts/evaluation-concepts",children:"Evaluation Concepts"})," guide."]})}),"\n",(0,t.jsx)(n.h2,{id:"meta-reference",children:"Meta Reference"}),"\n",(0,t.jsx)(n.p,{children:"Meta's reference implementation of evaluation tasks with support for multiple languages and evaluation metrics."}),"\n",(0,t.jsx)(n.h3,{id:"configuration",children:"Configuration"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Field"}),(0,t.jsx)(n.th,{children:"Type"}),(0,t.jsx)(n.th,{children:"Required"}),(0,t.jsx)(n.th,{children:"Default"}),(0,t.jsx)(n.th,{children:"Description"})]})}),(0,t.jsx)(n.tbody,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"kvstore"})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"RedisKVStoreConfig | SqliteKVStoreConfig | PostgresKVStoreConfig | MongoDBKVStoreConfig"})}),(0,t.jsx)(n.td,{children:"No"}),(0,t.jsx)(n.td,{children:"sqlite"}),(0,t.jsx)(n.td,{children:"Key-value store configuration"})]})})]}),"\n",(0,t.jsx)(n.h3,{id:"sample-configuration",children:"Sample Configuration"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"kvstore:\n  type: sqlite\n  db_path: ${env.SQLITE_STORE_DIR:=~/.llama/dummy}/meta_reference_eval.db\n"})}),"\n",(0,t.jsx)(n.h3,{id:"features",children:"Features"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Multi-language evaluation support"}),"\n",(0,t.jsx)(n.li,{children:"Comprehensive evaluation metrics"}),"\n",(0,t.jsx)(n.li,{children:"Integration with various key-value stores (SQLite, Redis, PostgreSQL, MongoDB)"}),"\n",(0,t.jsx)(n.li,{children:"Built-in support for popular benchmarks"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"nvidia",children:"NVIDIA"}),"\n",(0,t.jsx)(n.p,{children:"NVIDIA's evaluation provider for running evaluation tasks on NVIDIA's platform."}),"\n",(0,t.jsx)(n.h3,{id:"configuration-1",children:"Configuration"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Field"}),(0,t.jsx)(n.th,{children:"Type"}),(0,t.jsx)(n.th,{children:"Required"}),(0,t.jsx)(n.th,{children:"Default"}),(0,t.jsx)(n.th,{children:"Description"})]})}),(0,t.jsx)(n.tbody,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"evaluator_url"})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"str"})}),(0,t.jsx)(n.td,{children:"No"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"http://0.0.0.0:7331",children:"http://0.0.0.0:7331"})}),(0,t.jsx)(n.td,{children:"The url for accessing the evaluator service"})]})})]}),"\n",(0,t.jsx)(n.h3,{id:"sample-configuration-1",children:"Sample Configuration"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"evaluator_url: ${env.NVIDIA_EVALUATOR_URL:=http://localhost:7331}\n"})}),"\n",(0,t.jsx)(n.h3,{id:"features-1",children:"Features"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Integration with NVIDIA's evaluation platform"}),"\n",(0,t.jsx)(n.li,{children:"Remote evaluation capabilities"}),"\n",(0,t.jsx)(n.li,{children:"Scalable evaluation processing"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"usage-example",children:"Usage Example"}),"\n",(0,t.jsx)(n.p,{children:"Here's a basic example of using the evaluation API:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from llama_stack_client import LlamaStackClient\n\nclient = LlamaStackClient(base_url="http://localhost:8321")\n\n# Register a dataset for evaluation\nclient.datasets.register(\n    purpose="evaluation",\n    source={\n        "type": "uri", \n        "uri": "huggingface://datasets/llamastack/evaluation_dataset"\n    },\n    dataset_id="my_eval_dataset"\n)\n\n# Run evaluation\neval_result = client.eval.run_evaluation(\n    dataset_id="my_eval_dataset",\n    scoring_functions=["accuracy", "bleu"],\n    model_id="my_model"\n)\n\nprint(f"Evaluation completed: {eval_result}")\n'})}),"\n",(0,t.jsx)(n.h2,{id:"supported-benchmarks",children:"Supported Benchmarks"}),"\n",(0,t.jsx)(n.p,{children:"Llama Stack pre-registers several popular open-benchmarks for easy model evaluation:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"MMLU-COT"})," - Measuring Massive Multitask Language Understanding with Chain of Thought"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"GPQA-COT"})," - Graduate-level Google-Proof Q&A with Chain of Thought"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"SimpleQA"})," - Short fact-seeking question benchmark"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"MMMU"})," - Multimodal understanding and reasoning benchmark"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Choose appropriate providers"}),": Use Meta Reference for comprehensive evaluation, NVIDIA for platform-specific needs"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Configure storage properly"}),": Ensure your key-value store configuration matches your performance requirements"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Monitor evaluation progress"}),": Large evaluations can take time - implement proper monitoring"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Use appropriate scoring functions"}),": Select scoring metrics that align with your evaluation goals"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Check out the ",(0,t.jsx)(n.a,{href:"/llama-stack/docs/concepts/evaluation-concepts",children:"Evaluation Concepts"})," guide for detailed conceptual information"]}),"\n",(0,t.jsxs)(n.li,{children:["See the ",(0,t.jsx)(n.a,{href:"/llama-stack/docs/building-applications/evals",children:"Building Applications - Evaluation"})," guide for application examples"]}),"\n",(0,t.jsxs)(n.li,{children:["Review the ",(0,t.jsx)(n.a,{href:"/llama-stack/docs/references/evals-reference",children:"Evaluation Reference"})," for comprehensive CLI and API usage"]}),"\n",(0,t.jsxs)(n.li,{children:["Explore the ",(0,t.jsx)(n.a,{href:"/llama-stack/docs/advanced-apis/scoring",children:"Scoring"})," documentation for available scoring functions"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}}}]);