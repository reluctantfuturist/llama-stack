"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[4400],{29467:(e,s,i)=>{i.r(s),i.d(s,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>n,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"concepts/distributions","title":"Distributions","description":"Pre-packaged provider configurations for different deployment scenarios","source":"@site/docs/concepts/distributions.mdx","sourceDirName":"concepts","slug":"/concepts/distributions","permalink":"/docs/concepts/distributions","draft":false,"unlisted":false,"editUrl":"https://github.com/meta-llama/llama-stack/tree/main/docs/docs/concepts/distributions.mdx","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"title":"Distributions","description":"Pre-packaged provider configurations for different deployment scenarios","sidebar_label":"Distributions","sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"API Providers","permalink":"/docs/concepts/api-providers"},"next":{"title":"Resources","permalink":"/docs/concepts/resources"}}');var t=i(74848),o=i(28453);const n={title:"Distributions",description:"Pre-packaged provider configurations for different deployment scenarios",sidebar_label:"Distributions",sidebar_position:5},a="Distributions",c={},d=[];function l(e){const s={a:"a",em:"em",h1:"h1",header:"header",p:"p",strong:"strong",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(s.header,{children:(0,t.jsx)(s.h1,{id:"distributions",children:"Distributions"})}),"\n",(0,t.jsxs)(s.p,{children:["While there is a lot of flexibility to mix-and-match providers, often users will work with a specific set of providers (hardware support, contractual obligations, etc.) We therefore need to provide a ",(0,t.jsx)(s.em,{children:"convenient shorthand"})," for such collections. We call this shorthand a ",(0,t.jsx)(s.strong,{children:"Llama Stack Distribution"})," or a ",(0,t.jsx)(s.strong,{children:"Distro"}),". One can think of it as specific pre-packaged versions of the Llama Stack. Here are some examples:"]}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Remotely Hosted Distro"}),": These are the simplest to consume from a user perspective. You can simply obtain the API key for these providers, point to a URL and have ",(0,t.jsx)(s.em,{children:"all"})," Llama Stack APIs working out of the box. Currently, ",(0,t.jsx)(s.a,{href:"https://fireworks.ai/",children:"Fireworks"})," and ",(0,t.jsx)(s.a,{href:"https://together.xyz/",children:"Together"})," provide such easy-to-consume Llama Stack distributions."]}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Locally Hosted Distro"}),": You may want to run Llama Stack on your own hardware. Typically though, you still need to use Inference via an external service. You can use providers like HuggingFace TGI, Fireworks, Together, etc. for this purpose. Or you may have access to GPUs and can run a ",(0,t.jsx)(s.a,{href:"https://github.com/vllm-project/vllm",children:"vLLM"})," or ",(0,t.jsx)(s.a,{href:"https://build.nvidia.com/nim?filters=nimType%3Anim_type_run_anywhere&q=llama",children:"NVIDIA NIM"}),' instance. If you "just" have a regular desktop machine, you can use ',(0,t.jsx)(s.a,{href:"https://ollama.com/",children:"Ollama"})," for inference. To provide convenient quick access to these options, we provide a number of such pre-configured locally-hosted Distros."]}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"On-device Distro"}),": To run Llama Stack directly on an edge device (mobile phone or a tablet), we provide Distros for ",(0,t.jsx)(s.a,{href:"/docs/distributions/ondevice_distro/ios_sdk",children:"iOS"})," and ",(0,t.jsx)(s.a,{href:"/docs/distributions/ondevice_distro/android_sdk",children:"Android"})]})]})}function h(e={}){const{wrapper:s}={...(0,o.R)(),...e.components};return s?(0,t.jsx)(s,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}}}]);