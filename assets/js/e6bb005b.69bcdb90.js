"use strict";(self.webpackChunkdocusaurus_template_openapi_docs=self.webpackChunkdocusaurus_template_openapi_docs||[]).push([[483],{28453:(e,n,s)=>{s.d(n,{R:()=>i,x:()=>l});var a=s(96540);const t={},r=a.createContext(t);function i(e){const n=a.useContext(r);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:i(e.components),a.createElement(r.Provider,{value:n},e.children)}},64911:(e,n,s)=>{s.d(n,{A:()=>p});var a=s(96540),t=s(34164),r=s(65627),i=s(77448),l=s(9136);const o={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var c=s(74848);function u({className:e,block:n,selectedValue:s,selectValue:a,tabValues:i}){const l=[],{blockElementScrollPositionUntilNextRender:u}=(0,r.a_)(),d=e=>{const n=e.currentTarget,t=l.indexOf(n),r=i[t].value;r!==s&&(u(n),a(r))},h=e=>{let n=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const s=l.indexOf(e.currentTarget)+1;n=l[s]??l[0];break}case"ArrowLeft":{const s=l.indexOf(e.currentTarget)-1;n=l[s]??l[l.length-1];break}}n?.focus()};return(0,c.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,t.A)("tabs",{"tabs--block":n},e),children:i.map(({value:e,label:n,attributes:a})=>(0,c.jsx)("li",{role:"tab",tabIndex:s===e?0:-1,"aria-selected":s===e,ref:e=>{l.push(e)},onKeyDown:h,onClick:d,...a,className:(0,t.A)("tabs__item",o.tabItem,a?.className,{"tabs__item--active":s===e}),children:n??e},e))})}function d({lazy:e,children:n,selectedValue:s}){const r=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=r.find(e=>e.props.value===s);return e?(0,a.cloneElement)(e,{className:(0,t.A)("margin-top--md",e.props.className)}):null}return(0,c.jsx)("div",{className:"margin-top--md",children:r.map((e,n)=>(0,a.cloneElement)(e,{key:n,hidden:e.props.value!==s}))})}function h(e){const n=(0,i.u)(e);return(0,c.jsxs)("div",{className:(0,t.A)("tabs-container",o.tabList),children:[(0,c.jsx)(u,{...n,...e}),(0,c.jsx)(d,{...n,...e})]})}function p(e){const n=(0,l.default)();return(0,c.jsx)(h,{...e,children:(0,i.v)(e.children)},String(n))}},77448:(e,n,s)=>{s.d(n,{u:()=>p,v:()=>c});var a=s(96540),t=s(56347),r=s(50372),i=s(30604),l=s(78749),o=s(11861);function c(e){return a.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,a.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function u(e){const{values:n,children:s}=e;return(0,a.useMemo)(()=>{const e=n??function(e){return c(e).map(({props:{value:e,label:n,attributes:s,default:a}})=>({value:e,label:n,attributes:s,default:a}))}(s);return function(e){const n=(0,o.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,s])}function d({value:e,tabValues:n}){return n.some(n=>n.value===e)}function h({queryString:e=!1,groupId:n}){const s=(0,t.W6)(),r=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,i.aZ)(r),(0,a.useCallback)(e=>{if(!r)return;const n=new URLSearchParams(s.location.search);n.set(r,e),s.replace({...s.location,search:n.toString()})},[r,s])]}function p(e){const{defaultValue:n,queryString:s=!1,groupId:t}=e,i=u(e),[o,c]=(0,a.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!d({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const s=n.find(e=>e.default)??n[0];if(!s)throw new Error("Unexpected error: 0 tabValues");return s.value}({defaultValue:n,tabValues:i})),[p,g]=h({queryString:s,groupId:t}),[m,v]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[s,t]=(0,l.Dv)(n);return[s,(0,a.useCallback)(e=>{n&&t.set(e)},[n,t])]}({groupId:t}),x=(()=>{const e=p??m;return d({value:e,tabValues:i})?e:null})();(0,r.A)(()=>{x&&c(x)},[x]);return{selectedValue:o,selectValue:(0,a.useCallback)(e=>{if(!d({value:e,tabValues:i}))throw new Error(`Can't select invalid tab value=${e}`);c(e),g(e),v(e)},[g,v,i]),tabValues:i}}},79329:(e,n,s)=>{s.r(n),s.d(n,{default:()=>i});s(96540);var a=s(34164);const t={tabItem:"tabItem_Ymn6"};var r=s(74848);function i({children:e,hidden:n,className:s}){return(0,r.jsx)("div",{role:"tabpanel",className:(0,a.A)(t.tabItem,s),hidden:n,children:e})}},98484:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>u,contentTitle:()=>c,default:()=>p,frontMatter:()=>o,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"building-applications/evals","title":"Evaluations","description":"Evaluate LLM applications with Llama Stack\'s comprehensive evaluation framework","source":"@site/docs/building-applications/evals.mdx","sourceDirName":"building-applications","slug":"/building-applications/evals","permalink":"/llama-stack/docs/building-applications/evals","draft":false,"unlisted":false,"editUrl":"https://github.com/meta-llama/llama-stack/tree/main/docs/docs/building-applications/evals.mdx","tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"title":"Evaluations","description":"Evaluate LLM applications with Llama Stack\'s comprehensive evaluation framework","sidebar_label":"Evaluations","sidebar_position":7},"sidebar":"tutorialSidebar","previous":{"title":"Tools","permalink":"/llama-stack/docs/building-applications/tools"},"next":{"title":"Telemetry","permalink":"/llama-stack/docs/building-applications/telemetry"}}');var t=s(74848),r=s(28453),i=s(64911),l=s(79329);const o={title:"Evaluations",description:"Evaluate LLM applications with Llama Stack's comprehensive evaluation framework",sidebar_label:"Evaluations",sidebar_position:7},c="Evaluations",u={},d=[{value:"Application Evaluation Example",id:"application-evaluation-example",level:2},{value:"Step-by-Step Evaluation Process",id:"step-by-step-evaluation-process",level:2},{value:"1. Building a Search Agent",id:"1-building-a-search-agent",level:3},{value:"2. Query Agent Execution Steps",id:"2-query-agent-execution-steps",level:3},{value:"3. Evaluate Agent Responses",id:"3-evaluate-agent-responses",level:3},{value:"Available Scoring Functions",id:"available-scoring-functions",level:2},{value:"Basic Scoring Functions",id:"basic-scoring-functions",level:3},{value:"Advanced Scoring Functions",id:"advanced-scoring-functions",level:3},{value:"Custom Scoring Functions",id:"custom-scoring-functions",level:3},{value:"Evaluation Workflow Best Practices",id:"evaluation-workflow-best-practices",level:2},{value:"\ud83c\udfaf <strong>Dataset Preparation</strong>",id:"-dataset-preparation",level:3},{value:"\ud83d\udcca <strong>Metrics Selection</strong>",id:"-metrics-selection",level:3},{value:"\ud83d\udd04 <strong>Iterative Improvement</strong>",id:"-iterative-improvement",level:3},{value:"\ud83d\udcc8 <strong>Analysis &amp; Reporting</strong>",id:"-analysis--reporting",level:3},{value:"Advanced Evaluation Scenarios",id:"advanced-evaluation-scenarios",level:2},{value:"Batch Evaluation",id:"batch-evaluation",level:3},{value:"Multi-Metric Evaluation",id:"multi-metric-evaluation",level:3},{value:"Related Resources",id:"related-resources",level:2}];function h(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"evaluations",children:"Evaluations"})}),"\n",(0,t.jsx)(n.p,{children:"The Llama Stack provides a comprehensive set of APIs for supporting evaluations of LLM applications:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsxs)(n.strong,{children:[(0,t.jsx)(n.code,{children:"/datasetio"})," + ",(0,t.jsx)(n.code,{children:"/datasets"})," API"]}),": Manage evaluation datasets and data input/output"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsxs)(n.strong,{children:[(0,t.jsx)(n.code,{children:"/scoring"})," + ",(0,t.jsx)(n.code,{children:"/scoring_functions"})," API"]}),": Apply scoring functions to evaluate responses"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsxs)(n.strong,{children:[(0,t.jsx)(n.code,{children:"/eval"})," + ",(0,t.jsx)(n.code,{children:"/benchmarks"})," API"]}),": Run benchmarks and structured evaluations"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["This guide walks you through the process of evaluating an LLM application built using Llama Stack. For detailed API reference, check out the ",(0,t.jsx)(n.a,{href:"/docs/references/evals-reference",children:"Evaluation Reference"})," guide that covers the complete set of APIs and developer experience flow."]}),"\n",(0,t.jsx)(n.admonition,{title:"Interactive Examples",type:"tip",children:(0,t.jsxs)(n.p,{children:["Check out our ",(0,t.jsx)(n.a,{href:"https://colab.research.google.com/drive/10CHyykee9j2OigaIcRv47BKG9mrNm0tJ?usp=sharing",children:"Colab notebook"})," for working examples with evaluations, or try the ",(0,t.jsx)(n.a,{href:"https://colab.research.google.com/github/meta-llama/llama-stack/blob/main/docs/getting_started.ipynb",children:"Getting Started notebook"}),"."]})}),"\n",(0,t.jsx)(n.h2,{id:"application-evaluation-example",children:"Application Evaluation Example"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"https://colab.research.google.com/github/meta-llama/llama-stack/blob/main/docs/getting_started.ipynb",children:(0,t.jsx)(n.img,{src:"https://colab.research.google.com/assets/colab-badge.svg",alt:"Open In Colab"})})}),"\n",(0,t.jsxs)(n.p,{children:["Llama Stack offers a library of scoring functions and the ",(0,t.jsx)(n.code,{children:"/scoring"})," API, allowing you to run evaluations on your pre-annotated AI application datasets."]}),"\n",(0,t.jsx)(n.p,{children:"In this example, we will show you how to:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Build an Agent"})," with Llama Stack"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Query the agent's sessions, turns, and steps"})," to analyze execution"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Evaluate the results"})," using scoring functions"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"step-by-step-evaluation-process",children:"Step-by-Step Evaluation Process"}),"\n",(0,t.jsx)(n.h3,{id:"1-building-a-search-agent",children:"1. Building a Search Agent"}),"\n",(0,t.jsx)(n.p,{children:"First, let's create an agent that can search the web to answer questions:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from llama_stack_client import LlamaStackClient, Agent, AgentEventLogger\n\nclient = LlamaStackClient(base_url=f"http://{HOST}:{PORT}")\n\nagent = Agent(\n    client,\n    model="meta-llama/Llama-3.3-70B-Instruct",\n    instructions="You are a helpful assistant. Use search tool to answer the questions.",\n    tools=["builtin::websearch"],\n)\n\n# Test prompts for evaluation\nuser_prompts = [\n    "Which teams played in the NBA Western Conference Finals of 2024. Search the web for the answer.",\n    "In which episode and season of South Park does Bill Cosby (BSM-471) first appear? Give me the number and title. Search the web for the answer.",\n    "What is the British-American kickboxer Andrew Tate\'s kickboxing name? Search the web for the answer.",\n]\n\nsession_id = agent.create_session("test-session")\n\n# Execute all prompts in the session\nfor prompt in user_prompts:\n    response = agent.create_turn(\n        messages=[\n            {\n                "role": "user",\n                "content": prompt,\n            }\n        ],\n        session_id=session_id,\n    )\n\n    for log in AgentEventLogger().log(response):\n        log.print()\n'})}),"\n",(0,t.jsx)(n.h3,{id:"2-query-agent-execution-steps",children:"2. Query Agent Execution Steps"}),"\n",(0,t.jsx)(n.p,{children:"Now, let's analyze the agent's execution steps to understand its performance:"}),"\n",(0,t.jsxs)(i.A,{children:[(0,t.jsx)(l.default,{value:"session-analysis",label:"Session Analysis",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from rich.pretty import pprint\n\n# Query the agent's session to get detailed execution data\nsession_response = client.agents.session.retrieve(\n    session_id=session_id,\n    agent_id=agent.agent_id,\n)\n\npprint(session_response)\n"})})}),(0,t.jsx)(l.default,{value:"tool-validation",label:"Tool Usage Validation",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Sanity check: Verify that all user prompts are followed by tool calls\nnum_tool_call = 0\nfor turn in session_response.turns:\n    for step in turn.steps:\n        if (\n            step.step_type == "tool_execution"\n            and step.tool_calls[0].tool_name == "brave_search"\n        ):\n            num_tool_call += 1\n\nprint(\n    f"{num_tool_call}/{len(session_response.turns)} user prompts are followed by a tool call to `brave_search`"\n)\n'})})})]}),"\n",(0,t.jsx)(n.h3,{id:"3-evaluate-agent-responses",children:"3. Evaluate Agent Responses"}),"\n",(0,t.jsx)(n.p,{children:"Now we'll evaluate the agent's responses using Llama Stack's scoring API:"}),"\n",(0,t.jsxs)(i.A,{children:[(0,t.jsx)(l.default,{value:"data-preparation",label:"Data Preparation",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Process agent execution history into evaluation rows\neval_rows = []\n\n# Define expected answers for our test prompts\nexpected_answers = [\n    "Dallas Mavericks and the Minnesota Timberwolves",\n    "Season 4, Episode 12", \n    "King Cobra",\n]\n\n# Create evaluation dataset from agent responses\nfor i, turn in enumerate(session_response.turns):\n    eval_rows.append(\n        {\n            "input_query": turn.input_messages[0].content,\n            "generated_answer": turn.output_message.content,\n            "expected_answer": expected_answers[i],\n        }\n    )\n\npprint(eval_rows)\n'})})}),(0,t.jsx)(l.default,{value:"scoring",label:"Scoring & Evaluation",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Configure scoring parameters\nscoring_params = {\n    "basic::subset_of": None,  # Check if generated answer contains expected answer\n}\n\n# Run evaluation using Llama Stack\'s scoring API\nscoring_response = client.scoring.score(\n    input_rows=eval_rows, \n    scoring_functions=scoring_params\n)\n\npprint(scoring_response)\n\n# Analyze results\nfor i, result in enumerate(scoring_response.results):\n    print(f"Query {i+1}: {result.score}")\n    print(f"  Generated: {eval_rows[i][\'generated_answer\'][:100]}...")\n    print(f"  Expected: {expected_answers[i]}")\n    print(f"  Score: {result.score}")\n    print()\n'})})})]}),"\n",(0,t.jsx)(n.h2,{id:"available-scoring-functions",children:"Available Scoring Functions"}),"\n",(0,t.jsx)(n.p,{children:"Llama Stack provides several built-in scoring functions:"}),"\n",(0,t.jsx)(n.h3,{id:"basic-scoring-functions",children:"Basic Scoring Functions"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"basic::subset_of"})}),": Checks if the expected answer is contained in the generated response"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"basic::exact_match"})}),": Performs exact string matching between expected and generated answers"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"basic::regex_match"})}),": Uses regular expressions to match patterns in responses"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"advanced-scoring-functions",children:"Advanced Scoring Functions"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"llm_as_judge::accuracy"})}),": Uses an LLM to judge response accuracy"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"llm_as_judge::helpfulness"})}),": Evaluates how helpful the response is"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"llm_as_judge::safety"})}),": Assesses response safety and appropriateness"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"custom-scoring-functions",children:"Custom Scoring Functions"}),"\n",(0,t.jsx)(n.p,{children:"You can also create custom scoring functions for domain-specific evaluation needs."}),"\n",(0,t.jsx)(n.h2,{id:"evaluation-workflow-best-practices",children:"Evaluation Workflow Best Practices"}),"\n",(0,t.jsxs)(n.h3,{id:"-dataset-preparation",children:["\ud83c\udfaf ",(0,t.jsx)(n.strong,{children:"Dataset Preparation"})]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Use diverse test cases that cover edge cases and common scenarios"}),"\n",(0,t.jsx)(n.li,{children:"Include clear expected answers or success criteria"}),"\n",(0,t.jsx)(n.li,{children:"Balance your dataset across different difficulty levels"}),"\n"]}),"\n",(0,t.jsxs)(n.h3,{id:"-metrics-selection",children:["\ud83d\udcca ",(0,t.jsx)(n.strong,{children:"Metrics Selection"})]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Choose appropriate scoring functions for your use case"}),"\n",(0,t.jsx)(n.li,{children:"Combine multiple metrics for comprehensive evaluation"}),"\n",(0,t.jsx)(n.li,{children:"Consider both automated and human evaluation metrics"}),"\n"]}),"\n",(0,t.jsxs)(n.h3,{id:"-iterative-improvement",children:["\ud83d\udd04 ",(0,t.jsx)(n.strong,{children:"Iterative Improvement"})]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Run evaluations regularly during development"}),"\n",(0,t.jsx)(n.li,{children:"Use evaluation results to identify areas for improvement"}),"\n",(0,t.jsx)(n.li,{children:"Track performance changes over time"}),"\n"]}),"\n",(0,t.jsxs)(n.h3,{id:"-analysis--reporting",children:["\ud83d\udcc8 ",(0,t.jsx)(n.strong,{children:"Analysis & Reporting"})]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Analyze failures to understand model limitations"}),"\n",(0,t.jsx)(n.li,{children:"Generate comprehensive evaluation reports"}),"\n",(0,t.jsx)(n.li,{children:"Share results with stakeholders for informed decision-making"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"advanced-evaluation-scenarios",children:"Advanced Evaluation Scenarios"}),"\n",(0,t.jsx)(n.h3,{id:"batch-evaluation",children:"Batch Evaluation"}),"\n",(0,t.jsx)(n.p,{children:"For evaluating large datasets efficiently:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Prepare large evaluation dataset\nlarge_eval_dataset = [\n    {"input_query": query, "expected_answer": answer}\n    for query, answer in zip(queries, expected_answers)\n]\n\n# Run batch evaluation\nbatch_results = client.scoring.score(\n    input_rows=large_eval_dataset,\n    scoring_functions={\n        "basic::subset_of": None,\n        "llm_as_judge::accuracy": {"judge_model": "meta-llama/Llama-3.3-70B-Instruct"},\n    }\n)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"multi-metric-evaluation",children:"Multi-Metric Evaluation"}),"\n",(0,t.jsx)(n.p,{children:"Combining different scoring approaches:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'comprehensive_scoring = {\n    "exact_match": "basic::exact_match",\n    "subset_match": "basic::subset_of", \n    "llm_judge": "llm_as_judge::accuracy",\n    "safety_check": "llm_as_judge::safety",\n}\n\nresults = client.scoring.score(\n    input_rows=eval_rows,\n    scoring_functions=comprehensive_scoring\n)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"related-resources",children:"Related Resources"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"./agent",children:"Agents"})})," - Building agents for evaluation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"./tools",children:"Tools Integration"})})," - Using tools in evaluated agents"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"/docs/references/evals-reference",children:"Evaluation Reference"})})," - Complete API reference for evaluations"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"https://colab.research.google.com/github/meta-llama/llama-stack/blob/main/docs/getting_started.ipynb",children:"Getting Started Notebook"})})," - Interactive examples"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"https://colab.research.google.com/drive/10CHyykee9j2OigaIcRv47BKG9mrNm0tJ?usp=sharing",children:"Evaluation Examples"})})," - Additional evaluation scenarios"]}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(h,{...e})}):h(e)}}}]);