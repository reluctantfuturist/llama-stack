"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[1007],{4865:(e,n,i)=>{i.d(n,{A:()=>m});var t=i(96540),a=i(34164),r=i(23104),s=i(47751),l=i(92303);const o={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var d=i(74848);function c(e){var n=e.className,i=e.block,t=e.selectedValue,s=e.selectValue,l=e.tabValues,c=[],u=(0,r.a_)().blockElementScrollPositionUntilNextRender,h=function(e){var n=e.currentTarget,i=c.indexOf(n),a=l[i].value;a!==t&&(u(n),s(a))},m=function(e){var n,i=null;switch(e.key){case"Enter":h(e);break;case"ArrowRight":var t,a=c.indexOf(e.currentTarget)+1;i=null!=(t=c[a])?t:c[0];break;case"ArrowLeft":var r,s=c.indexOf(e.currentTarget)-1;i=null!=(r=c[s])?r:c[c.length-1]}null==(n=i)||n.focus()};return(0,d.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,a.A)("tabs",{"tabs--block":i},n),children:l.map(function(e){var n=e.value,i=e.label,r=e.attributes;return(0,d.jsx)("li",Object.assign({role:"tab",tabIndex:t===n?0:-1,"aria-selected":t===n,ref:function(e){c.push(e)},onKeyDown:m,onClick:h},r,{className:(0,a.A)("tabs__item",o.tabItem,null==r?void 0:r.className,{"tabs__item--active":t===n}),children:null!=i?i:n}),n)})})}function u(e){var n=e.lazy,i=e.children,r=e.selectedValue,s=(Array.isArray(i)?i:[i]).filter(Boolean);if(n){var l=s.find(function(e){return e.props.value===r});return l?(0,t.cloneElement)(l,{className:(0,a.A)("margin-top--md",l.props.className)}):null}return(0,d.jsx)("div",{className:"margin-top--md",children:s.map(function(e,n){return(0,t.cloneElement)(e,{key:n,hidden:e.props.value!==r})})})}function h(e){var n=(0,s.u)(e);return(0,d.jsxs)("div",{className:(0,a.A)("tabs-container",o.tabList),children:[(0,d.jsx)(c,Object.assign({},n,e)),(0,d.jsx)(u,Object.assign({},n,e))]})}function m(e){var n=(0,l.default)();return(0,d.jsx)(h,Object.assign({},e,{children:(0,s.v)(e.children)}),String(n))}},49149:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>d,default:()=>m,frontMatter:()=>o,metadata:()=>t,toc:()=>u});const t=JSON.parse('{"id":"distributions/building-distro","title":"Build Your Own Distribution","description":"Step-by-step guide to create custom Llama Stack distributions with your choice of API providers","source":"@site/docs/distributions/building-distro.mdx","sourceDirName":"distributions","slug":"/distributions/building-distro","permalink":"/llama-stack/docs/distributions/building-distro","draft":false,"unlisted":false,"editUrl":"https://github.com/meta-llama/llama-stack/tree/main/docs/docs/distributions/building-distro.mdx","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Build Your Own Distribution","description":"Step-by-step guide to create custom Llama Stack distributions with your choice of API providers","sidebar_label":"Building Custom Distributions","sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Available Distributions","permalink":"/llama-stack/docs/distributions/list-of-distributions"},"next":{"title":"Customizing run.yaml","permalink":"/llama-stack/docs/distributions/customizing-run-yaml"}}');var a=i(74848),r=i(28453),s=i(4865),l=i(19365);const o={title:"Build Your Own Distribution",description:"Step-by-step guide to create custom Llama Stack distributions with your choice of API providers",sidebar_label:"Building Custom Distributions",sidebar_position:3},d="Build Your Own Distribution",c={},u=[{value:"Setting Your Log Level",id:"setting-your-log-level",level:2},{value:"Llama Stack Build",id:"llama-stack-build",level:2},{value:"Build Methods",id:"build-methods",level:2},{value:"Running Your Stack Server",id:"running-your-stack-server",level:2},{value:"Managing Distributions",id:"managing-distributions",level:2},{value:"Listing Distributions",id:"listing-distributions",level:3},{value:"Removing a Distribution",id:"removing-a-distribution",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2}];function h(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"build-your-own-distribution",children:"Build Your Own Distribution"})}),"\n",(0,a.jsx)(n.p,{children:"This guide will walk you through the steps to get started with building a Llama Stack distribution from scratch with your choice of API providers."}),"\n",(0,a.jsx)(n.h2,{id:"setting-your-log-level",children:"Setting Your Log Level"}),"\n",(0,a.jsxs)(n.p,{children:["In order to specify the proper logging level users can apply the following environment variable ",(0,a.jsx)(n.code,{children:"LLAMA_STACK_LOGGING"})," with the following format:"]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.code,{children:"LLAMA_STACK_LOGGING=server=debug;core=info"})}),"\n",(0,a.jsx)(n.p,{children:"Where each category in the following list:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"all"}),"\n",(0,a.jsx)(n.li,{children:"core"}),"\n",(0,a.jsx)(n.li,{children:"server"}),"\n",(0,a.jsx)(n.li,{children:"router"}),"\n",(0,a.jsx)(n.li,{children:"inference"}),"\n",(0,a.jsx)(n.li,{children:"agents"}),"\n",(0,a.jsx)(n.li,{children:"safety"}),"\n",(0,a.jsx)(n.li,{children:"eval"}),"\n",(0,a.jsx)(n.li,{children:"tools"}),"\n",(0,a.jsx)(n.li,{children:"client"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Can be set to any of the following log levels:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"debug"}),"\n",(0,a.jsx)(n.li,{children:"info"}),"\n",(0,a.jsx)(n.li,{children:"warning"}),"\n",(0,a.jsx)(n.li,{children:"error"}),"\n",(0,a.jsx)(n.li,{children:"critical"}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["The default global log level is ",(0,a.jsx)(n.code,{children:"info"}),". ",(0,a.jsx)(n.code,{children:"all"})," sets the log level for all components."]}),"\n",(0,a.jsxs)(n.p,{children:["A user can also set ",(0,a.jsx)(n.code,{children:"LLAMA_STACK_LOG_FILE"})," which will pipe the logs to the specified path as well as to the terminal. An example would be: ",(0,a.jsx)(n.code,{children:"export LLAMA_STACK_LOG_FILE=server.log"})]}),"\n",(0,a.jsx)(n.h2,{id:"llama-stack-build",children:"Llama Stack Build"}),"\n",(0,a.jsxs)(n.p,{children:["In order to build your own distribution, we recommend you clone the ",(0,a.jsx)(n.code,{children:"llama-stack"})," repository."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"git clone git@github.com:meta-llama/llama-stack.git\ncd llama-stack\npip install -e .\n"})}),"\n",(0,a.jsx)(n.p,{children:"Use the CLI to build your distribution. The main points to consider are:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Image Type"})," - Do you want a venv environment or a Container (eg. Docker)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Template"})," - Do you want to use a template to build your distribution? or start from scratch ?"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Config"})," - Do you want to use a pre-existing config file to build your distribution?"]}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"llama stack build -h\nusage: llama stack build [-h] [--config CONFIG] [--template TEMPLATE] [--distro DISTRIBUTION] [--list-distros] [--image-type {container,venv}] [--image-name IMAGE_NAME] [--print-deps-only]\n                         [--run] [--providers PROVIDERS]\n\nBuild a Llama stack container\n\noptions:\n  -h, --help            show this help message and exit\n  --config CONFIG       Path to a config file to use for the build. You can find example configs in llama_stack.cores/**/build.yaml. If this argument is not provided, you will be prompted to\n                        enter information interactively (default: None)\n  --template TEMPLATE   (deprecated) Name of the example template config to use for build. You may use `llama stack build --list-distros` to check out the available distributions (default:\n                        None)\n  --distro DISTRIBUTION, --distribution DISTRIBUTION\n                        Name of the distribution to use for build. You may use `llama stack build --list-distros` to check out the available distributions (default: None)\n  --list-distros, --list-distributions\n                        Show the available distributions for building a Llama Stack distribution (default: False)\n  --image-type {container,venv}\n                        Image Type to use for the build. If not specified, will use the image type from the template config. (default: None)\n  --image-name IMAGE_NAME\n                        [for image-type=container|venv] Name of the virtual environment to use for the build. If not specified, currently active environment will be used if found. (default:\n                        None)\n  --print-deps-only     Print the dependencies for the stack only, without building the stack (default: False)\n  --run                 Run the stack after building using the same image type, name, and other applicable arguments (default: False)\n  --providers PROVIDERS\n                        Build a config for a list of providers and only those providers. This list is formatted like: api1=provider1,api2=provider2. Where there can be multiple providers per\n                        API. (default: None)\n"})}),"\n",(0,a.jsxs)(n.p,{children:["After this step is complete, a file named ",(0,a.jsx)(n.code,{children:"<name>-build.yaml"})," and template file ",(0,a.jsx)(n.code,{children:"<name>-run.yaml"})," will be generated and saved at the output file path specified at the end of the command."]}),"\n",(0,a.jsx)(n.h2,{id:"build-methods",children:"Build Methods"}),"\n",(0,a.jsxs)(s.A,{children:[(0,a.jsxs)(l.default,{value:"template",label:"Building from a template",children:[(0,a.jsx)(n.p,{children:"To build from alternative API providers, we provide distribution templates for users to get started building a distribution backed by different providers."}),(0,a.jsx)(n.p,{children:"The following command will allow you to see the available templates and their corresponding providers."}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"llama stack build --list-templates\n"})}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"------------------------------+-----------------------------------------------------------------------------+\n| Template Name                | Description                                                                 |\n+------------------------------+-----------------------------------------------------------------------------+\n| watsonx                      | Use watsonx for running LLM inference                                       |\n+------------------------------+-----------------------------------------------------------------------------+\n| vllm-gpu                     | Use a built-in vLLM engine for running LLM inference                        |\n+------------------------------+-----------------------------------------------------------------------------+\n| together                     | Use Together.AI for running LLM inference                                   |\n+------------------------------+-----------------------------------------------------------------------------+\n| tgi                          | Use (an external) TGI server for running LLM inference                      |\n+------------------------------+-----------------------------------------------------------------------------+\n| starter                      | Quick start template for running Llama Stack with several popular providers |\n+------------------------------+-----------------------------------------------------------------------------+\n| sambanova                    | Use SambaNova for running LLM inference and safety                          |\n+------------------------------+-----------------------------------------------------------------------------+\n| remote-vllm                  | Use (an external) vLLM server for running LLM inference                     |\n+------------------------------+-----------------------------------------------------------------------------+\n| postgres-demo                | Quick start template for running Llama Stack with several popular providers |\n+------------------------------+-----------------------------------------------------------------------------+\n| passthrough                  | Use Passthrough hosted llama-stack endpoint for LLM inference               |\n+------------------------------+-----------------------------------------------------------------------------+\n| open-benchmark               | Distribution for running open benchmarks                                    |\n+------------------------------+-----------------------------------------------------------------------------+\n| ollama                       | Use (an external) Ollama server for running LLM inference                   |\n+------------------------------+-----------------------------------------------------------------------------+\n| nvidia                       | Use NVIDIA NIM for running LLM inference, evaluation and safety             |\n+------------------------------+-----------------------------------------------------------------------------+\n| meta-reference-gpu           | Use Meta Reference for running LLM inference                                |\n+------------------------------+-----------------------------------------------------------------------------+\n| llama_api                    | Distribution for running e2e tests in CI                                    |\n+------------------------------+-----------------------------------------------------------------------------+\n| hf-serverless                | Use (an external) Hugging Face Inference Endpoint for running LLM inference |\n+------------------------------+-----------------------------------------------------------------------------+\n| hf-endpoint                  | Use (an external) Hugging Face Inference Endpoint for running LLM inference |\n+------------------------------+-----------------------------------------------------------------------------+\n| groq                         | Use Groq for running LLM inference                                          |\n+------------------------------+-----------------------------------------------------------------------------+\n| fireworks                    | Use Fireworks.AI for running LLM inference                                  |\n+------------------------------+-----------------------------------------------------------------------------+\n| experimental-post-training   | Experimental template for post training                                     |\n+------------------------------+-----------------------------------------------------------------------------+\n| dell                         | Dell's distribution of Llama Stack. TGI inference via Dell's custom         |\n|                              | container                                                                   |\n+------------------------------+-----------------------------------------------------------------------------+\n| ci-tests                     | Distribution for running e2e tests in CI                                    |\n+------------------------------+-----------------------------------------------------------------------------+\n| cerebras                     | Use Cerebras for running LLM inference                                      |\n+------------------------------+-----------------------------------------------------------------------------+\n| bedrock                      | Use AWS Bedrock for running LLM inference and safety                        |\n+------------------------------+-----------------------------------------------------------------------------+\n"})}),(0,a.jsx)(n.p,{children:"You may then pick a template to build your distribution with providers fitted to your liking."}),(0,a.jsx)(n.p,{children:"For example, to build a distribution with TGI as the inference provider, you can run:"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"$ llama stack build --distro starter\n...\nYou can now edit ~/.llama/distributions/llamastack-starter/starter-run.yaml and run `llama stack run ~/.llama/distributions/llamastack-starter/starter-run.yaml`\n"})}),(0,a.jsx)(n.admonition,{type:"tip",children:(0,a.jsxs)(n.p,{children:["The generated ",(0,a.jsx)(n.code,{children:"run.yaml"})," file is a starting point for your configuration. For comprehensive guidance on customizing it for your specific needs, infrastructure, and deployment scenarios, see ",(0,a.jsx)(n.a,{href:"./customizing-run-yaml",children:"Customizing Your run.yaml Configuration"}),"."]})})]}),(0,a.jsxs)(l.default,{value:"scratch",label:"Building from Scratch",children:[(0,a.jsxs)(n.p,{children:["If the provided templates do not fit your use case, you could start off with running ",(0,a.jsx)(n.code,{children:"llama stack build"})," which will allow you to a interactively enter wizard where you will be prompted to enter build configurations."]}),(0,a.jsx)(n.p,{children:"It would be best to start with a template and understand the structure of the config file and the various concepts ( APIS, providers, resources, etc.) before starting from scratch."}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"llama stack build\n\n> Enter a name for your Llama Stack (e.g. my-local-stack): my-stack\n> Enter the image type you want your Llama Stack to be built as (container or venv): venv\n\nLlama Stack is composed of several APIs working together. Let's select\nthe provider types (implementations) you want to use for these APIs.\n\nTip: use <TAB> to see options for the providers.\n\n> Enter provider for API inference: inline::meta-reference\n> Enter provider for API safety: inline::llama-guard\n> Enter provider for API agents: inline::meta-reference\n> Enter provider for API memory: inline::faiss\n> Enter provider for API datasetio: inline::meta-reference\n> Enter provider for API scoring: inline::meta-reference\n> Enter provider for API eval: inline::meta-reference\n> Enter provider for API telemetry: inline::meta-reference\n\n > (Optional) Enter a short description for your Llama Stack:\n\nYou can now edit ~/.llama/distributions/llamastack-my-local-stack/my-local-stack-run.yaml and run `llama stack run ~/.llama/distributions/llamastack-my-local-stack/my-local-stack-run.yaml`\n"})})]}),(0,a.jsxs)(l.default,{value:"config",label:"Building from a pre-existing build config file",children:[(0,a.jsx)(n.p,{children:"In addition to templates, you may customize the build to your liking through editing config files and build from config files with the following command."}),(0,a.jsxs)(n.p,{children:["The config file will be of contents like the ones in ",(0,a.jsx)(n.code,{children:"llama_stack/distributions/*build.yaml"}),"."]}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"llama stack build --config llama_stack/distributions/starter/build.yaml\n"})})]}),(0,a.jsxs)(l.default,{value:"external",label:"Building with External Providers",children:[(0,a.jsx)(n.p,{children:"Llama Stack supports external providers that live outside of the main codebase. This allows you to create and maintain your own providers independently or use community-provided providers."}),(0,a.jsx)(n.p,{children:"To build a distribution with external providers, you need to:"}),(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["Configure the ",(0,a.jsx)(n.code,{children:"external_providers_dir"})," in your build configuration file:"]}),"\n"]}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:"# Example my-external-stack.yaml with external providers\nversion: '2'\ndistribution_spec:\n  description: Custom distro for CI tests\n  providers:\n    inference:\n    - remote::custom_ollama\n# Add more providers as needed\nimage_type: container\nimage_name: ci-test\n# Path to external provider implementations\nexternal_providers_dir: ~/.llama/providers.d\n"})}),(0,a.jsx)(n.p,{children:"Here's an example for a custom Ollama provider:"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:"adapter:\n  adapter_type: custom_ollama\n  pip_packages:\n  - ollama\n  - aiohttp\n  - llama-stack-provider-ollama # This is the provider package\n  config_class: llama_stack_ollama_provider.config.OllamaImplConfig\n  module: llama_stack_ollama_provider\napi_dependencies: []\noptional_api_dependencies: []\n"})}),(0,a.jsxs)(n.p,{children:["The ",(0,a.jsx)(n.code,{children:"pip_packages"})," section lists the Python packages required by the provider, as well as the\nprovider package itself. The package must be available on PyPI or can be provided from a local\ndirectory or a git repository (git must be installed on the build environment)."]}),(0,a.jsxs)(n.ol,{start:"2",children:["\n",(0,a.jsx)(n.li,{children:"Build your distribution using the config file:"}),"\n"]}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"llama stack build --config my-external-stack.yaml\n"})}),(0,a.jsxs)(n.p,{children:["For more information on external providers, including directory structure, provider types, and implementation requirements, see the ",(0,a.jsx)(n.a,{href:"/docs/providers/external/external-providers-guide",children:"External Providers documentation"}),"."]})]}),(0,a.jsxs)(l.default,{value:"container",label:"Building Container",children:[(0,a.jsx)(n.admonition,{title:"Podman Alternative",type:"tip",children:(0,a.jsxs)(n.p,{children:["Podman is supported as an alternative to Docker. Set ",(0,a.jsx)(n.code,{children:"CONTAINER_BINARY"})," to ",(0,a.jsx)(n.code,{children:"podman"})," in your environment to use Podman."]})}),(0,a.jsxs)(n.p,{children:["To build a container image, you may start off from a template and use the ",(0,a.jsx)(n.code,{children:"--image-type container"})," flag to specify ",(0,a.jsx)(n.code,{children:"container"})," as the build image type."]}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"llama stack build --distro starter --image-type container\n"})}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"$ llama stack build --distro starter --image-type container\n...\nContainerfile created successfully in /tmp/tmp.viA3a3Rdsg/ContainerfileFROM python:3.10-slim\n...\n"})}),(0,a.jsxs)(n.p,{children:["You can now edit ~/meta-llama/llama-stack/tmp/configs/ollama-run.yaml and run ",(0,a.jsx)(n.code,{children:"llama stack run ~/meta-llama/llama-stack/tmp/configs/ollama-run.yaml"})]}),(0,a.jsx)(n.p,{children:"Now set some environment variables for the inference model ID and Llama Stack Port and create a local directory to mount into the container's file system."}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'export INFERENCE_MODEL="llama3.2:3b"\nexport LLAMA_STACK_PORT=8321\nmkdir -p ~/.llama\n'})}),(0,a.jsx)(n.p,{children:"After this step is successful, you should be able to find the built container image and test it with the below Docker command:"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"docker run -d \\\n  -p $LLAMA_STACK_PORT:$LLAMA_STACK_PORT \\\n  -v ~/.llama:/root/.llama \\\n  localhost/distribution-ollama:dev \\\n  --port $LLAMA_STACK_PORT \\\n  --env INFERENCE_MODEL=$INFERENCE_MODEL \\\n  --env OLLAMA_URL=http://host.docker.internal:11434\n"})}),(0,a.jsx)(n.p,{children:"Here are the docker flags and their uses:"}),(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"-d"}),": Runs the container in the detached mode as a background process"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"-p $LLAMA_STACK_PORT:$LLAMA_STACK_PORT"}),": Maps the container port to the host port for accessing the server"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"-v ~/.llama:/root/.llama"}),": Mounts the local .llama directory to persist configurations and data"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"localhost/distribution-ollama:dev"}),": The name and tag of the container image to run"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"--port $LLAMA_STACK_PORT"}),": Port number for the server to listen on"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"--env INFERENCE_MODEL=$INFERENCE_MODEL"}),": Sets the model to use for inference"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"--env OLLAMA_URL=http://host.docker.internal:11434"}),": Configures the URL for the Ollama service"]}),"\n"]})]})]}),"\n",(0,a.jsx)(n.h2,{id:"running-your-stack-server",children:"Running Your Stack Server"}),"\n",(0,a.jsxs)(n.p,{children:["Now, let's start the Llama Stack Distribution Server. You will need the YAML configuration file which was written out at the end by the ",(0,a.jsx)(n.code,{children:"llama stack build"})," step."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"llama stack run -h\nusage: llama stack run [-h] [--port PORT] [--image-name IMAGE_NAME] [--env KEY=VALUE]\n                       [--image-type {venv}] [--enable-ui]\n                       [config | template]\n\nStart the server for a Llama Stack Distribution. You should have already built (or downloaded) and configured the distribution.\n\npositional arguments:\n  config | template     Path to config file to use for the run or name of known template (`llama stack list` for a list). (default: None)\n\noptions:\n  -h, --help            show this help message and exit\n  --port PORT           Port to run the server on. It can also be passed via the env var LLAMA_STACK_PORT. (default: 8321)\n  --image-name IMAGE_NAME\n                        Name of the image to run. Defaults to the current environment (default: None)\n  --env KEY=VALUE       Environment variables to pass to the server in KEY=VALUE format. Can be specified multiple times. (default: None)\n  --image-type {venv}\n                        Image Type used during the build. This should be venv. (default: None)\n  --enable-ui           Start the UI server (default: False)\n"})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Note:"})," Container images built with ",(0,a.jsx)(n.code,{children:"llama stack build --image-type container"})," cannot be run using ",(0,a.jsx)(n.code,{children:"llama stack run"}),". Instead, they must be run directly using Docker or Podman commands as shown in the container building section above."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Start using template name\nllama stack run tgi\n\n# Start using config file\nllama stack run ~/.llama/distributions/llamastack-my-local-stack/my-local-stack-run.yaml\n\n# Start using a venv\nllama stack run --image-type venv ~/.llama/distributions/llamastack-my-local-stack/my-local-stack-run.yaml\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"$ llama stack run ~/.llama/distributions/llamastack-my-local-stack/my-local-stack-run.yaml\n\nServing API inspect\n GET /health\n GET /providers/list\n GET /routes/list\nServing API inference\n POST /inference/chat_completion\n POST /inference/completion\n POST /inference/embeddings\n...\nServing API agents\n POST /agents/create\n POST /agents/session/create\n POST /agents/turn/create\n POST /agents/delete\n POST /agents/session/delete\n POST /agents/session/get\n POST /agents/step/get\n POST /agents/turn/get\n\nListening on ['::', '0.0.0.0']:8321\nINFO:     Started server process [2935911]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://['::', '0.0.0.0']:8321 (Press CTRL+C to quit)\nINFO:     2401:db00:35c:2d2b:face:0:c9:0:54678 - \"GET /models/list HTTP/1.1\" 200 OK\n"})}),"\n",(0,a.jsx)(n.h2,{id:"managing-distributions",children:"Managing Distributions"}),"\n",(0,a.jsx)(n.h3,{id:"listing-distributions",children:"Listing Distributions"}),"\n",(0,a.jsx)(n.p,{children:"Using the list command, you can view all existing Llama Stack distributions, including stacks built from templates, from scratch, or using custom configuration files."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"llama stack list -h\nusage: llama stack list [-h]\n\nlist the build stacks\n\noptions:\n  -h, --help  show this help message and exit\n"})}),"\n",(0,a.jsx)(n.p,{children:"Example Usage:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"llama stack list\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"------------------------------+-----------------------------------------------------------------+--------------+------------+\n| Stack Name                  | Path                                                            | Build Config | Run Config |\n+------------------------------+-----------------------------------------------------------------------------+--------------+\n| together                    | ~/.llama/distributions/together                                 | Yes          | No         |\n+------------------------------+-----------------------------------------------------------------------------+--------------+\n| bedrock                     | ~/.llama/distributions/bedrock                                  | Yes          | No         |\n+------------------------------+-----------------------------------------------------------------------------+--------------+\n| starter                     | ~/.llama/distributions/starter                                  | Yes          | Yes        |\n+------------------------------+-----------------------------------------------------------------------------+--------------+\n| remote-vllm                 | ~/.llama/distributions/remote-vllm                              | Yes          | Yes        |\n+------------------------------+-----------------------------------------------------------------------------+--------------+\n"})}),"\n",(0,a.jsx)(n.h3,{id:"removing-a-distribution",children:"Removing a Distribution"}),"\n",(0,a.jsx)(n.p,{children:"Use the remove command to delete a distribution you've previously built."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"llama stack rm -h\nusage: llama stack rm [-h] [--all] [name]\n\nRemove the build stack\n\npositional arguments:\n  name        Name of the stack to delete (default: None)\n\noptions:\n  -h, --help  show this help message and exit\n  --all, -a   Delete all stacks (use with caution) (default: False)\n"})}),"\n",(0,a.jsx)(n.p,{children:"Example:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"llama stack rm llamastack-test\n"})}),"\n",(0,a.jsxs)(n.p,{children:["To keep your environment organized and avoid clutter, consider using ",(0,a.jsx)(n.code,{children:"llama stack list"})," to review old or unused distributions and ",(0,a.jsx)(n.code,{children:"llama stack rm <name>"})," to delete them when they're no longer needed."]}),"\n",(0,a.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,a.jsxs)(n.p,{children:["If you encounter any issues, ask questions in our discord or search through our ",(0,a.jsx)(n.a,{href:"https://github.com/meta-llama/llama-stack/issues",children:"GitHub Issues"}),", or file an new issue."]})]})}function m(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(h,{...e})}):h(e)}}}]);