"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[2117],{4865:(e,n,t)=>{t.d(n,{A:()=>m});var a=t(96540),s=t(34164),i=t(23104),o=t(47751),l=t(92303);const r={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var c=t(74848);function d(e){var n=e.className,t=e.block,a=e.selectedValue,o=e.selectValue,l=e.tabValues,d=[],u=(0,i.a_)().blockElementScrollPositionUntilNextRender,h=function(e){var n=e.currentTarget,t=d.indexOf(n),s=l[t].value;s!==a&&(u(n),o(s))},m=function(e){var n,t=null;switch(e.key){case"Enter":h(e);break;case"ArrowRight":var a,s=d.indexOf(e.currentTarget)+1;t=null!=(a=d[s])?a:d[0];break;case"ArrowLeft":var i,o=d.indexOf(e.currentTarget)-1;t=null!=(i=d[o])?i:d[d.length-1]}null==(n=t)||n.focus()};return(0,c.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,s.A)("tabs",{"tabs--block":t},n),children:l.map(function(e){var n=e.value,t=e.label,i=e.attributes;return(0,c.jsx)("li",Object.assign({role:"tab",tabIndex:a===n?0:-1,"aria-selected":a===n,ref:function(e){d.push(e)},onKeyDown:m,onClick:h},i,{className:(0,s.A)("tabs__item",r.tabItem,null==i?void 0:i.className,{"tabs__item--active":a===n}),children:null!=t?t:n}),n)})})}function u(e){var n=e.lazy,t=e.children,i=e.selectedValue,o=(Array.isArray(t)?t:[t]).filter(Boolean);if(n){var l=o.find(function(e){return e.props.value===i});return l?(0,a.cloneElement)(l,{className:(0,s.A)("margin-top--md",l.props.className)}):null}return(0,c.jsx)("div",{className:"margin-top--md",children:o.map(function(e,n){return(0,a.cloneElement)(e,{key:n,hidden:e.props.value!==i})})})}function h(e){var n=(0,o.u)(e);return(0,c.jsxs)("div",{className:(0,s.A)("tabs-container",r.tabList),children:[(0,c.jsx)(d,Object.assign({},n,e)),(0,c.jsx)(u,Object.assign({},n,e))]})}function m(e){var n=(0,l.default)();return(0,c.jsx)(h,Object.assign({},e,{children:(0,o.v)(e.children)}),String(n))}},67415:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>c,default:()=>m,frontMatter:()=>r,metadata:()=>a,toc:()=>u});const a=JSON.parse('{"id":"getting-started/detailed-tutorial","title":"Detailed Tutorial","description":"Complete guide to using Llama Stack server and client SDK to build AI agents","source":"@site/docs/getting-started/detailed-tutorial.mdx","sourceDirName":"getting-started","slug":"/getting-started/detailed-tutorial","permalink":"/llama-stack/docs/getting-started/detailed-tutorial","draft":false,"unlisted":false,"editUrl":"https://github.com/meta-llama/llama-stack/tree/main/docs/docs/getting-started/detailed-tutorial.mdx","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Detailed Tutorial","description":"Complete guide to using Llama Stack server and client SDK to build AI agents","sidebar_label":"Detailed Tutorial","sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Quickstart","permalink":"/llama-stack/docs/getting-started/"},"next":{"title":"Libraries","permalink":"/llama-stack/docs/getting-started/libraries"}}');var s=t(74848),i=t(28453),o=t(4865),l=t(19365);const r={title:"Detailed Tutorial",description:"Complete guide to using Llama Stack server and client SDK to build AI agents",sidebar_label:"Detailed Tutorial",sidebar_position:3},c=void 0,d={},u=[{value:"Step 1: Installation and Setup",id:"step-1-installation-and-setup",level:3},{value:"Step 2:  Run Llama Stack",id:"step-2--run-llama-stack",level:3},{value:"Step 3: Run Client CLI",id:"step-3-run-client-cli",level:3},{value:"Step 4: Run the Demos",id:"step-4-run-the-demos",level:3},{value:"i. Create the Script",id:"i-create-the-script",level:4},{value:"ii. Run the Script",id:"ii-run-the-script",level:4},{value:"i. Create the Script",id:"i-create-the-script-1",level:4},{value:"ii. Run the Script",id:"ii-run-the-script-1",level:3},{value:"i. Create the Script",id:"i-create-the-script-2",level:4},{value:"ii. Run the Script",id:"ii-run-the-script-2",level:4}];function h(e){const n={a:"a",admonition:"admonition",code:"code",h3:"h3",h4:"h4",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components},{Details:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.p,{children:"In this guide, we'll walk through how you can use the Llama Stack (server and client SDK) to test a simple agent.\nA Llama Stack agent is a simple integrated system that can perform tasks by combining a Llama model for reasoning with\ntools (e.g., RAG, web search, code execution, etc.) for taking actions.\nIn Llama Stack, we provide a server exposing multiple APIs. These APIs are backed by implementations from different providers."}),"\n",(0,s.jsx)(n.p,{children:"Llama Stack is a stateful service with REST APIs to support seamless transition of AI applications across different environments. The server can be run in a variety of ways, including as a standalone binary, Docker container, or hosted service. You can build and test using a local server first and deploy to a hosted endpoint for production."}),"\n",(0,s.jsxs)(n.p,{children:["In this guide, we'll walk through how to build a RAG agent locally using Llama Stack with ",(0,s.jsx)(n.a,{href:"https://ollama.com/",children:"Ollama"}),"\nas the inference ",(0,s.jsx)(n.a,{href:"/docs/providers#inference",children:"provider"})," for a Llama Model."]}),"\n",(0,s.jsx)(n.h3,{id:"step-1-installation-and-setup",children:"Step 1: Installation and Setup"}),"\n",(0,s.jsxs)(n.p,{children:["Install Ollama by following the instructions on the ",(0,s.jsx)(n.a,{href:"https://ollama.com/download",children:"Ollama website"}),", then\ndownload Llama 3.2 3B model, and then start the Ollama service."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ollama pull llama3.2:3b\nollama run llama3.2:3b --keepalive 60m\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Install ",(0,s.jsx)(n.a,{href:"https://docs.astral.sh/uv/",children:"uv"})," to setup your virtual environment"]}),"\n",(0,s.jsxs)(o.A,{children:[(0,s.jsxs)(l.default,{value:"macos",label:"macOS and Linux",children:[(0,s.jsxs)(n.p,{children:["Use ",(0,s.jsx)(n.code,{children:"curl"})," to download the script and execute it with ",(0,s.jsx)(n.code,{children:"sh"}),":"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-console",children:"curl -LsSf https://astral.sh/uv/install.sh | sh\n"})})]}),(0,s.jsxs)(l.default,{value:"windows",label:"Windows",children:[(0,s.jsxs)(n.p,{children:["Use ",(0,s.jsx)(n.code,{children:"irm"})," to download the script and execute it with ",(0,s.jsx)(n.code,{children:"iex"}),":"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-console",children:'powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"\n'})})]})]}),"\n",(0,s.jsx)(n.p,{children:"Setup your virtual environment."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"uv sync --python 3.12\nsource .venv/bin/activate\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-2--run-llama-stack",children:"Step 2:  Run Llama Stack"}),"\n",(0,s.jsx)(n.p,{children:"Llama Stack is a server that exposes multiple APIs, you connect with it using the Llama Stack client SDK."}),"\n",(0,s.jsxs)(o.A,{children:[(0,s.jsxs)(l.default,{value:"venv1",label:"Using venv",children:[(0,s.jsx)(n.p,{children:"You can use Python to build and run the Llama Stack server, which is useful for testing and development."}),(0,s.jsxs)(n.p,{children:["Llama Stack uses a ",(0,s.jsx)(n.a,{href:"/docs/distributions/configuration",children:"YAML configuration file"})," to specify the stack setup,\nwhich defines the providers and their settings. The generated configuration serves as a starting point that you can ",(0,s.jsx)(n.a,{href:"/docs/distributions/customizing-run-yaml",children:"customize for your specific needs"}),".\nNow let's build and run the Llama Stack config for Ollama.\nWe use ",(0,s.jsx)(n.code,{children:"starter"})," as template. By default all providers are disabled, this requires enable ollama by passing environment variables."]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"llama stack build --distro starter --image-type venv --run\n"})})]}),(0,s.jsxs)(l.default,{value:"container",label:"Using a Container",children:[(0,s.jsxs)(n.p,{children:["You can use a container image to run the Llama Stack server. We provide several container images for the server\ncomponent that works with different inference providers out of the box. For this guide, we will use\n",(0,s.jsx)(n.code,{children:"llamastack/distribution-starter"})," as the container image. If you'd like to build your own image or customize the\nconfigurations, please check out ",(0,s.jsx)(n.a,{href:"/docs/distributions/building-distro",children:"this guide"}),"."]}),(0,s.jsx)(n.p,{children:"First lets setup some environment variables and create a local directory to mount into the container's file system."}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"export LLAMA_STACK_PORT=8321\nmkdir -p ~/.llama\n"})}),(0,s.jsx)(n.p,{children:"Then start the server using the container tool of your choice.  For example, if you are running Docker you can use the\nfollowing command:"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"docker run -it \\\n  --pull always \\\n  -p $LLAMA_STACK_PORT:$LLAMA_STACK_PORT \\\n  -v ~/.llama:/root/.llama \\\n  llamastack/distribution-starter \\\n  --port $LLAMA_STACK_PORT \\\n  --env OLLAMA_URL=http://host.docker.internal:11434\n"})}),(0,s.jsxs)(n.p,{children:["Note to start the container with Podman, you can do the same but replace ",(0,s.jsx)(n.code,{children:"docker"})," at the start of the command with\n",(0,s.jsx)(n.code,{children:"podman"}),". If you are using ",(0,s.jsx)(n.code,{children:"podman"})," older than ",(0,s.jsx)(n.code,{children:"4.7.0"}),", please also replace ",(0,s.jsx)(n.code,{children:"host.docker.internal"})," in the ",(0,s.jsx)(n.code,{children:"OLLAMA_URL"}),"\nwith ",(0,s.jsx)(n.code,{children:"host.containers.internal"}),"."]}),(0,s.jsxs)(n.p,{children:["The configuration YAML for the Ollama distribution is available at ",(0,s.jsx)(n.code,{children:"distributions/ollama/run.yaml"}),"."]}),(0,s.jsxs)(n.admonition,{type:"tip",children:[(0,s.jsxs)(n.p,{children:["Docker containers run in their own isolated network namespaces on Linux. To allow the container to communicate with services running on the host via ",(0,s.jsx)(n.code,{children:"localhost"}),", you need ",(0,s.jsx)(n.code,{children:"--network=host"}),". This makes the container use the host's network directly so it can connect to Ollama running on ",(0,s.jsx)(n.code,{children:"localhost:11434"}),"."]}),(0,s.jsx)(n.p,{children:"Linux users having issues running the above command should instead try the following:"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"docker run -it \\\n  --pull always \\\n  -p $LLAMA_STACK_PORT:$LLAMA_STACK_PORT \\\n  -v ~/.llama:/root/.llama \\\n  --network=host \\\n  llamastack/distribution-starter \\\n  --port $LLAMA_STACK_PORT \\\n  --env OLLAMA_URL=http://localhost:11434\n"})})]})]})]}),"\n",(0,s.jsx)(n.p,{children:"You will see output like below:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"INFO:     Application startup complete.\nINFO:     Uvicorn running on http://['::', '0.0.0.0']:8321 (Press CTRL+C to quit)\n"})}),"\n",(0,s.jsx)(n.p,{children:"Now you can use the Llama Stack client to run inference and build agents!"}),"\n",(0,s.jsxs)(n.p,{children:["You can reuse the server setup or use the ",(0,s.jsx)(n.a,{href:"https://github.com/meta-llama/llama-stack-client-python/",children:"Llama Stack Client"}),".\nNote that the client package is already included in the ",(0,s.jsx)(n.code,{children:"llama-stack"})," package."]}),"\n",(0,s.jsx)(n.h3,{id:"step-3-run-client-cli",children:"Step 3: Run Client CLI"}),"\n",(0,s.jsx)(n.p,{children:"Open a new terminal and navigate to the same directory you started the server from. Then set up a new or activate your\nexisting server virtual environment."}),"\n",(0,s.jsxs)(o.A,{children:[(0,s.jsx)(l.default,{value:"reuse",label:"Reuse Server venv",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# The client is included in the llama-stack package so we just activate the server venv\nsource .venv/bin/activate\n"})})}),(0,s.jsx)(l.default,{value:"install",label:"Install with venv",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"uv venv client --python 3.12\nsource client/bin/activate\npip install llama-stack-client\n"})})})]}),"\n",(0,s.jsxs)(n.p,{children:["Now let's use the ",(0,s.jsx)(n.code,{children:"llama-stack-client"})," ",(0,s.jsx)(n.a,{href:"/docs/references/llama-stack-client-cli-reference",children:"CLI"})," to check the\nconnectivity to the server."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"llama-stack-client configure --endpoint http://localhost:8321 --api-key none\n"})}),"\n",(0,s.jsx)(n.p,{children:"You will see the below:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Done! You can now use the Llama Stack Client CLI with endpoint http://localhost:8321\n"})}),"\n",(0,s.jsx)(n.p,{children:"List the models"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"llama-stack-client models list\nAvailable Models\n\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 model_type      \u2503 identifier                          \u2503 provider_resource_id                \u2503 metadata                                  \u2503 provider_id           \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 embedding       \u2502 ollama/all-minilm:l6-v2             \u2502 all-minilm:l6-v2                    \u2502 {'embedding_dimension': 384.0}            \u2502 ollama                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 ...             \u2502 ...                                 \u2502 ...                                 \u2502                                           \u2502 ...                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 llm             \u2502 ollama/Llama-3.2:3b                 \u2502 llama3.2:3b                         \u2502                                           \u2502 ollama                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n"})}),"\n",(0,s.jsx)(n.p,{children:"You can test basic Llama inference completion using the CLI."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'llama-stack-client inference chat-completion --model-id "ollama/llama3.2:3b" --message "tell me a joke"\n\n'})}),"\n",(0,s.jsx)(n.p,{children:"Sample output:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'OpenAIChatCompletion(\n    id="chatcmpl-08d7b2be-40f3-47ed-8f16-a6f29f2436af",\n    choices=[\n        OpenAIChatCompletionChoice(\n            finish_reason="stop",\n            index=0,\n            message=OpenAIChatCompletionChoiceMessageOpenAIAssistantMessageParam(\n                role="assistant",\n                content="Why couldn\'t the bicycle stand up by itself?\\n\\nBecause it was two-tired.",\n                name=None,\n                tool_calls=None,\n                refusal=None,\n                annotations=None,\n                audio=None,\n                function_call=None,\n            ),\n            logprobs=None,\n        )\n    ],\n    created=1751725254,\n    model="llama3.2:3b",\n    object="chat.completion",\n    service_tier=None,\n    system_fingerprint="fp_ollama",\n    usage={\n        "completion_tokens": 18,\n        "prompt_tokens": 29,\n        "total_tokens": 47,\n        "completion_tokens_details": None,\n        "prompt_tokens_details": None,\n    },\n)\n'})}),"\n",(0,s.jsx)(n.h3,{id:"step-4-run-the-demos",children:"Step 4: Run the Demos"}),"\n",(0,s.jsxs)(n.p,{children:["Note that these demos show the ",(0,s.jsx)(n.a,{href:"/docs/references/python-sdk-reference",children:"Python Client SDK"}),".\nOther SDKs are also available, please refer to the ",(0,s.jsx)(n.a,{href:"/docs/getting-started#client-sdks",children:"Client SDK"})," list for the complete options."]}),"\n",(0,s.jsxs)(o.A,{children:[(0,s.jsxs)(l.default,{value:"basic",label:"Basic Inference",children:[(0,s.jsx)(n.p,{children:"Now you can run inference using the Llama Stack client SDK."}),(0,s.jsx)(n.h4,{id:"i-create-the-script",children:"i. Create the Script"}),(0,s.jsxs)(n.p,{children:["Create a file ",(0,s.jsx)(n.code,{children:"inference.py"})," and add the following code:"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from llama_stack_client import LlamaStackClient\n\nclient = LlamaStackClient(base_url="http://localhost:8321")\n\n# List available models\nmodels = client.models.list()\n\n# Select the first LLM\nllm = next(m for m in models if m.model_type == "llm" and m.provider_id == "ollama")\nmodel_id = llm.identifier\n\nprint("Model:", model_id)\n\nresponse = client.chat.completions.create(\n    model=model_id,\n    messages=[\n        {"role": "system", "content": "You are a helpful assistant."},\n        {"role": "user", "content": "Write a haiku about coding"},\n    ],\n)\nprint(response)\n'})}),(0,s.jsx)(n.h4,{id:"ii-run-the-script",children:"ii. Run the Script"}),(0,s.jsxs)(n.p,{children:["Let's run the script using ",(0,s.jsx)(n.code,{children:"uv"})]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"uv run python inference.py\n"})}),(0,s.jsx)(n.p,{children:"Which will output:"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Model: ollama/llama3.2:3b\nOpenAIChatCompletion(id='chatcmpl-30cd0f28-a2ad-4b6d-934b-13707fc60ebf', choices=[OpenAIChatCompletionChoice(finish_reason='stop', index=0, message=OpenAIChatCompletionChoiceMessageOpenAIAssistantMessageParam(role='assistant', content=\"Lines of code unfold\\nAlgorithms dance with ease\\nLogic's gentle kiss\", name=None, tool_calls=None, refusal=None, annotations=None, audio=None, function_call=None), logprobs=None)], created=1751732480, model='llama3.2:3b', object='chat.completion', service_tier=None, system_fingerprint='fp_ollama', usage={'completion_tokens': 16, 'prompt_tokens': 37, 'total_tokens': 53, 'completion_tokens_details': None, 'prompt_tokens_details': None})\n"})})]}),(0,s.jsxs)(l.default,{value:"simple",label:"Build a Simple Agent",children:[(0,s.jsx)(n.p,{children:"Next we can move beyond simple inference and build an agent that can perform tasks using the Llama Stack server."}),(0,s.jsx)(n.h4,{id:"i-create-the-script-1",children:"i. Create the Script"}),(0,s.jsxs)(n.p,{children:["Create a file ",(0,s.jsx)(n.code,{children:"agent.py"})," and add the following code:"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from llama_stack_client import LlamaStackClient\nfrom llama_stack_client import Agent, AgentEventLogger\nfrom rich.pretty import pprint\nimport uuid\n\nclient = LlamaStackClient(base_url=f"http://localhost:8321")\n\nmodels = client.models.list()\nllm = next(m for m in models if m.model_type == "llm" and m.provider_id == "ollama")\nmodel_id = llm.identifier\n\nagent = Agent(client, model=model_id, instructions="You are a helpful assistant.")\n\ns_id = agent.create_session(session_name=f"s{uuid.uuid4().hex}")\n\nprint("Non-streaming ...")\nresponse = agent.create_turn(\n    messages=[{"role": "user", "content": "Who are you?"}],\n    session_id=s_id,\n    stream=False,\n)\nprint("agent>", response.output_message.content)\n\nprint("Streaming ...")\nstream = agent.create_turn(\n    messages=[{"role": "user", "content": "Who are you?"}], session_id=s_id, stream=True\n)\nfor event in stream:\n    pprint(event)\n\nprint("Streaming with print helper...")\nstream = agent.create_turn(\n    messages=[{"role": "user", "content": "Who are you?"}], session_id=s_id, stream=True\n)\nfor event in AgentEventLogger().log(stream):\n    event.print()\n'})}),(0,s.jsx)(n.h3,{id:"ii-run-the-script-1",children:"ii. Run the Script"}),(0,s.jsxs)(n.p,{children:["Let's run the script using ",(0,s.jsx)(n.code,{children:"uv"})]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"uv run python agent.py\n"})}),(0,s.jsxs)(t,{children:[(0,s.jsxs)(n.p,{children:[(0,s.jsx)("summary",{children:"\ud83d\udc4b Click here to see the sample output"}),"\nNon-streaming ...\nagent> I'm an artificial intelligence designed to assist and communicate with users like you. I don't have a personal identity, but I can provide information, answer questions, and help with tasks to the best of my abilities."]}),(0,s.jsx)(n.p,{children:"I'm a large language model, which means I've been trained on a massive dataset of text from various sources, allowing me to understand and respond to a wide range of topics and questions. My purpose is to provide helpful and accurate information, and I'm constantly learning and improving my responses based on the interactions I have with users like you."}),(0,s.jsx)(n.p,{children:"I can help with:"}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Answering questions on various subjects"}),"\n",(0,s.jsx)(n.li,{children:"Providing definitions and explanations"}),"\n",(0,s.jsx)(n.li,{children:"Offering suggestions and ideas"}),"\n",(0,s.jsx)(n.li,{children:"Assisting with language-related tasks, such as proofreading and editing"}),"\n",(0,s.jsx)(n.li,{children:"Generating text and content"}),"\n",(0,s.jsx)(n.li,{children:"And more!"}),"\n"]}),(0,s.jsxs)(n.p,{children:["Feel free to ask me anything, and I'll do my best to help!\nStreaming ...\nAgentTurnResponseStreamChunk(\n\u2502   event=TurnResponseEvent(\n\u2502   \u2502   payload=AgentTurnResponseStepStartPayload(\n\u2502   \u2502   \u2502   event_type='step_start',\n\u2502   \u2502   \u2502   step_id='69831607-fa75-424a-949b-e2049e3129d1',\n\u2502   \u2502   \u2502   step_type='inference',\n\u2502   \u2502   \u2502   metadata=","\n\u2502   \u2502   )\n\u2502   )\n)\nAgentTurnResponseStreamChunk(\n\u2502   event=TurnResponseEvent(\n\u2502   \u2502   payload=AgentTurnResponseStepProgressPayload(\n\u2502   \u2502   \u2502   delta=TextDelta(text='As', type='text'),\n\u2502   \u2502   \u2502   event_type='step_progress',\n\u2502   \u2502   \u2502   step_id='69831607-fa75-424a-949b-e2049e3129d1',\n\u2502   \u2502   \u2502   step_type='inference'\n\u2502   \u2502   )\n\u2502   )\n)\nAgentTurnResponseStreamChunk(\n\u2502   event=TurnResponseEvent(\n\u2502   \u2502   payload=AgentTurnResponseStepProgressPayload(\n\u2502   \u2502   \u2502   delta=TextDelta(text=' a', type='text'),\n\u2502   \u2502   \u2502   event_type='step_progress',\n\u2502   \u2502   \u2502   step_id='69831607-fa75-424a-949b-e2049e3129d1',\n\u2502   \u2502   \u2502   step_type='inference'\n\u2502   \u2502   )\n\u2502   )\n)\n...\nAgentTurnResponseStreamChunk(\n\u2502   event=TurnResponseEvent(\n\u2502   \u2502   payload=AgentTurnResponseStepCompletePayload(\n\u2502   \u2502   \u2502   event_type='step_complete',\n\u2502   \u2502   \u2502   step_details=InferenceStep(\n\u2502   \u2502   \u2502   \u2502   api_model_response=CompletionMessage(\n\u2502   \u2502   \u2502   \u2502   \u2502   content='As a conversational AI, I don't have a personal identity in the classical sense. I exist as a program running on computer servers, designed to process and respond to text-based inputs.\\n\\nI'm an instance of a type of artificial intelligence called a \"language model,\" which is trained on vast amounts of text data to generate human-like responses. My primary function is to understand and respond to natural language inputs, like our conversation right now.\\n\\nThink of me as a virtual assistant, a chatbot, or a conversational interface \u2013 I'm here to provide information, answer questions, and engage in conversation to the best of my abilities. I don't have feelings, emotions, or consciousness like humans do, but I'm designed to simulate human-like interactions to make our conversations feel more natural and helpful.\\n\\nSo, that's me in a nutshell! What can I help you with today?',\n\u2502   \u2502   \u2502   \u2502   \u2502   role='assistant',\n\u2502   \u2502   \u2502   \u2502   \u2502   stop_reason='end_of_turn',\n\u2502   \u2502   \u2502   \u2502   \u2502   tool_calls=[]\n\u2502   \u2502   \u2502   \u2502   ),\n\u2502   \u2502   \u2502   \u2502   step_id='69831607-fa75-424a-949b-e2049e3129d1',\n\u2502   \u2502   \u2502   \u2502   step_type='inference',\n\u2502   \u2502   \u2502   \u2502   turn_id='8b360202-f7cb-4786-baa9-166a1b46e2ca',\n\u2502   \u2502   \u2502   \u2502   completed_at=datetime.datetime(2025, 4, 3, 1, 15, 21, 716174, tzinfo=TzInfo(UTC)),\n\u2502   \u2502   \u2502   \u2502   started_at=datetime.datetime(2025, 4, 3, 1, 15, 14, 28823, tzinfo=TzInfo(UTC))\n\u2502   \u2502   \u2502   ),\n\u2502   \u2502   \u2502   step_id='69831607-fa75-424a-949b-e2049e3129d1',\n\u2502   \u2502   \u2502   step_type='inference'\n\u2502   \u2502   )\n\u2502   )\n)\nAgentTurnResponseStreamChunk(\n\u2502   event=TurnResponseEvent(\n\u2502   \u2502   payload=AgentTurnResponseTurnCompletePayload(\n\u2502   \u2502   \u2502   event_type='turn_complete',\n\u2502   \u2502   \u2502   turn=Turn(\n\u2502   \u2502   \u2502   \u2502   input_messages=[UserMessage(content='Who are you?', role='user', context=None)],\n\u2502   \u2502   \u2502   \u2502   output_message=CompletionMessage(\n\u2502   \u2502   \u2502   \u2502   \u2502   content='As a conversational AI, I don't have a personal identity in the classical sense. I exist as a program running on computer servers, designed to process and respond to text-based inputs.\\n\\nI'm an instance of a type of artificial intelligence called a \"language model,\" which is trained on vast amounts of text data to generate human-like responses. My primary function is to understand and respond to natural language inputs, like our conversation right now.\\n\\nThink of me as a virtual assistant, a chatbot, or a conversational interface \u2013 I'm here to provide information, answer questions, and engage in conversation to the best of my abilities. I don't have feelings, emotions, or consciousness like humans do, but I'm designed to simulate human-like interactions to make our conversations feel more natural and helpful.\\n\\nSo, that's me in a nutshell! What can I help you with today?',\n\u2502   \u2502   \u2502   \u2502   \u2502   role='assistant',\n\u2502   \u2502   \u2502   \u2502   \u2502   stop_reason='end_of_turn',\n\u2502   \u2502   \u2502   \u2502   \u2502   tool_calls=[]\n\u2502   \u2502   \u2502   \u2502   ),\n\u2502   \u2502   \u2502   \u2502   session_id='abd4afea-4324-43f4-9513-cfe3970d92e8',\n\u2502   \u2502   \u2502   \u2502   started_at=datetime.datetime(2025, 4, 3, 1, 15, 14, 28722, tzinfo=TzInfo(UTC)),\n\u2502   \u2502   \u2502   \u2502   steps=[\n\u2502   \u2502   \u2502   \u2502   \u2502   InferenceStep(\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   api_model_response=CompletionMessage(\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   content='As a conversational AI, I don't have a personal identity in the classical sense. I exist as a program running on computer servers, designed to process and respond to text-based inputs.\\n\\nI'm an instance of a type of artificial intelligence called a \"language model,\" which is trained on vast amounts of text data to generate human-like responses. My primary function is to understand and respond to natural language inputs, like our conversation right now.\\n\\nThink of me as a virtual assistant, a chatbot, or a conversational interface \u2013 I'm here to provide information, answer questions, and engage in conversation to the best of my abilities. I don't have feelings, emotions, or consciousness like humans do, but I'm designed to simulate human-like interactions to make our conversations feel more natural and helpful.\\n\\nSo, that's me in a nutshell! What can I help you with today?',\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   role='assistant',\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   stop_reason='end_of_turn',\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   tool_calls=[]\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   ),\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   step_id='69831607-fa75-424a-949b-e2049e3129d1',\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   step_type='inference',\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   turn_id='8b360202-f7cb-4786-baa9-166a1b46e2ca',\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   completed_at=datetime.datetime(2025, 4, 3, 1, 15, 21, 716174, tzinfo=TzInfo(UTC)),\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   started_at=datetime.datetime(2025, 4, 3, 1, 15, 14, 28823, tzinfo=TzInfo(UTC))\n\u2502   \u2502   \u2502   \u2502   \u2502   )\n\u2502   \u2502   \u2502   \u2502   ],\n\u2502   \u2502   \u2502   \u2502   turn_id='8b360202-f7cb-4786-baa9-166a1b46e2ca',\n\u2502   \u2502   \u2502   \u2502   completed_at=datetime.datetime(2025, 4, 3, 1, 15, 21, 727364, tzinfo=TzInfo(UTC)),\n\u2502   \u2502   \u2502   \u2502   output_attachments=[]\n\u2502   \u2502   \u2502   )\n\u2502   \u2502   )\n\u2502   )\n)"]}),(0,s.jsx)(n.p,{children:"Streaming with print helper...\ninference> D\xe9j\xe0 vu! You're asking me again!"}),(0,s.jsx)(n.p,{children:"As I mentioned earlier, I'm a computer program designed to simulate conversation and answer questions. I don't have a personal identity or consciousness like a human would. I exist solely as a digital entity, running on computer servers and responding to inputs from users like you."}),(0,s.jsx)(n.p,{children:"I'm a type of artificial intelligence (AI) called a large language model, which means I've been trained on a massive dataset of text from various sources. This training allows me to understand and respond to a wide range of questions and topics."}),(0,s.jsx)(n.p,{children:"My purpose is to provide helpful and accurate information, answer questions, and assist users like you with tasks and conversations. I don't have personal preferences, emotions, or opinions like humans do. My goal is to be informative, neutral, and respectful in my responses."}),(0,s.jsx)(n.p,{children:"So, that's me in a nutshell!"})]})]}),(0,s.jsxs)(l.default,{value:"rag",label:"Build a RAG Agent",children:[(0,s.jsx)(n.p,{children:"For our last demo, we can build a RAG agent that can answer questions about the Torchtune project using the documents\nin a vector database."}),(0,s.jsx)(n.h4,{id:"i-create-the-script-2",children:"i. Create the Script"}),(0,s.jsxs)(n.p,{children:["Create a file ",(0,s.jsx)(n.code,{children:"rag_agent.py"})," and add the following code:"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from llama_stack_client import LlamaStackClient\nfrom llama_stack_client import Agent, AgentEventLogger\nfrom llama_stack_client.types import Document\nimport uuid\n\nclient = LlamaStackClient(base_url="http://localhost:8321")\n\n# Create a vector database instance\nembed_lm = next(m for m in client.models.list() if m.model_type == "embedding")\nembedding_model = embed_lm.identifier\nvector_db_id = f"v{uuid.uuid4().hex}"\nclient.vector_dbs.register(\n    vector_db_id=vector_db_id,\n    embedding_model=embedding_model,\n)\n\n# Create Documents\nurls = [\n    "memory_optimizations.rst",\n    "chat.rst",\n    "llama3.rst",\n    "qat_finetune.rst",\n    "lora_finetune.rst",\n]\ndocuments = [\n    Document(\n        document_id=f"num-{i}",\n        content=f"https://raw.githubusercontent.com/pytorch/torchtune/main/docs/source/tutorials/{url}",\n        mime_type="text/plain",\n        metadata={},\n    )\n    for i, url in enumerate(urls)\n]\n\n# Insert documents\nclient.tool_runtime.rag_tool.insert(\n    documents=documents,\n    vector_db_id=vector_db_id,\n    chunk_size_in_tokens=512,\n)\n\n# Get the model being served\nllm = next(\n    m\n    for m in client.models.list()\n    if m.model_type == "llm" and m.provider_id == "ollama"\n)\nmodel = llm.identifier\n\n# Create the RAG agent\nrag_agent = Agent(\n    client,\n    model=model,\n    instructions="You are a helpful assistant. Use the RAG tool to answer questions as needed.",\n    tools=[\n        {\n            "name": "builtin::rag/knowledge_search",\n            "args": {"vector_db_ids": [vector_db_id]},\n        }\n    ],\n)\n\nsession_id = rag_agent.create_session(session_name=f"s{uuid.uuid4().hex}")\n\nturns = ["what is torchtune", "tell me about dora"]\n\nfor t in turns:\n    print("user>", t)\n    stream = rag_agent.create_turn(\n        messages=[{"role": "user", "content": t}], session_id=session_id, stream=True\n    )\n    for event in AgentEventLogger().log(stream):\n        event.print()\n'})}),(0,s.jsx)(n.h4,{id:"ii-run-the-script-2",children:"ii. Run the Script"}),(0,s.jsxs)(n.p,{children:["Let's run the script using ",(0,s.jsx)(n.code,{children:"uv"})]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"uv run python rag_agent.py\n"})}),(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:"\ud83d\udc4b Click here to see the sample output"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"user> what is torchtune\ninference> [knowledge_search(query='TorchTune')]\ntool_execution> Tool:knowledge_search Args:{'query': 'TorchTune'}\ntool_execution> Tool:knowledge_search Response:[TextContentItem(text='knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n', type='text'), TextContentItem(text='Result 1:\\nDocument_id:num-1\\nContent:  conversational data, :func:`~torchtune.datasets.chat_dataset` seems to be a good fit. ..., type='text'), TextContentItem(text='END of knowledge_search tool results.\\n', type='text')]\ninference> Here is a high-level overview of the text:\n\n**LoRA Finetuning with PyTorch Tune**\n\nPyTorch Tune provides a recipe for LoRA (Low-Rank Adaptation) finetuning, which is a technique to adapt pre-trained models to new tasks. The recipe uses the `lora_finetune_distributed` command.\n...\nOverall, DORA is a powerful reinforcement learning algorithm that can learn complex tasks from human demonstrations. However, it requires careful consideration of the challenges and limitations to achieve optimal results.\n"})})]})]})]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"You're Ready to Build Your Own Apps!"})}),"\n",(0,s.jsxs)(n.p,{children:["Congrats! \ud83e\udd73 Now you're ready to ",(0,s.jsx)(n.a,{href:"/docs/building-applications",children:"build your own Llama Stack applications"}),"! \ud83d\ude80"]})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}}}]);