"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[6059],{28666:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>d,frontMatter:()=>t,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"concepts/evaluation-concepts","title":"Evaluation Concepts","description":"The Llama Stack Evaluation flow allows you to run evaluations on your GenAI application datasets or pre-registered benchmarks.","source":"@site/docs/concepts/evaluation-concepts.mdx","sourceDirName":"concepts","slug":"/concepts/evaluation-concepts","permalink":"/docs/concepts/evaluation-concepts","draft":false,"unlisted":false,"editUrl":"https://github.com/meta-llama/llama-stack/tree/main/docs/docs/concepts/evaluation-concepts.mdx","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Resources","permalink":"/docs/concepts/resources"},"next":{"title":"Overview","permalink":"/docs/distributions/"}}');var i=s(74848),r=s(28453);const t={},l="Evaluation Concepts",o={},c=[{value:"Open-benchmark Eval",id:"open-benchmark-eval",level:2},{value:"List of open-benchmarks Llama Stack support",id:"list-of-open-benchmarks-llama-stack-support",level:3},{value:"Run evaluation on open-benchmarks via CLI",id:"run-evaluation-on-open-benchmarks-via-cli",level:3},{value:"Spin up Llama Stack server",id:"spin-up-llama-stack-server",level:4},{value:"Run eval CLI",id:"run-eval-cli",level:4},{value:"What&#39;s Next?",id:"whats-next",level:2}];function h(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"evaluation-concepts",children:"Evaluation Concepts"})}),"\n",(0,i.jsx)(n.p,{children:"The Llama Stack Evaluation flow allows you to run evaluations on your GenAI application datasets or pre-registered benchmarks."}),"\n",(0,i.jsx)(n.p,{children:"We introduce a set of APIs in Llama Stack for supporting running evaluations of LLM applications:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"/datasetio"})," + ",(0,i.jsx)(n.code,{children:"/datasets"})," API"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"/scoring"})," + ",(0,i.jsx)(n.code,{children:"/scoring_functions"})," API"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"/eval"})," + ",(0,i.jsx)(n.code,{children:"/benchmarks"})," API"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["This guide goes over the sets of APIs and developer experience flow of using Llama Stack to run evaluations for different use cases. Checkout our Colab notebook on working examples with evaluations ",(0,i.jsx)(n.a,{href:"https://colab.research.google.com/drive/10CHyykee9j2OigaIcRv47BKG9mrNm0tJ?usp=sharing",children:"here"}),"."]}),"\n",(0,i.jsxs)(n.p,{children:["The Evaluation APIs are associated with a set of Resources. Please visit the Resources section in our ",(0,i.jsx)(n.a,{href:"/docs/concepts/",children:"Core Concepts"})," guide for better high-level understanding."]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"DatasetIO"}),": defines interface with datasets and data loaders.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Associated with ",(0,i.jsx)(n.code,{children:"Dataset"})," resource."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Scoring"}),": evaluate outputs of the system.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Associated with ",(0,i.jsx)(n.code,{children:"ScoringFunction"})," resource. We provide a suite of out-of-the box scoring functions and also the ability for you to add custom evaluators. These scoring functions are the core part of defining an evaluation task to output evaluation metrics."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Eval"}),": generate outputs (via Inference or Agents) and perform scoring.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Associated with ",(0,i.jsx)(n.code,{children:"Benchmark"})," resource."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"open-benchmark-eval",children:"Open-benchmark Eval"}),"\n",(0,i.jsx)(n.h3,{id:"list-of-open-benchmarks-llama-stack-support",children:"List of open-benchmarks Llama Stack support"}),"\n",(0,i.jsx)(n.p,{children:"Llama stack pre-registers several popular open-benchmarks to easily evaluate model perfomance via CLI."}),"\n",(0,i.jsx)(n.p,{children:"The list of open-benchmarks we currently support:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://arxiv.org/abs/2009.03300",children:"MMLU-COT"})," (Measuring Massive Multitask Language Understanding): Benchmark designed to comprehensively evaluate the breadth and depth of a model's academic and professional understanding"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://arxiv.org/abs/2311.12022",children:"GPQA-COT"})," (A Graduate-Level Google-Proof Q&A Benchmark): A challenging benchmark of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://openai.com/index/introducing-simpleqa/",children:"SimpleQA"}),": Benchmark designed to access models to answer short, fact-seeking questions."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://arxiv.org/abs/2311.16502",children:"MMMU"})," (A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI)]: Benchmark designed to evaluate multimodal models."]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["You can follow this ",(0,i.jsx)(n.a,{href:"/docs/references/evals-reference#open-benchmark-contributing-guide",children:"contributing guide"})," to add more open-benchmarks to Llama Stack"]}),"\n",(0,i.jsx)(n.h3,{id:"run-evaluation-on-open-benchmarks-via-cli",children:"Run evaluation on open-benchmarks via CLI"}),"\n",(0,i.jsx)(n.p,{children:"We have built-in functionality to run the supported open-benckmarks using llama-stack-client CLI"}),"\n",(0,i.jsx)(n.h4,{id:"spin-up-llama-stack-server",children:"Spin up Llama Stack server"}),"\n",(0,i.jsx)(n.p,{children:"Spin up llama stack server with 'open-benchmark' template"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"llama stack run llama_stack/distributions/open-benchmark/run.yaml\n"})}),"\n",(0,i.jsx)(n.h4,{id:"run-eval-cli",children:"Run eval CLI"}),"\n",(0,i.jsx)(n.p,{children:"There are 3 necessary inputs to run a benchmark eval"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"list of benchmark_ids"}),": The list of benchmark ids to run evaluation on"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"model-id"}),": The model id to evaluate on"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"output_dir"}),": Path to store the evaluate results"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"llama-stack-client eval run-benchmark <benchmark_id_1> <benchmark_id_2> ... \\\n--model_id <model id to evaluate on> \\\n--output_dir <directory to store the evaluate results>\n"})}),"\n",(0,i.jsx)(n.p,{children:"You can run"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"llama-stack-client eval run-benchmark help\n"})}),"\n",(0,i.jsx)(n.p,{children:"to see the description of all the flags that eval run-benchmark has"}),"\n",(0,i.jsx)(n.p,{children:"In the output log, you can find the file path that has your evaluation results. Open that file and you can see you aggregate\nevaluation results over there."}),"\n",(0,i.jsx)(n.h2,{id:"whats-next",children:"What's Next?"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Check out our Colab notebook on working examples with running benchmark evaluations ",(0,i.jsx)(n.a,{href:"https://colab.research.google.com/github/meta-llama/llama-stack/blob/main/docs/notebooks/Llama_Stack_Benchmark_Evals.ipynb#scrollTo=mxLCsP4MvFqP",children:"here"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Check out our ",(0,i.jsx)(n.a,{href:"/docs/building-applications/evals",children:"Building Applications - Evaluation"})," guide for more details on how to use the Evaluation APIs to evaluate your applications."]}),"\n",(0,i.jsxs)(n.li,{children:["Check out our ",(0,i.jsx)(n.a,{href:"/docs/references/evals-reference",children:"Evaluation Reference"})," for more details on the APIs."]}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}}}]);