"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[483],{4865:(e,n,s)=>{s.d(n,{A:()=>p});var a=s(96540),t=s(34164),i=s(23104),r=s(47751),l=s(92303);const o={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var c=s(74848);function d(e){var n=e.className,s=e.block,a=e.selectedValue,r=e.selectValue,l=e.tabValues,d=[],u=(0,i.a_)().blockElementScrollPositionUntilNextRender,h=function(e){var n=e.currentTarget,s=d.indexOf(n),t=l[s].value;t!==a&&(u(n),r(t))},p=function(e){var n,s=null;switch(e.key){case"Enter":h(e);break;case"ArrowRight":var a,t=d.indexOf(e.currentTarget)+1;s=null!=(a=d[t])?a:d[0];break;case"ArrowLeft":var i,r=d.indexOf(e.currentTarget)-1;s=null!=(i=d[r])?i:d[d.length-1]}null==(n=s)||n.focus()};return(0,c.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,t.A)("tabs",{"tabs--block":s},n),children:l.map(function(e){var n=e.value,s=e.label,i=e.attributes;return(0,c.jsx)("li",Object.assign({role:"tab",tabIndex:a===n?0:-1,"aria-selected":a===n,ref:function(e){d.push(e)},onKeyDown:p,onClick:h},i,{className:(0,t.A)("tabs__item",o.tabItem,null==i?void 0:i.className,{"tabs__item--active":a===n}),children:null!=s?s:n}),n)})})}function u(e){var n=e.lazy,s=e.children,i=e.selectedValue,r=(Array.isArray(s)?s:[s]).filter(Boolean);if(n){var l=r.find(function(e){return e.props.value===i});return l?(0,a.cloneElement)(l,{className:(0,t.A)("margin-top--md",l.props.className)}):null}return(0,c.jsx)("div",{className:"margin-top--md",children:r.map(function(e,n){return(0,a.cloneElement)(e,{key:n,hidden:e.props.value!==i})})})}function h(e){var n=(0,r.u)(e);return(0,c.jsxs)("div",{className:(0,t.A)("tabs-container",o.tabList),children:[(0,c.jsx)(d,Object.assign({},n,e)),(0,c.jsx)(u,Object.assign({},n,e))]})}function p(e){var n=(0,l.default)();return(0,c.jsx)(h,Object.assign({},e,{children:(0,r.v)(e.children)}),String(n))}},24545:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>d,contentTitle:()=>c,default:()=>p,frontMatter:()=>o,metadata:()=>a,toc:()=>u});const a=JSON.parse('{"id":"building-applications/evals","title":"Evaluations","description":"Evaluate LLM applications with Llama Stack\'s comprehensive evaluation framework","source":"@site/docs/building-applications/evals.mdx","sourceDirName":"building-applications","slug":"/building-applications/evals","permalink":"/llama-stack/docs/building-applications/evals","draft":false,"unlisted":false,"editUrl":"https://github.com/meta-llama/llama-stack/tree/main/docs/docs/building-applications/evals.mdx","tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"title":"Evaluations","description":"Evaluate LLM applications with Llama Stack\'s comprehensive evaluation framework","sidebar_label":"Evaluations","sidebar_position":7},"sidebar":"tutorialSidebar","previous":{"title":"Tools","permalink":"/llama-stack/docs/building-applications/tools"},"next":{"title":"Telemetry","permalink":"/llama-stack/docs/building-applications/telemetry"}}');var t=s(74848),i=s(28453),r=s(4865),l=s(19365);const o={title:"Evaluations",description:"Evaluate LLM applications with Llama Stack's comprehensive evaluation framework",sidebar_label:"Evaluations",sidebar_position:7},c="Evaluations",d={},u=[{value:"Application Evaluation Example",id:"application-evaluation-example",level:2},{value:"Step-by-Step Evaluation Process",id:"step-by-step-evaluation-process",level:2},{value:"1. Building a Search Agent",id:"1-building-a-search-agent",level:3},{value:"2. Query Agent Execution Steps",id:"2-query-agent-execution-steps",level:3},{value:"3. Evaluate Agent Responses",id:"3-evaluate-agent-responses",level:3},{value:"Available Scoring Functions",id:"available-scoring-functions",level:2},{value:"Basic Scoring Functions",id:"basic-scoring-functions",level:3},{value:"Advanced Scoring Functions",id:"advanced-scoring-functions",level:3},{value:"Custom Scoring Functions",id:"custom-scoring-functions",level:3},{value:"Evaluation Workflow Best Practices",id:"evaluation-workflow-best-practices",level:2},{value:"\ud83c\udfaf <strong>Dataset Preparation</strong>",id:"-dataset-preparation",level:3},{value:"\ud83d\udcca <strong>Metrics Selection</strong>",id:"-metrics-selection",level:3},{value:"\ud83d\udd04 <strong>Iterative Improvement</strong>",id:"-iterative-improvement",level:3},{value:"\ud83d\udcc8 <strong>Analysis &amp; Reporting</strong>",id:"-analysis--reporting",level:3},{value:"Advanced Evaluation Scenarios",id:"advanced-evaluation-scenarios",level:2},{value:"Batch Evaluation",id:"batch-evaluation",level:3},{value:"Multi-Metric Evaluation",id:"multi-metric-evaluation",level:3},{value:"Related Resources",id:"related-resources",level:2}];function h(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"evaluations",children:"Evaluations"})}),"\n",(0,t.jsx)(n.p,{children:"The Llama Stack provides a comprehensive set of APIs for supporting evaluations of LLM applications:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsxs)(n.strong,{children:[(0,t.jsx)(n.code,{children:"/datasetio"})," + ",(0,t.jsx)(n.code,{children:"/datasets"})," API"]}),": Manage evaluation datasets and data input/output"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsxs)(n.strong,{children:[(0,t.jsx)(n.code,{children:"/scoring"})," + ",(0,t.jsx)(n.code,{children:"/scoring_functions"})," API"]}),": Apply scoring functions to evaluate responses"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsxs)(n.strong,{children:[(0,t.jsx)(n.code,{children:"/eval"})," + ",(0,t.jsx)(n.code,{children:"/benchmarks"})," API"]}),": Run benchmarks and structured evaluations"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["This guide walks you through the process of evaluating an LLM application built using Llama Stack. For detailed API reference, check out the ",(0,t.jsx)(n.a,{href:"/docs/references/evals-reference",children:"Evaluation Reference"})," guide that covers the complete set of APIs and developer experience flow."]}),"\n",(0,t.jsx)(n.admonition,{title:"Interactive Examples",type:"tip",children:(0,t.jsxs)(n.p,{children:["Check out our ",(0,t.jsx)(n.a,{href:"https://colab.research.google.com/drive/10CHyykee9j2OigaIcRv47BKG9mrNm0tJ?usp=sharing",children:"Colab notebook"})," for working examples with evaluations, or try the ",(0,t.jsx)(n.a,{href:"https://colab.research.google.com/github/meta-llama/llama-stack/blob/main/docs/getting_started.ipynb",children:"Getting Started notebook"}),"."]})}),"\n",(0,t.jsx)(n.h2,{id:"application-evaluation-example",children:"Application Evaluation Example"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"https://colab.research.google.com/github/meta-llama/llama-stack/blob/main/docs/getting_started.ipynb",children:(0,t.jsx)(n.img,{src:"https://colab.research.google.com/assets/colab-badge.svg",alt:"Open In Colab"})})}),"\n",(0,t.jsxs)(n.p,{children:["Llama Stack offers a library of scoring functions and the ",(0,t.jsx)(n.code,{children:"/scoring"})," API, allowing you to run evaluations on your pre-annotated AI application datasets."]}),"\n",(0,t.jsx)(n.p,{children:"In this example, we will show you how to:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Build an Agent"})," with Llama Stack"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Query the agent's sessions, turns, and steps"})," to analyze execution"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Evaluate the results"})," using scoring functions"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"step-by-step-evaluation-process",children:"Step-by-Step Evaluation Process"}),"\n",(0,t.jsx)(n.h3,{id:"1-building-a-search-agent",children:"1. Building a Search Agent"}),"\n",(0,t.jsx)(n.p,{children:"First, let's create an agent that can search the web to answer questions:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from llama_stack_client import LlamaStackClient, Agent, AgentEventLogger\n\nclient = LlamaStackClient(base_url=f"http://{HOST}:{PORT}")\n\nagent = Agent(\n    client,\n    model="meta-llama/Llama-3.3-70B-Instruct",\n    instructions="You are a helpful assistant. Use search tool to answer the questions.",\n    tools=["builtin::websearch"],\n)\n\n# Test prompts for evaluation\nuser_prompts = [\n    "Which teams played in the NBA Western Conference Finals of 2024. Search the web for the answer.",\n    "In which episode and season of South Park does Bill Cosby (BSM-471) first appear? Give me the number and title. Search the web for the answer.",\n    "What is the British-American kickboxer Andrew Tate\'s kickboxing name? Search the web for the answer.",\n]\n\nsession_id = agent.create_session("test-session")\n\n# Execute all prompts in the session\nfor prompt in user_prompts:\n    response = agent.create_turn(\n        messages=[\n            {\n                "role": "user",\n                "content": prompt,\n            }\n        ],\n        session_id=session_id,\n    )\n\n    for log in AgentEventLogger().log(response):\n        log.print()\n'})}),"\n",(0,t.jsx)(n.h3,{id:"2-query-agent-execution-steps",children:"2. Query Agent Execution Steps"}),"\n",(0,t.jsx)(n.p,{children:"Now, let's analyze the agent's execution steps to understand its performance:"}),"\n",(0,t.jsxs)(r.A,{children:[(0,t.jsx)(l.default,{value:"session-analysis",label:"Session Analysis",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from rich.pretty import pprint\n\n# Query the agent's session to get detailed execution data\nsession_response = client.agents.session.retrieve(\n    session_id=session_id,\n    agent_id=agent.agent_id,\n)\n\npprint(session_response)\n"})})}),(0,t.jsx)(l.default,{value:"tool-validation",label:"Tool Usage Validation",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Sanity check: Verify that all user prompts are followed by tool calls\nnum_tool_call = 0\nfor turn in session_response.turns:\n    for step in turn.steps:\n        if (\n            step.step_type == "tool_execution"\n            and step.tool_calls[0].tool_name == "brave_search"\n        ):\n            num_tool_call += 1\n\nprint(\n    f"{num_tool_call}/{len(session_response.turns)} user prompts are followed by a tool call to `brave_search`"\n)\n'})})})]}),"\n",(0,t.jsx)(n.h3,{id:"3-evaluate-agent-responses",children:"3. Evaluate Agent Responses"}),"\n",(0,t.jsx)(n.p,{children:"Now we'll evaluate the agent's responses using Llama Stack's scoring API:"}),"\n",(0,t.jsxs)(r.A,{children:[(0,t.jsx)(l.default,{value:"data-preparation",label:"Data Preparation",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Process agent execution history into evaluation rows\neval_rows = []\n\n# Define expected answers for our test prompts\nexpected_answers = [\n    "Dallas Mavericks and the Minnesota Timberwolves",\n    "Season 4, Episode 12", \n    "King Cobra",\n]\n\n# Create evaluation dataset from agent responses\nfor i, turn in enumerate(session_response.turns):\n    eval_rows.append(\n        {\n            "input_query": turn.input_messages[0].content,\n            "generated_answer": turn.output_message.content,\n            "expected_answer": expected_answers[i],\n        }\n    )\n\npprint(eval_rows)\n'})})}),(0,t.jsx)(l.default,{value:"scoring",label:"Scoring & Evaluation",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Configure scoring parameters\nscoring_params = {\n    "basic::subset_of": None,  # Check if generated answer contains expected answer\n}\n\n# Run evaluation using Llama Stack\'s scoring API\nscoring_response = client.scoring.score(\n    input_rows=eval_rows, \n    scoring_functions=scoring_params\n)\n\npprint(scoring_response)\n\n# Analyze results\nfor i, result in enumerate(scoring_response.results):\n    print(f"Query {i+1}: {result.score}")\n    print(f"  Generated: {eval_rows[i][\'generated_answer\'][:100]}...")\n    print(f"  Expected: {expected_answers[i]}")\n    print(f"  Score: {result.score}")\n    print()\n'})})})]}),"\n",(0,t.jsx)(n.h2,{id:"available-scoring-functions",children:"Available Scoring Functions"}),"\n",(0,t.jsx)(n.p,{children:"Llama Stack provides several built-in scoring functions:"}),"\n",(0,t.jsx)(n.h3,{id:"basic-scoring-functions",children:"Basic Scoring Functions"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"basic::subset_of"})}),": Checks if the expected answer is contained in the generated response"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"basic::exact_match"})}),": Performs exact string matching between expected and generated answers"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"basic::regex_match"})}),": Uses regular expressions to match patterns in responses"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"advanced-scoring-functions",children:"Advanced Scoring Functions"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"llm_as_judge::accuracy"})}),": Uses an LLM to judge response accuracy"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"llm_as_judge::helpfulness"})}),": Evaluates how helpful the response is"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"llm_as_judge::safety"})}),": Assesses response safety and appropriateness"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"custom-scoring-functions",children:"Custom Scoring Functions"}),"\n",(0,t.jsx)(n.p,{children:"You can also create custom scoring functions for domain-specific evaluation needs."}),"\n",(0,t.jsx)(n.h2,{id:"evaluation-workflow-best-practices",children:"Evaluation Workflow Best Practices"}),"\n",(0,t.jsxs)(n.h3,{id:"-dataset-preparation",children:["\ud83c\udfaf ",(0,t.jsx)(n.strong,{children:"Dataset Preparation"})]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Use diverse test cases that cover edge cases and common scenarios"}),"\n",(0,t.jsx)(n.li,{children:"Include clear expected answers or success criteria"}),"\n",(0,t.jsx)(n.li,{children:"Balance your dataset across different difficulty levels"}),"\n"]}),"\n",(0,t.jsxs)(n.h3,{id:"-metrics-selection",children:["\ud83d\udcca ",(0,t.jsx)(n.strong,{children:"Metrics Selection"})]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Choose appropriate scoring functions for your use case"}),"\n",(0,t.jsx)(n.li,{children:"Combine multiple metrics for comprehensive evaluation"}),"\n",(0,t.jsx)(n.li,{children:"Consider both automated and human evaluation metrics"}),"\n"]}),"\n",(0,t.jsxs)(n.h3,{id:"-iterative-improvement",children:["\ud83d\udd04 ",(0,t.jsx)(n.strong,{children:"Iterative Improvement"})]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Run evaluations regularly during development"}),"\n",(0,t.jsx)(n.li,{children:"Use evaluation results to identify areas for improvement"}),"\n",(0,t.jsx)(n.li,{children:"Track performance changes over time"}),"\n"]}),"\n",(0,t.jsxs)(n.h3,{id:"-analysis--reporting",children:["\ud83d\udcc8 ",(0,t.jsx)(n.strong,{children:"Analysis & Reporting"})]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Analyze failures to understand model limitations"}),"\n",(0,t.jsx)(n.li,{children:"Generate comprehensive evaluation reports"}),"\n",(0,t.jsx)(n.li,{children:"Share results with stakeholders for informed decision-making"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"advanced-evaluation-scenarios",children:"Advanced Evaluation Scenarios"}),"\n",(0,t.jsx)(n.h3,{id:"batch-evaluation",children:"Batch Evaluation"}),"\n",(0,t.jsx)(n.p,{children:"For evaluating large datasets efficiently:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Prepare large evaluation dataset\nlarge_eval_dataset = [\n    {"input_query": query, "expected_answer": answer}\n    for query, answer in zip(queries, expected_answers)\n]\n\n# Run batch evaluation\nbatch_results = client.scoring.score(\n    input_rows=large_eval_dataset,\n    scoring_functions={\n        "basic::subset_of": None,\n        "llm_as_judge::accuracy": {"judge_model": "meta-llama/Llama-3.3-70B-Instruct"},\n    }\n)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"multi-metric-evaluation",children:"Multi-Metric Evaluation"}),"\n",(0,t.jsx)(n.p,{children:"Combining different scoring approaches:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'comprehensive_scoring = {\n    "exact_match": "basic::exact_match",\n    "subset_match": "basic::subset_of", \n    "llm_judge": "llm_as_judge::accuracy",\n    "safety_check": "llm_as_judge::safety",\n}\n\nresults = client.scoring.score(\n    input_rows=eval_rows,\n    scoring_functions=comprehensive_scoring\n)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"related-resources",children:"Related Resources"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"./agent",children:"Agents"})})," - Building agents for evaluation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"./tools",children:"Tools Integration"})})," - Using tools in evaluated agents"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"/docs/references/evals-reference",children:"Evaluation Reference"})})," - Complete API reference for evaluations"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"https://colab.research.google.com/github/meta-llama/llama-stack/blob/main/docs/getting_started.ipynb",children:"Getting Started Notebook"})})," - Interactive examples"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"https://colab.research.google.com/drive/10CHyykee9j2OigaIcRv47BKG9mrNm0tJ?usp=sharing",children:"Evaluation Examples"})})," - Additional evaluation scenarios"]}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(h,{...e})}):h(e)}}}]);