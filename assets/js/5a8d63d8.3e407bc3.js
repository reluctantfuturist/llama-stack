"use strict";(self.webpackChunkdocusaurus_template_openapi_docs=self.webpackChunkdocusaurus_template_openapi_docs||[]).push([[3756],{28453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>s});var t=i(96540);const r={},l=t.createContext(r);function a(e){const n=t.useContext(l);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),t.createElement(l.Provider,{value:n},e.children)}},35602:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>c,default:()=>p,frontMatter:()=>o,metadata:()=>t,toc:()=>h});const t=JSON.parse('{"id":"distributions/ondevice-distro/android-sdk","title":"Android SDK","description":"Llama Stack Client Kotlin API Library for native Android development with local and remote inference","source":"@site/docs/distributions/ondevice-distro/android-sdk.mdx","sourceDirName":"distributions/ondevice-distro","slug":"/distributions/ondevice-distro/android-sdk","permalink":"/llama-stack/docs/distributions/ondevice-distro/android-sdk","draft":false,"unlisted":false,"editUrl":"https://github.com/meta-llama/llama-stack/tree/main/docs/docs/distributions/ondevice-distro/android-sdk.mdx","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Android SDK","description":"Llama Stack Client Kotlin API Library for native Android development with local and remote inference","sidebar_label":"Android SDK","sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"iOS SDK","permalink":"/llama-stack/docs/distributions/ondevice-distro/ios-sdk"},"next":{"title":"Overview","permalink":"/llama-stack/docs/providers/"}}');var r=i(74848),l=i(28453),a=i(64911),s=i(79329);const o={title:"Android SDK",description:"Llama Stack Client Kotlin API Library for native Android development with local and remote inference",sidebar_label:"Android SDK",sidebar_position:2},c="Llama Stack Client Kotlin API Library",d={},h=[{value:"Features",id:"features",level:2},{value:"Android Demo App",id:"android-demo-app",level:2},{value:"Quick Start",id:"quick-start",level:2},{value:"Add Dependencies",id:"add-dependencies",level:3},{value:"Llama Stack APIs in Your Android App",id:"llama-stack-apis-in-your-android-app",level:2},{value:"Setup Remote Inferencing",id:"setup-remote-inferencing",level:3},{value:"Initialize the Client",id:"initialize-the-client",level:3},{value:"Run Inference",id:"run-inference",level:3},{value:"Setup Custom Tool Calling",id:"setup-custom-tool-calling",level:3},{value:"Advanced Users",id:"advanced-users",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Building for Development/Debugging",id:"building-for-developmentdebugging",level:3},{value:"Additional Options for Local Inferencing",id:"additional-options-for-local-inferencing",level:3},{value:"Additional Options for Remote Inferencing",id:"additional-options-for-remote-inferencing",level:3},{value:"Network Options",id:"network-options",level:4},{value:"Retries",id:"retries",level:5},{value:"Timeouts",id:"timeouts",level:5},{value:"Proxies",id:"proxies",level:5},{value:"Environments",id:"environments",level:5},{value:"Error Handling",id:"error-handling",level:3},{value:"Known Issues",id:"known-issues",level:2},{value:"Reporting Issues",id:"reporting-issues",level:2},{value:"Thanks",id:"thanks",level:2},{value:"Related Resources",id:"related-resources",level:2}];function u(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",h5:"h5",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,l.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"llama-stack-client-kotlin-api-library",children:"Llama Stack Client Kotlin API Library"})}),"\n",(0,r.jsx)(n.p,{children:"We are excited to share a guide for a Kotlin Library that brings the benefits of Llama Stack to your Android device. This library is a set of SDKs that provide a simple and effective way to integrate AI capabilities into your Android app whether it is local (on-device) or remote inference."}),"\n",(0,r.jsx)(n.h2,{id:"features",children:"Features"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Local Inferencing"}),": Run Llama models purely on-device with real-time processing. We currently utilize ExecuTorch as the local inference distributor and may support others in the future.","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://github.com/pytorch/executorch/tree/main",children:"ExecuTorch"})," is a complete end-to-end solution within the PyTorch framework for inferencing capabilities on-device with high portability and seamless performance."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Remote Inferencing"}),": Perform inferencing tasks remotely with Llama models hosted on a remote connection (or serverless localhost)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simple Integration"}),": With easy-to-use APIs, a developer can quickly integrate Llama Stack in their Android app. The difference with local vs remote inferencing is also minimal."]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Latest Release Notes"}),": ",(0,r.jsx)(n.a,{href:"https://github.com/meta-llama/llama-stack-client-kotlin/tree/latest-release",children:"GitHub Release"})]}),"\n",(0,r.jsx)(n.admonition,{title:"Stability",type:"info",children:(0,r.jsx)(n.p,{children:"Tagged releases are stable versions of the project. While we strive to maintain a stable main branch, it's not guaranteed to be free of bugs or issues."})}),"\n",(0,r.jsx)(n.h2,{id:"android-demo-app",children:"Android Demo App"}),"\n",(0,r.jsxs)(n.p,{children:["Check out our demo app to see how to integrate Llama Stack into your Android app: ",(0,r.jsx)(n.a,{href:"https://github.com/meta-llama/llama-stack-client-kotlin/tree/latest-release/examples/android_app",children:"Android Demo App"})]}),"\n",(0,r.jsxs)(n.p,{children:["The key files in the app are ",(0,r.jsx)(n.code,{children:"ExampleLlamaStackLocalInference.kt"}),", ",(0,r.jsx)(n.code,{children:"ExampleLlamaStackRemoteInference.kts"}),", and ",(0,r.jsx)(n.code,{children:"MainActivity.java"}),". With encompassed business logic, the app shows how to use Llama Stack for both environments."]}),"\n",(0,r.jsx)(n.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,r.jsx)(n.h3,{id:"add-dependencies",children:"Add Dependencies"}),"\n",(0,r.jsxs)(a.A,{children:[(0,r.jsxs)(s.default,{value:"remote",label:"Remote Only",children:[(0,r.jsxs)(n.p,{children:["Add the following dependency in your ",(0,r.jsx)(n.code,{children:"build.gradle.kts"})," file:"]}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-kotlin",children:'dependencies {\n implementation("com.llama.llamastack:llama-stack-client-kotlin:0.2.2")\n}\n'})}),(0,r.jsxs)(n.p,{children:["This will download jar files in your gradle cache in a directory like ",(0,r.jsx)(n.code,{children:"~/.gradle/caches/modules-2/files-2.1/com.llama.llamastack/"})]}),(0,r.jsx)(n.p,{children:"If you plan on doing remote inferencing only, this is sufficient to get started."})]}),(0,r.jsxs)(s.default,{value:"local",label:"With Local Inference",children:[(0,r.jsx)(n.p,{children:"For local inferencing, it is required to include the ExecuTorch library into your app."}),(0,r.jsx)(n.p,{children:"Include the ExecuTorch library by:"}),(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["Download the ",(0,r.jsx)(n.code,{children:"download-prebuilt-et-lib.sh"})," script file from the ",(0,r.jsx)(n.a,{href:"https://github.com/meta-llama/llama-stack-client-kotlin/tree/latest-release/llama-stack-client-kotlin-client-local/download-prebuilt-et-lib.sh",children:"llama-stack-client-kotlin-client-local"})," directory to your local machine."]}),"\n",(0,r.jsxs)(n.li,{children:["Move the script to the top level of your Android app where the ",(0,r.jsx)(n.code,{children:"app"})," directory resides."]}),"\n",(0,r.jsxs)(n.li,{children:["Run ",(0,r.jsx)(n.code,{children:"sh download-prebuilt-et-lib.sh"})," to create an ",(0,r.jsx)(n.code,{children:"app/libs"})," directory and download the ",(0,r.jsx)(n.code,{children:"executorch.aar"})," in that path. This generates an ExecuTorch library for the XNNPACK delegate."]}),"\n",(0,r.jsxs)(n.li,{children:["Add the ",(0,r.jsx)(n.code,{children:"executorch.aar"})," dependency in your ",(0,r.jsx)(n.code,{children:"build.gradle.kts"})," file:"]}),"\n"]}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-kotlin",children:'dependencies {\n  ...\n  implementation(files("libs/executorch.aar"))\n  ...\n}\n'})}),(0,r.jsxs)(n.p,{children:["See other dependencies for the local RAG in Android app ",(0,r.jsx)(n.a,{href:"https://github.com/meta-llama/llama-stack-client-kotlin/tree/latest-release/examples/android_app#quick-start",children:"README"}),"."]})]})]}),"\n",(0,r.jsx)(n.h2,{id:"llama-stack-apis-in-your-android-app",children:"Llama Stack APIs in Your Android App"}),"\n",(0,r.jsx)(n.p,{children:"Breaking down the demo app, this section will show the core pieces that are used to initialize and run inference with Llama Stack using the Kotlin library."}),"\n",(0,r.jsx)(n.h3,{id:"setup-remote-inferencing",children:"Setup Remote Inferencing"}),"\n",(0,r.jsx)(n.p,{children:"Start a Llama Stack server on localhost. Here is an example of how you can do this using the firework.ai distribution:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"uv venv starter --python 3.12\nsource starter/bin/activate  # On Windows: starter\\Scripts\\activate\npip install --no-cache llama-stack==0.2.2\nllama stack build --distro starter --image-type venv\nexport FIREWORKS_API_KEY=<SOME_KEY>\nllama stack run starter --port 5050\n"})}),"\n",(0,r.jsx)(n.admonition,{title:"Version Compatibility",type:"warning",children:(0,r.jsx)(n.p,{children:"Ensure the Llama Stack server version is the same as the Kotlin SDK Library for maximum compatibility."})}),"\n",(0,r.jsxs)(n.p,{children:["Other inference providers: ",(0,r.jsx)(n.a,{href:"/docs/#supported-llama-stack-implementations",children:"Supported Implementations"})]}),"\n",(0,r.jsxs)(n.p,{children:["How to set remote localhost in Demo App: ",(0,r.jsx)(n.a,{href:"https://github.com/meta-llama/llama-stack-client-kotlin/tree/latest-release/examples/android_app#settings",children:"Settings"})]}),"\n",(0,r.jsx)(n.h3,{id:"initialize-the-client",children:"Initialize the Client"}),"\n",(0,r.jsx)(n.p,{children:"A client serves as the primary interface for interacting with a specific inference type and its associated parameters. Only after client is initialized then you can configure and start inferences."}),"\n",(0,r.jsxs)(a.A,{children:[(0,r.jsx)(s.default,{value:"local",label:"Local Inference",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-kotlin",children:"client = LlamaStackClientLocalClient\n                    .builder()\n                    .modelPath(modelPath)\n                    .tokenizerPath(tokenizerPath)\n                    .temperature(temperature)\n                    .build()\n"})})}),(0,r.jsx)(s.default,{value:"remote",label:"Remote Inference",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-kotlin",children:'// remoteURL is a string like "http://localhost:5050"\nclient = LlamaStackClientOkHttpClient\n                .builder()\n                .baseUrl(remoteURL)\n                .build()\n'})})})]}),"\n",(0,r.jsx)(n.h3,{id:"run-inference",children:"Run Inference"}),"\n",(0,r.jsx)(n.p,{children:"With the Kotlin Library managing all the major operational logic, there are minimal to no changes when running simple chat inference for local or remote:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-kotlin",children:"val result = client!!.inference().chatCompletion(\n            InferenceChatCompletionParams.builder()\n                .modelId(modelName)\n                .messages(listOfMessages)\n                .build()\n        )\n\n// response contains string with response from model\nvar response = result.asChatCompletionResponse().completionMessage().content().string();\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"[Remote only]"})," For inference with a streaming response:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-kotlin",children:"val result = client!!.inference().chatCompletionStreaming(\n            InferenceChatCompletionParams.builder()\n                .modelId(modelName)\n                .messages(listOfMessages)\n                .build()\n        )\n\n// Response can be received as a asChatCompletionResponseStreamChunk as part of a callback.\n// See Android demo app for a detailed implementation example.\n"})}),"\n",(0,r.jsx)(n.h3,{id:"setup-custom-tool-calling",children:"Setup Custom Tool Calling"}),"\n",(0,r.jsxs)(n.p,{children:["Android demo app for more details: ",(0,r.jsx)(n.a,{href:"https://github.com/meta-llama/llama-stack-client-kotlin/tree/latest-release/examples/android_app#tool-calling",children:"Custom Tool Calling"})]}),"\n",(0,r.jsx)(n.h2,{id:"advanced-users",children:"Advanced Users"}),"\n",(0,r.jsx)(n.p,{children:"The purpose of this section is to share more details with users that would like to dive deeper into the Llama Stack Kotlin Library. Whether you're interested in contributing to the open source library, debugging or just want to learn more, this section is for you!"}),"\n",(0,r.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsx)(n.p,{children:"You must complete the following steps:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["Clone the repo (",(0,r.jsx)(n.code,{children:"git clone https://github.com/meta-llama/llama-stack-client-kotlin.git -b latest-release"}),")"]}),"\n",(0,r.jsx)(n.li,{children:"Port the appropriate ExecuTorch libraries over into your Llama Stack Kotlin library environment."}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"cd llama-stack-client-kotlin-client-local\nsh download-prebuilt-et-lib.sh --unzip\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Now you will notice that the ",(0,r.jsx)(n.code,{children:"jni/"}),", ",(0,r.jsx)(n.code,{children:"libs/"}),", and ",(0,r.jsx)(n.code,{children:"AndroidManifest.xml"})," files from the ",(0,r.jsx)(n.code,{children:"executorch.aar"})," file are present in the local module. This way the local client module will be able to realize the ExecuTorch SDK."]}),"\n",(0,r.jsx)(n.h3,{id:"building-for-developmentdebugging",children:"Building for Development/Debugging"}),"\n",(0,r.jsx)(n.p,{children:"If you'd like to contribute to the Kotlin library via development, debug, or add play around with the library with various print statements, run the following command in your terminal under the llama-stack-client-kotlin directory."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"sh build-libs.sh\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Output"}),": .jar files located in the build-jars directory"]}),"\n",(0,r.jsx)(n.p,{children:"Copy the .jar files over to the lib directory in your Android app. At the same time make sure to remove the llama-stack-client-kotlin dependency within your build.gradle.kts file in your app (or if you are using the demo app) to avoid having multiple llama stack client dependencies."}),"\n",(0,r.jsx)(n.h3,{id:"additional-options-for-local-inferencing",children:"Additional Options for Local Inferencing"}),"\n",(0,r.jsx)(n.p,{children:"Currently we provide additional properties support with local inferencing. In order to get the tokens/sec metric for each inference call, add the following code in your Android app after you run your chatCompletion inference function. The Reference app has this implementation as well:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-kotlin",children:'var tps = (result.asChatCompletionResponse()._additionalProperties()["tps"] as JsonNumber).value as Float\n'})}),"\n",(0,r.jsx)(n.p,{children:"We will be adding more properties in the future."}),"\n",(0,r.jsx)(n.h3,{id:"additional-options-for-remote-inferencing",children:"Additional Options for Remote Inferencing"}),"\n",(0,r.jsx)(n.h4,{id:"network-options",children:"Network Options"}),"\n",(0,r.jsx)(n.h5,{id:"retries",children:"Retries"}),"\n",(0,r.jsx)(n.p,{children:"Requests that experience certain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and >=500 Internal errors will all be retried by default."}),"\n",(0,r.jsxs)(n.p,{children:["You can provide a ",(0,r.jsx)(n.code,{children:"maxRetries"})," on the client builder to configure this:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-kotlin",children:"val client = LlamaStackClientOkHttpClient.builder()\n    .fromEnv()\n    .maxRetries(4)\n    .build()\n"})}),"\n",(0,r.jsx)(n.h5,{id:"timeouts",children:"Timeouts"}),"\n",(0,r.jsx)(n.p,{children:"Requests time out after 1 minute by default. You can configure this on the client builder:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-kotlin",children:"val client = LlamaStackClientOkHttpClient.builder()\n    .fromEnv()\n    .timeout(Duration.ofSeconds(30))\n    .build()\n"})}),"\n",(0,r.jsx)(n.h5,{id:"proxies",children:"Proxies"}),"\n",(0,r.jsx)(n.p,{children:"Requests can be routed through a proxy. You can configure this on the client builder:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-kotlin",children:'val client = LlamaStackClientOkHttpClient.builder()\n    .fromEnv()\n    .proxy(new Proxy(\n        Type.HTTP,\n        new InetSocketAddress("proxy.com", 8080)\n    ))\n    .build()\n'})}),"\n",(0,r.jsx)(n.h5,{id:"environments",children:"Environments"}),"\n",(0,r.jsxs)(n.p,{children:["Requests are made to the production environment by default. You can connect to other environments, like ",(0,r.jsx)(n.code,{children:"sandbox"}),", via the client builder:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-kotlin",children:"val client = LlamaStackClientOkHttpClient.builder()\n    .fromEnv()\n    .sandbox()\n    .build()\n"})}),"\n",(0,r.jsx)(n.h3,{id:"error-handling",children:"Error Handling"}),"\n",(0,r.jsx)(n.p,{children:"This library throws exceptions in a single hierarchy for easy handling:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"LlamaStackClientException"})})," - Base exception for all exceptions"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"LlamaStackClientServiceException"})})," - HTTP errors with a well-formed response body we were able to parse. The exception message and the ",(0,r.jsx)(n.code,{children:".debuggingRequestId()"})," will be set by the server."]}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Status"}),(0,r.jsx)(n.th,{children:"Exception"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"400"}),(0,r.jsx)(n.td,{children:"BadRequestException"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"401"}),(0,r.jsx)(n.td,{children:"AuthenticationException"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"403"}),(0,r.jsx)(n.td,{children:"PermissionDeniedException"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"404"}),(0,r.jsx)(n.td,{children:"NotFoundException"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"422"}),(0,r.jsx)(n.td,{children:"UnprocessableEntityException"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"429"}),(0,r.jsx)(n.td,{children:"RateLimitException"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"5xx"}),(0,r.jsx)(n.td,{children:"InternalServerException"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"others"}),(0,r.jsx)(n.td,{children:"UnexpectedStatusCodeException"})]})]})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"LlamaStackClientIoException"})})," - I/O networking errors"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"LlamaStackClientInvalidDataException"})})," - any other exceptions on the client side, e.g.:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"We failed to serialize the request body"}),"\n",(0,r.jsx)(n.li,{children:"We failed to parse the response body (has access to response code and body)"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"known-issues",children:"Known Issues"}),"\n",(0,r.jsx)(n.p,{children:"We're aware of the following issues and are working to resolve them:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Streaming response is a work-in-progress for local and remote inference"}),"\n",(0,r.jsx)(n.li,{children:"Due to #1, agents are not supported at the time. LS agents only work in streaming mode"}),"\n",(0,r.jsx)(n.li,{children:"Changing to another model is a work in progress for local and remote platforms"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"reporting-issues",children:"Reporting Issues"}),"\n",(0,r.jsxs)(n.p,{children:["If you encountered any bugs or issues following this guide please file a bug/issue on our ",(0,r.jsx)(n.a,{href:"https://github.com/meta-llama/llama-stack-client-kotlin/issues",children:"GitHub issue tracker"}),"."]}),"\n",(0,r.jsx)(n.h2,{id:"thanks",children:"Thanks"}),"\n",(0,r.jsxs)(n.p,{children:["We'd like to extend our thanks to the ExecuTorch team for providing their support as we integrated ExecuTorch as one of the local inference distributors for Llama Stack. Checkout ",(0,r.jsx)(n.a,{href:"https://github.com/pytorch/executorch/tree/main",children:"ExecuTorch GitHub repo"})," for more information."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsxs)(n.p,{children:["The API interface is generated using the OpenAPI standard with ",(0,r.jsx)(n.a,{href:"https://www.stainlessapi.com/",children:"Stainless"}),"."]}),"\n",(0,r.jsx)(n.h2,{id:"related-resources",children:"Related Resources"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.a,{href:"https://github.com/meta-llama/llama-stack-client-kotlin",children:"llama-stack-client-kotlin"})})," - Official Kotlin SDK repository"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.a,{href:"https://github.com/meta-llama/llama-stack-client-kotlin/tree/latest-release/examples/android_app",children:"Android Demo App"})})," - Complete example app"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.a,{href:"https://github.com/pytorch/executorch/",children:"ExecuTorch"})})," - PyTorch on-device inference library"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.a,{href:"./ios-sdk",children:"iOS SDK"})})," - iOS development guide"]}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(u,{...e})}):u(e)}},64911:(e,n,i)=>{i.d(n,{A:()=>p});var t=i(96540),r=i(34164),l=i(65627),a=i(77448),s=i(9136);const o={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var c=i(74848);function d({className:e,block:n,selectedValue:i,selectValue:t,tabValues:a}){const s=[],{blockElementScrollPositionUntilNextRender:d}=(0,l.a_)(),h=e=>{const n=e.currentTarget,r=s.indexOf(n),l=a[r].value;l!==i&&(d(n),t(l))},u=e=>{let n=null;switch(e.key){case"Enter":h(e);break;case"ArrowRight":{const i=s.indexOf(e.currentTarget)+1;n=s[i]??s[0];break}case"ArrowLeft":{const i=s.indexOf(e.currentTarget)-1;n=s[i]??s[s.length-1];break}}n?.focus()};return(0,c.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.A)("tabs",{"tabs--block":n},e),children:a.map(({value:e,label:n,attributes:t})=>(0,c.jsx)("li",{role:"tab",tabIndex:i===e?0:-1,"aria-selected":i===e,ref:e=>{s.push(e)},onKeyDown:u,onClick:h,...t,className:(0,r.A)("tabs__item",o.tabItem,t?.className,{"tabs__item--active":i===e}),children:n??e},e))})}function h({lazy:e,children:n,selectedValue:i}){const l=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=l.find(e=>e.props.value===i);return e?(0,t.cloneElement)(e,{className:(0,r.A)("margin-top--md",e.props.className)}):null}return(0,c.jsx)("div",{className:"margin-top--md",children:l.map((e,n)=>(0,t.cloneElement)(e,{key:n,hidden:e.props.value!==i}))})}function u(e){const n=(0,a.u)(e);return(0,c.jsxs)("div",{className:(0,r.A)("tabs-container",o.tabList),children:[(0,c.jsx)(d,{...n,...e}),(0,c.jsx)(h,{...n,...e})]})}function p(e){const n=(0,s.default)();return(0,c.jsx)(u,{...e,children:(0,a.v)(e.children)},String(n))}},77448:(e,n,i)=>{i.d(n,{u:()=>p,v:()=>c});var t=i(96540),r=i(56347),l=i(50372),a=i(30604),s=i(78749),o=i(11861);function c(e){return t.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,t.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function d(e){const{values:n,children:i}=e;return(0,t.useMemo)(()=>{const e=n??function(e){return c(e).map(({props:{value:e,label:n,attributes:i,default:t}})=>({value:e,label:n,attributes:i,default:t}))}(i);return function(e){const n=(0,o.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,i])}function h({value:e,tabValues:n}){return n.some(n=>n.value===e)}function u({queryString:e=!1,groupId:n}){const i=(0,r.W6)(),l=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,a.aZ)(l),(0,t.useCallback)(e=>{if(!l)return;const n=new URLSearchParams(i.location.search);n.set(l,e),i.replace({...i.location,search:n.toString()})},[l,i])]}function p(e){const{defaultValue:n,queryString:i=!1,groupId:r}=e,a=d(e),[o,c]=(0,t.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!h({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const i=n.find(e=>e.default)??n[0];if(!i)throw new Error("Unexpected error: 0 tabValues");return i.value}({defaultValue:n,tabValues:a})),[p,m]=u({queryString:i,groupId:r}),[x,f]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[i,r]=(0,s.Dv)(n);return[i,(0,t.useCallback)(e=>{n&&r.set(e)},[n,r])]}({groupId:r}),j=(()=>{const e=p??x;return h({value:e,tabValues:a})?e:null})();(0,l.A)(()=>{j&&c(j)},[j]);return{selectedValue:o,selectValue:(0,t.useCallback)(e=>{if(!h({value:e,tabValues:a}))throw new Error(`Can't select invalid tab value=${e}`);c(e),m(e),f(e)},[m,f,a]),tabValues:a}}},79329:(e,n,i)=>{i.r(n),i.d(n,{default:()=>a});i(96540);var t=i(34164);const r={tabItem:"tabItem_Ymn6"};var l=i(74848);function a({children:e,hidden:n,className:i}){return(0,l.jsx)("div",{role:"tabpanel",className:(0,t.A)(r.tabItem,i),hidden:n,children:e})}}}]);