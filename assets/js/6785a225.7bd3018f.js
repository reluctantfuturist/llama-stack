"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[3017],{38796:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>t,toc:()=>o});const t=JSON.parse('{"id":"concepts/apis","title":"APIs","description":"Available REST APIs and planned capabilities in Llama Stack","source":"@site/docs/concepts/apis.mdx","sourceDirName":"concepts","slug":"/concepts/apis","permalink":"/llama-stack/docs/concepts/apis","draft":false,"unlisted":false,"editUrl":"https://github.com/meta-llama/llama-stack/tree/main/docs/docs/concepts/apis.mdx","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"APIs","description":"Available REST APIs and planned capabilities in Llama Stack","sidebar_label":"APIs","sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Architecture","permalink":"/llama-stack/docs/concepts/architecture"},"next":{"title":"API Providers","permalink":"/llama-stack/docs/concepts/api-providers"}}');var i=n(74848),a=n(28453);const r={title:"APIs",description:"Available REST APIs and planned capabilities in Llama Stack",sidebar_label:"APIs",sidebar_position:3},l="APIs",c={},o=[];function d(e){const s={h1:"h1",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(s.header,{children:(0,i.jsx)(s.h1,{id:"apis",children:"APIs"})}),"\n",(0,i.jsx)(s.p,{children:"A Llama Stack API is described as a collection of REST endpoints. We currently support the following APIs:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Inference"}),": run inference with a LLM"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Safety"}),": apply safety policies to the output at a Systems (not only model) level"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Agents"}),": run multi-step agentic workflows with LLMs with tool usage, memory (RAG), etc."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"DatasetIO"}),": interface with datasets and data loaders"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Scoring"}),": evaluate outputs of the system"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Eval"}),": generate outputs (via Inference or Agents) and perform scoring"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"VectorIO"}),": perform operations on vector stores, such as adding documents, searching, and deleting documents"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Telemetry"}),": collect telemetry data from the system"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Post Training"}),": fine-tune a model"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Tool Runtime"}),": interact with various tools and protocols"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Responses"}),": generate responses from an LLM using this OpenAI compatible API."]}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"We are working on adding a few more APIs to complete the application lifecycle. These will include:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Batch Inference"}),": run inference on a dataset of inputs"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Batch Agents"}),": run agents on a dataset of inputs"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Synthetic Data Generation"}),": generate synthetic data for model development"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Batches"}),": OpenAI-compatible batch management for inference"]}),"\n"]})]})}function h(e={}){const{wrapper:s}={...(0,a.R)(),...e.components};return s?(0,i.jsx)(s,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}}}]);