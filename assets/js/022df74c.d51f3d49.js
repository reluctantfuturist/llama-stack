"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[3037],{94939:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>o,contentTitle:()=>l,default:()=>u,frontMatter:()=>t,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"distributions/importing-as-library","title":"Using Llama Stack as a Library","description":"How to use Llama Stack as a Python library instead of running a server","source":"@site/docs/distributions/importing-as-library.mdx","sourceDirName":"distributions","slug":"/distributions/importing-as-library","permalink":"/docs/distributions/importing-as-library","draft":false,"unlisted":false,"editUrl":"https://github.com/meta-llama/llama-stack/tree/main/docs/docs/distributions/importing-as-library.mdx","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"title":"Using Llama Stack as a Library","description":"How to use Llama Stack as a Python library instead of running a server","sidebar_label":"Importing as Library","sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"Customizing run.yaml","permalink":"/docs/distributions/customizing-run-yaml"},"next":{"title":"Configuration Reference","permalink":"/docs/distributions/configuration"}}');var a=n(74848),s=n(28453);const t={title:"Using Llama Stack as a Library",description:"How to use Llama Stack as a Python library instead of running a server",sidebar_label:"Importing as Library",sidebar_position:5},l="Using Llama Stack as a Library",o={},d=[{value:"Setup Llama Stack without a Server",id:"setup-llama-stack-without-a-server",level:2},{value:"Benefits of Library Mode",id:"benefits-of-library-mode",level:2},{value:"Use Cases",id:"use-cases",level:2},{value:"Related Guides",id:"related-guides",level:2}];function c(e){const i={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(i.header,{children:(0,a.jsx)(i.h1,{id:"using-llama-stack-as-a-library",children:"Using Llama Stack as a Library"})}),"\n",(0,a.jsx)(i.h2,{id:"setup-llama-stack-without-a-server",children:"Setup Llama Stack without a Server"}),"\n",(0,a.jsx)(i.p,{children:"If you are planning to use an external service for Inference (even Ollama or TGI counts as external), it is often easier to use Llama Stack as a library.\nThis avoids the overhead of setting up a server."}),"\n",(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-bash",children:"# setup\nuv pip install llama-stack\nllama stack build --distro starter --image-type venv\n"})}),"\n",(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-python",children:'from llama_stack.core.library_client import LlamaStackAsLibraryClient\n\nclient = LlamaStackAsLibraryClient(\n    "starter",\n    # provider_data is optional, but if you need to pass in any provider specific data, you can do so here.\n    provider_data={"tavily_search_api_key": os.environ["TAVILY_SEARCH_API_KEY"]},\n)\n'})}),"\n",(0,a.jsx)(i.p,{children:"This will parse your config and set up any inline implementations and remote clients needed for your implementation."}),"\n",(0,a.jsxs)(i.p,{children:["Then, you can access the APIs like ",(0,a.jsx)(i.code,{children:"models"})," and ",(0,a.jsx)(i.code,{children:"inference"})," on the client and call their methods directly:"]}),"\n",(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-python",children:"response = client.models.list()\n"})}),"\n",(0,a.jsxs)(i.p,{children:["If you've created a ",(0,a.jsx)(i.a,{href:"./building-distro",children:"custom distribution"}),", you can also use the run.yaml configuration file directly:"]}),"\n",(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-python",children:"client = LlamaStackAsLibraryClient(config_path)\n"})}),"\n",(0,a.jsx)(i.h2,{id:"benefits-of-library-mode",children:"Benefits of Library Mode"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"No server overhead"}),": Direct Python API calls without HTTP requests"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Simplified deployment"}),": No need to manage server processes"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Better integration"}),": Seamlessly embed in existing Python applications"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Reduced latency"}),": Eliminate network round-trips for inline providers"]}),"\n"]}),"\n",(0,a.jsx)(i.h2,{id:"use-cases",children:"Use Cases"}),"\n",(0,a.jsx)(i.p,{children:"Library mode is ideal when:"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Using external services for most APIs (Ollama, remote inference providers, etc.)"}),"\n",(0,a.jsx)(i.li,{children:"Building Python applications that need Llama Stack functionality"}),"\n",(0,a.jsx)(i.li,{children:"Prototyping and development workflows"}),"\n",(0,a.jsx)(i.li,{children:"Serverless or container environments where you want minimal overhead"}),"\n"]}),"\n",(0,a.jsx)(i.h2,{id:"related-guides",children:"Related Guides"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:(0,a.jsx)(i.a,{href:"./building-distro",children:"Building Custom Distributions"})})," - Create your own distribution for library use"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:(0,a.jsx)(i.a,{href:"./configuration",children:"Configuration Reference"})})," - Understanding the configuration format"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:(0,a.jsx)(i.a,{href:"./starting-llama-stack-server",children:"Starting Llama Stack Server"})})," - Alternative server-based deployment"]}),"\n"]})]})}function u(e={}){const{wrapper:i}={...(0,s.R)(),...e.components};return i?(0,a.jsx)(i,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}}}]);