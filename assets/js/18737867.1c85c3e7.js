"use strict";(self.webpackChunkdocusaurus_template_openapi_docs=self.webpackChunkdocusaurus_template_openapi_docs||[]).push([[1906],{28453:(e,i,n)=>{n.d(i,{R:()=>o,x:()=>l});var s=n(96540);const r={},t=s.createContext(r);function o(e){const i=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function l(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),s.createElement(t.Provider,{value:i},e.children)}},92085:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>d,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>a});const s=JSON.parse('{"id":"distributions/self-hosted-distro/nvidia","title":"NVIDIA Distribution","description":"Use NVIDIA NIM for running LLM inference, evaluation and safety with NeMo Microservices","source":"@site/docs/distributions/self-hosted-distro/nvidia.mdx","sourceDirName":"distributions/self-hosted-distro","slug":"/distributions/self-hosted-distro/nvidia","permalink":"/llama-stack/docs/distributions/self-hosted-distro/nvidia","draft":false,"unlisted":false,"editUrl":"https://github.com/meta-llama/llama-stack/tree/main/docs/docs/distributions/self-hosted-distro/nvidia.mdx","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"NVIDIA Distribution","description":"Use NVIDIA NIM for running LLM inference, evaluation and safety with NeMo Microservices","sidebar_label":"NVIDIA","sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Meta Reference GPU","permalink":"/llama-stack/docs/distributions/self-hosted-distro/meta-reference-gpu"},"next":{"title":"Passthrough","permalink":"/llama-stack/docs/distributions/self-hosted-distro/passthrough"}}');var r=n(74848),t=n(28453);const o={title:"NVIDIA Distribution",description:"Use NVIDIA NIM for running LLM inference, evaluation and safety with NeMo Microservices",sidebar_label:"NVIDIA",sidebar_position:4},l="NVIDIA Distribution",d={},a=[{value:"Provider Configuration",id:"provider-configuration",level:2},{value:"Environment Variables",id:"environment-variables",level:2},{value:"Available Models",id:"available-models",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"NVIDIA API Keys",id:"nvidia-api-keys",level:3},{value:"Deploy NeMo Microservices Platform",id:"deploy-nemo-microservices-platform",level:3},{value:"Supported Services",id:"supported-services",level:2},{value:"Inference: NVIDIA NIM",id:"inference-nvidia-nim",level:3},{value:"Datasetio API: NeMo Data Store",id:"datasetio-api-nemo-data-store",level:3},{value:"Eval API: NeMo Evaluator",id:"eval-api-nemo-evaluator",level:3},{value:"Post-Training API: NeMo Customizer",id:"post-training-api-nemo-customizer",level:3},{value:"Safety API: NeMo Guardrails",id:"safety-api-nemo-guardrails",level:3},{value:"Deploying Models",id:"deploying-models",level:2},{value:"Running Llama Stack with NVIDIA",id:"running-llama-stack-with-nvidia",level:2},{value:"Via Docker",id:"via-docker",level:3},{value:"Via venv",id:"via-venv",level:3},{value:"Example Notebooks",id:"example-notebooks",level:2},{value:"Use Cases",id:"use-cases",level:2},{value:"Related Guides",id:"related-guides",level:2}];function c(e){const i={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(i.header,{children:(0,r.jsx)(i.h1,{id:"nvidia-distribution",children:"NVIDIA Distribution"})}),"\n",(0,r.jsxs)(i.p,{children:["The ",(0,r.jsx)(i.code,{children:"llamastack/distribution-nvidia"})," distribution consists of the following provider configurations."]}),"\n",(0,r.jsx)(i.h2,{id:"provider-configuration",children:"Provider Configuration"}),"\n",(0,r.jsxs)(i.table,{children:[(0,r.jsx)(i.thead,{children:(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.th,{children:"API"}),(0,r.jsx)(i.th,{children:"Provider(s)"})]})}),(0,r.jsxs)(i.tbody,{children:[(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:"agents"}),(0,r.jsx)(i.td,{children:(0,r.jsx)(i.code,{children:"inline::meta-reference"})})]}),(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:"datasetio"}),(0,r.jsxs)(i.td,{children:[(0,r.jsx)(i.code,{children:"inline::localfs"}),", ",(0,r.jsx)(i.code,{children:"remote::nvidia"})]})]}),(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:"eval"}),(0,r.jsx)(i.td,{children:(0,r.jsx)(i.code,{children:"remote::nvidia"})})]}),(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:"inference"}),(0,r.jsx)(i.td,{children:(0,r.jsx)(i.code,{children:"remote::nvidia"})})]}),(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:"post_training"}),(0,r.jsx)(i.td,{children:(0,r.jsx)(i.code,{children:"remote::nvidia"})})]}),(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:"safety"}),(0,r.jsx)(i.td,{children:(0,r.jsx)(i.code,{children:"remote::nvidia"})})]}),(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:"scoring"}),(0,r.jsx)(i.td,{children:(0,r.jsx)(i.code,{children:"inline::basic"})})]}),(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:"telemetry"}),(0,r.jsx)(i.td,{children:(0,r.jsx)(i.code,{children:"inline::meta-reference"})})]}),(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:"tool_runtime"}),(0,r.jsx)(i.td,{children:(0,r.jsx)(i.code,{children:"inline::rag-runtime"})})]}),(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:"vector_io"}),(0,r.jsx)(i.td,{children:(0,r.jsx)(i.code,{children:"inline::faiss"})})]})]})]}),"\n",(0,r.jsx)(i.h2,{id:"environment-variables",children:"Environment Variables"}),"\n",(0,r.jsx)(i.p,{children:"The following environment variables can be configured:"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.code,{children:"NVIDIA_API_KEY"}),": NVIDIA API Key (default: ``)"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.code,{children:"NVIDIA_APPEND_API_VERSION"}),": Whether to append the API version to the base_url (default: ",(0,r.jsx)(i.code,{children:"True"}),")"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.code,{children:"NVIDIA_DATASET_NAMESPACE"}),": NVIDIA Dataset Namespace (default: ",(0,r.jsx)(i.code,{children:"default"}),")"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.code,{children:"NVIDIA_PROJECT_ID"}),": NVIDIA Project ID (default: ",(0,r.jsx)(i.code,{children:"test-project"}),")"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.code,{children:"NVIDIA_CUSTOMIZER_URL"}),": NVIDIA Customizer URL (default: ",(0,r.jsx)(i.code,{children:"https://customizer.api.nvidia.com"}),")"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.code,{children:"NVIDIA_OUTPUT_MODEL_DIR"}),": NVIDIA Output Model Directory (default: ",(0,r.jsx)(i.code,{children:"test-example-model@v1"}),")"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.code,{children:"GUARDRAILS_SERVICE_URL"}),": URL for the NeMo Guardrails Service (default: ",(0,r.jsx)(i.code,{children:"http://0.0.0.0:7331"}),")"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.code,{children:"NVIDIA_GUARDRAILS_CONFIG_ID"}),": NVIDIA Guardrail Configuration ID (default: ",(0,r.jsx)(i.code,{children:"self-check"}),")"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.code,{children:"NVIDIA_EVALUATOR_URL"}),": URL for the NeMo Evaluator Service (default: ",(0,r.jsx)(i.code,{children:"http://0.0.0.0:7331"}),")"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.code,{children:"INFERENCE_MODEL"}),": Inference model (default: ",(0,r.jsx)(i.code,{children:"Llama3.1-8B-Instruct"}),")"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.code,{children:"SAFETY_MODEL"}),": Name of the model to use for safety (default: ",(0,r.jsx)(i.code,{children:"meta/llama-3.1-8b-instruct"}),")"]}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"available-models",children:"Available Models"}),"\n",(0,r.jsx)(i.p,{children:"The following models are available by default:"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:(0,r.jsx)(i.code,{children:"meta/llama3-8b-instruct"})}),"\n",(0,r.jsx)(i.li,{children:(0,r.jsx)(i.code,{children:"meta/llama3-70b-instruct"})}),"\n",(0,r.jsx)(i.li,{children:(0,r.jsx)(i.code,{children:"meta/llama-3.1-8b-instruct"})}),"\n",(0,r.jsx)(i.li,{children:(0,r.jsx)(i.code,{children:"meta/llama-3.1-70b-instruct"})}),"\n",(0,r.jsx)(i.li,{children:(0,r.jsx)(i.code,{children:"meta/llama-3.1-405b-instruct"})}),"\n",(0,r.jsx)(i.li,{children:(0,r.jsx)(i.code,{children:"meta/llama-3.2-1b-instruct"})}),"\n",(0,r.jsx)(i.li,{children:(0,r.jsx)(i.code,{children:"meta/llama-3.2-3b-instruct"})}),"\n",(0,r.jsx)(i.li,{children:(0,r.jsx)(i.code,{children:"meta/llama-3.2-11b-vision-instruct"})}),"\n",(0,r.jsx)(i.li,{children:(0,r.jsx)(i.code,{children:"meta/llama-3.2-90b-vision-instruct"})}),"\n",(0,r.jsx)(i.li,{children:(0,r.jsx)(i.code,{children:"meta/llama-3.3-70b-instruct"})}),"\n",(0,r.jsx)(i.li,{children:(0,r.jsx)(i.code,{children:"nvidia/vila"})}),"\n",(0,r.jsx)(i.li,{children:(0,r.jsx)(i.code,{children:"nvidia/llama-3.2-nv-embedqa-1b-v2"})}),"\n",(0,r.jsx)(i.li,{children:(0,r.jsx)(i.code,{children:"nvidia/nv-embedqa-e5-v5"})}),"\n",(0,r.jsx)(i.li,{children:(0,r.jsx)(i.code,{children:"nvidia/nv-embedqa-mistral-7b-v2"})}),"\n",(0,r.jsx)(i.li,{children:(0,r.jsx)(i.code,{children:"snowflake/arctic-embed-l"})}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsx)(i.h3,{id:"nvidia-api-keys",children:"NVIDIA API Keys"}),"\n",(0,r.jsxs)(i.p,{children:["Make sure you have access to a NVIDIA API Key. You can get one by visiting ",(0,r.jsx)(i.a,{href:"https://build.nvidia.com/",children:"https://build.nvidia.com/"}),". Use this key for the ",(0,r.jsx)(i.code,{children:"NVIDIA_API_KEY"})," environment variable."]}),"\n",(0,r.jsx)(i.h3,{id:"deploy-nemo-microservices-platform",children:"Deploy NeMo Microservices Platform"}),"\n",(0,r.jsxs)(i.p,{children:["The NVIDIA NeMo microservices platform supports end-to-end microservice deployment of a complete AI flywheel on your Kubernetes cluster through the NeMo Microservices Helm Chart. Please reference the ",(0,r.jsx)(i.a,{href:"https://docs.nvidia.com/nemo/microservices/latest/about/index.html",children:"NVIDIA NeMo Microservices documentation"})," for platform prerequisites and instructions to install and deploy the platform."]}),"\n",(0,r.jsx)(i.h2,{id:"supported-services",children:"Supported Services"}),"\n",(0,r.jsx)(i.p,{children:"Each Llama Stack API corresponds to a specific NeMo microservice. The core microservices (Customizer, Evaluator, Guardrails) are exposed by the same endpoint. The platform components (Data Store) are each exposed by separate endpoints."}),"\n",(0,r.jsx)(i.h3,{id:"inference-nvidia-nim",children:"Inference: NVIDIA NIM"}),"\n",(0,r.jsx)(i.p,{children:"NVIDIA NIM is used for running inference with registered models. There are two ways to access NVIDIA NIMs:"}),"\n",(0,r.jsxs)(i.ol,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Hosted (default)"}),": Preview APIs hosted at ",(0,r.jsx)(i.a,{href:"https://integrate.api.nvidia.com",children:"https://integrate.api.nvidia.com"})," (Requires an API key)"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Self-hosted"}),": NVIDIA NIMs that run on your own infrastructure."]}),"\n"]}),"\n",(0,r.jsxs)(i.p,{children:["The deployed platform includes the NIM Proxy microservice, which is the service that provides to access your NIMs (for example, to run inference on a model). Set the ",(0,r.jsx)(i.code,{children:"NVIDIA_BASE_URL"})," environment variable to use your NVIDIA NIM Proxy deployment."]}),"\n",(0,r.jsx)(i.h3,{id:"datasetio-api-nemo-data-store",children:"Datasetio API: NeMo Data Store"}),"\n",(0,r.jsxs)(i.p,{children:["The NeMo Data Store microservice serves as the default file storage solution for the NeMo microservices platform. It exposes APIs compatible with the Hugging Face Hub client (",(0,r.jsx)(i.code,{children:"HfApi"}),"), so you can use the client to interact with Data Store. The ",(0,r.jsx)(i.code,{children:"NVIDIA_DATASETS_URL"})," environment variable should point to your NeMo Data Store endpoint."]}),"\n",(0,r.jsx)(i.h3,{id:"eval-api-nemo-evaluator",children:"Eval API: NeMo Evaluator"}),"\n",(0,r.jsxs)(i.p,{children:["The NeMo Evaluator microservice supports evaluation of LLMs. Launching an Evaluation job with NeMo Evaluator requires an Evaluation Config (an object that contains metadata needed by the job). A Llama Stack Benchmark maps to an Evaluation Config, so registering a Benchmark creates an Evaluation Config in NeMo Evaluator. The ",(0,r.jsx)(i.code,{children:"NVIDIA_EVALUATOR_URL"})," environment variable should point to your NeMo Microservices endpoint."]}),"\n",(0,r.jsx)(i.h3,{id:"post-training-api-nemo-customizer",children:"Post-Training API: NeMo Customizer"}),"\n",(0,r.jsxs)(i.p,{children:["The NeMo Customizer microservice supports fine-tuning models. The ",(0,r.jsx)(i.code,{children:"NVIDIA_CUSTOMIZER_URL"})," environment variable should point to your NeMo Microservices endpoint."]}),"\n",(0,r.jsx)(i.h3,{id:"safety-api-nemo-guardrails",children:"Safety API: NeMo Guardrails"}),"\n",(0,r.jsxs)(i.p,{children:["The NeMo Guardrails microservice sits between your application and the LLM, and adds checks and content moderation to a model. The ",(0,r.jsx)(i.code,{children:"GUARDRAILS_SERVICE_URL"})," environment variable should point to your NeMo Microservices endpoint."]}),"\n",(0,r.jsx)(i.h2,{id:"deploying-models",children:"Deploying Models"}),"\n",(0,r.jsxs)(i.p,{children:["In order to use a registered model with the Llama Stack APIs, ensure the corresponding NIM is deployed to your environment. For example, you can use the NIM Proxy microservice to deploy ",(0,r.jsx)(i.code,{children:"meta/llama-3.2-1b-instruct"}),"."]}),"\n",(0,r.jsx)(i.admonition,{title:"Improved Performance",type:"note",children:(0,r.jsxs)(i.p,{children:["For improved inference speeds, we need to use NIM with ",(0,r.jsx)(i.code,{children:"fast_outlines"})," guided decoding system (specified in the request body). This is the default if you deployed the platform with the NeMo Microservices Helm Chart."]})}),"\n",(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{className:"language-bash",children:'# URL to NeMo NIM Proxy service\nexport NEMO_URL="http://nemo.test"\n\ncurl --location "$NEMO_URL/v1/deployment/model-deployments" \\\n   -H \'accept: application/json\' \\\n   -H \'Content-Type: application/json\' \\\n   -d \'{\n      "name": "llama-3.2-1b-instruct",\n      "namespace": "meta",\n      "config": {\n         "model": "meta/llama-3.2-1b-instruct",\n         "nim_deployment": {\n            "image_name": "nvcr.io/nim/meta/llama-3.2-1b-instruct",\n            "image_tag": "1.8.3",\n            "pvc_size": "25Gi",\n            "gpu": 1,\n            "additional_envs": {\n               "NIM_GUIDED_DECODING_BACKEND": "fast_outlines"\n            }\n         }\n      }\n   }\'\n'})}),"\n",(0,r.jsxs)(i.p,{children:["This NIM deployment should take approximately 10 minutes to go live. ",(0,r.jsx)(i.a,{href:"https://docs.nvidia.com/nemo/microservices/latest/get-started/tutorials/deploy-nims.html",children:"See the docs"})," for more information on how to deploy a NIM and verify it's available for inference."]}),"\n",(0,r.jsx)(i.p,{children:"You can also remove a deployed NIM to free up GPU resources, if needed:"}),"\n",(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{className:"language-bash",children:'export NEMO_URL="http://nemo.test"\n\ncurl -X DELETE "$NEMO_URL/v1/deployment/model-deployments/meta/llama-3.1-8b-instruct"\n'})}),"\n",(0,r.jsx)(i.h2,{id:"running-llama-stack-with-nvidia",children:"Running Llama Stack with NVIDIA"}),"\n",(0,r.jsx)(i.p,{children:"You can do this via venv (build code), or Docker which has a pre-built image."}),"\n",(0,r.jsx)(i.h3,{id:"via-docker",children:"Via Docker"}),"\n",(0,r.jsx)(i.p,{children:"This method allows you to get started quickly without having to build the distribution code."}),"\n",(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{className:"language-bash",children:"LLAMA_STACK_PORT=8321\ndocker run \\\n  -it \\\n  --pull always \\\n  -p $LLAMA_STACK_PORT:$LLAMA_STACK_PORT \\\n  -v ./run.yaml:/root/my-run.yaml \\\n  llamastack/distribution-nvidia \\\n  --config /root/my-run.yaml \\\n  --port $LLAMA_STACK_PORT \\\n  --env NVIDIA_API_KEY=$NVIDIA_API_KEY\n"})}),"\n",(0,r.jsx)(i.h3,{id:"via-venv",children:"Via venv"}),"\n",(0,r.jsx)(i.p,{children:"If you've set up your local development environment, you can also build the image using your local virtual environment."}),"\n",(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{className:"language-bash",children:"INFERENCE_MODEL=meta-llama/Llama-3.1-8B-Instruct\nllama stack build --distro nvidia --image-type venv\nllama stack run ./run.yaml \\\n  --port 8321 \\\n  --env NVIDIA_API_KEY=$NVIDIA_API_KEY \\\n  --env INFERENCE_MODEL=$INFERENCE_MODEL\n"})}),"\n",(0,r.jsx)(i.h2,{id:"example-notebooks",children:"Example Notebooks"}),"\n",(0,r.jsxs)(i.p,{children:["For examples of how to use the NVIDIA Distribution to run inference, fine-tune, evaluate, and run safety checks on your LLMs, you can reference the example notebooks in the ",(0,r.jsx)(i.a,{href:"/docs/notebooks/nvidia/",children:"NVIDIA notebooks directory"}),"."]}),"\n",(0,r.jsx)(i.h2,{id:"use-cases",children:"Use Cases"}),"\n",(0,r.jsx)(i.p,{children:"The NVIDIA distribution is ideal for:"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Enterprise deployments"})," with NVIDIA infrastructure"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"End-to-end ML workflows"})," from training to deployment"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"High-performance inference"})," with NVIDIA NIMs"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Advanced safety"})," with NeMo Guardrails"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Model customization"})," and fine-tuning"]}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"related-guides",children:"Related Guides"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:(0,r.jsx)(i.a,{href:"../list-of-distributions",children:"Available Distributions"})})," - Compare with other distributions"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:(0,r.jsx)(i.a,{href:"../configuration",children:"Configuration Reference"})})," - Understanding configuration options"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:(0,r.jsx)(i.a,{href:"../building-distro",children:"Building Custom Distributions"})})," - Create your own distribution"]}),"\n"]})]})}function h(e={}){const{wrapper:i}={...(0,t.R)(),...e.components};return i?(0,r.jsx)(i,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}}}]);