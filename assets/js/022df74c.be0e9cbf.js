"use strict";(self.webpackChunkdocusaurus_template_openapi_docs=self.webpackChunkdocusaurus_template_openapi_docs||[]).push([[3037],{28453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>l});var a=i(96540);const r={},s=a.createContext(r);function t(e){const n=a.useContext(s);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:t(e.components),a.createElement(s.Provider,{value:n},e.children)}},42172:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>u,frontMatter:()=>t,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"distributions/importing-as-library","title":"Using Llama Stack as a Library","description":"How to use Llama Stack as a Python library instead of running a server","source":"@site/docs/distributions/importing-as-library.mdx","sourceDirName":"distributions","slug":"/distributions/importing-as-library","permalink":"/llama-stack/docs/distributions/importing-as-library","draft":false,"unlisted":false,"editUrl":"https://github.com/meta-llama/llama-stack/tree/main/docs/docs/distributions/importing-as-library.mdx","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"title":"Using Llama Stack as a Library","description":"How to use Llama Stack as a Python library instead of running a server","sidebar_label":"Importing as Library","sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"Customizing run.yaml","permalink":"/llama-stack/docs/distributions/customizing-run-yaml"},"next":{"title":"Configuration Reference","permalink":"/llama-stack/docs/distributions/configuration"}}');var r=i(74848),s=i(28453);const t={title:"Using Llama Stack as a Library",description:"How to use Llama Stack as a Python library instead of running a server",sidebar_label:"Importing as Library",sidebar_position:5},l="Using Llama Stack as a Library",o={},c=[{value:"Setup Llama Stack without a Server",id:"setup-llama-stack-without-a-server",level:2},{value:"Benefits of Library Mode",id:"benefits-of-library-mode",level:2},{value:"Use Cases",id:"use-cases",level:2},{value:"Related Guides",id:"related-guides",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"using-llama-stack-as-a-library",children:"Using Llama Stack as a Library"})}),"\n",(0,r.jsx)(n.h2,{id:"setup-llama-stack-without-a-server",children:"Setup Llama Stack without a Server"}),"\n",(0,r.jsx)(n.p,{children:"If you are planning to use an external service for Inference (even Ollama or TGI counts as external), it is often easier to use Llama Stack as a library.\nThis avoids the overhead of setting up a server."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# setup\nuv pip install llama-stack\nllama stack build --distro starter --image-type venv\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from llama_stack.core.library_client import LlamaStackAsLibraryClient\n\nclient = LlamaStackAsLibraryClient(\n    "starter",\n    # provider_data is optional, but if you need to pass in any provider specific data, you can do so here.\n    provider_data={"tavily_search_api_key": os.environ["TAVILY_SEARCH_API_KEY"]},\n)\n'})}),"\n",(0,r.jsx)(n.p,{children:"This will parse your config and set up any inline implementations and remote clients needed for your implementation."}),"\n",(0,r.jsxs)(n.p,{children:["Then, you can access the APIs like ",(0,r.jsx)(n.code,{children:"models"})," and ",(0,r.jsx)(n.code,{children:"inference"})," on the client and call their methods directly:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"response = client.models.list()\n"})}),"\n",(0,r.jsxs)(n.p,{children:["If you've created a ",(0,r.jsx)(n.a,{href:"./building-distro",children:"custom distribution"}),", you can also use the run.yaml configuration file directly:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"client = LlamaStackAsLibraryClient(config_path)\n"})}),"\n",(0,r.jsx)(n.h2,{id:"benefits-of-library-mode",children:"Benefits of Library Mode"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"No server overhead"}),": Direct Python API calls without HTTP requests"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simplified deployment"}),": No need to manage server processes"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Better integration"}),": Seamlessly embed in existing Python applications"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Reduced latency"}),": Eliminate network round-trips for inline providers"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"use-cases",children:"Use Cases"}),"\n",(0,r.jsx)(n.p,{children:"Library mode is ideal when:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Using external services for most APIs (Ollama, remote inference providers, etc.)"}),"\n",(0,r.jsx)(n.li,{children:"Building Python applications that need Llama Stack functionality"}),"\n",(0,r.jsx)(n.li,{children:"Prototyping and development workflows"}),"\n",(0,r.jsx)(n.li,{children:"Serverless or container environments where you want minimal overhead"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"related-guides",children:"Related Guides"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.a,{href:"./building-distro",children:"Building Custom Distributions"})})," - Create your own distribution for library use"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.a,{href:"./configuration",children:"Configuration Reference"})})," - Understanding the configuration format"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.a,{href:"./starting-llama-stack-server",children:"Starting Llama Stack Server"})})," - Alternative server-based deployment"]}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}}}]);