"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[1024],{41167:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>a,contentTitle:()=>d,default:()=>h,frontMatter:()=>t,metadata:()=>s,toc:()=>o});const s=JSON.parse('{"id":"distributions/self-hosted-distro/meta-reference-gpu","title":"Meta Reference GPU Distribution","description":"High-performance GPU inference distribution using Meta\'s reference implementation","source":"@site/docs/distributions/self-hosted-distro/meta-reference-gpu.mdx","sourceDirName":"distributions/self-hosted-distro","slug":"/distributions/self-hosted-distro/meta-reference-gpu","permalink":"/docs/distributions/self-hosted-distro/meta-reference-gpu","draft":false,"unlisted":false,"editUrl":"https://github.com/meta-llama/llama-stack/tree/main/docs/docs/distributions/self-hosted-distro/meta-reference-gpu.mdx","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Meta Reference GPU Distribution","description":"High-performance GPU inference distribution using Meta\'s reference implementation","sidebar_label":"Meta Reference GPU","sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Dell-TGI","permalink":"/docs/distributions/self-hosted-distro/dell-tgi"},"next":{"title":"NVIDIA","permalink":"/docs/distributions/self-hosted-distro/nvidia"}}');var r=n(74848),l=n(28453);const t={title:"Meta Reference GPU Distribution",description:"High-performance GPU inference distribution using Meta's reference implementation",sidebar_label:"Meta Reference GPU",sidebar_position:3},d="Meta Reference GPU Distribution",a={},o=[{value:"Provider Configuration",id:"provider-configuration",level:2},{value:"Environment Variables",id:"environment-variables",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Downloading Models",id:"downloading-models",level:3},{value:"Running the Distribution",id:"running-the-distribution",level:2},{value:"Via Docker",id:"via-docker",level:3},{value:"Basic Setup",id:"basic-setup",level:4},{value:"With Safety/Shield APIs",id:"with-safetyshield-apis",level:4},{value:"Via venv",id:"via-venv",level:3},{value:"Basic Setup",id:"basic-setup-1",level:4},{value:"With Safety/Shield APIs",id:"with-safetyshield-apis-1",level:4},{value:"Use Cases",id:"use-cases",level:2},{value:"Performance Considerations",id:"performance-considerations",level:2},{value:"Related Guides",id:"related-guides",level:2}];function c(e){const i={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,l.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(i.header,{children:(0,r.jsx)(i.h1,{id:"meta-reference-gpu-distribution",children:"Meta Reference GPU Distribution"})}),"\n",(0,r.jsxs)(i.p,{children:["The ",(0,r.jsx)(i.code,{children:"llamastack/distribution-meta-reference-gpu"})," distribution consists of the following provider configurations:"]}),"\n",(0,r.jsx)(i.h2,{id:"provider-configuration",children:"Provider Configuration"}),"\n",(0,r.jsxs)(i.table,{children:[(0,r.jsx)(i.thead,{children:(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.th,{children:"API"}),(0,r.jsx)(i.th,{children:"Provider(s)"})]})}),(0,r.jsxs)(i.tbody,{children:[(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:"agents"}),(0,r.jsx)(i.td,{children:(0,r.jsx)(i.code,{children:"inline::meta-reference"})})]}),(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:"datasetio"}),(0,r.jsxs)(i.td,{children:[(0,r.jsx)(i.code,{children:"remote::huggingface"}),", ",(0,r.jsx)(i.code,{children:"inline::localfs"})]})]}),(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:"eval"}),(0,r.jsx)(i.td,{children:(0,r.jsx)(i.code,{children:"inline::meta-reference"})})]}),(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:"inference"}),(0,r.jsx)(i.td,{children:(0,r.jsx)(i.code,{children:"inline::meta-reference"})})]}),(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:"safety"}),(0,r.jsx)(i.td,{children:(0,r.jsx)(i.code,{children:"inline::llama-guard"})})]}),(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:"scoring"}),(0,r.jsxs)(i.td,{children:[(0,r.jsx)(i.code,{children:"inline::basic"}),", ",(0,r.jsx)(i.code,{children:"inline::llm-as-judge"}),", ",(0,r.jsx)(i.code,{children:"inline::braintrust"})]})]}),(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:"telemetry"}),(0,r.jsx)(i.td,{children:(0,r.jsx)(i.code,{children:"inline::meta-reference"})})]}),(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:"tool_runtime"}),(0,r.jsxs)(i.td,{children:[(0,r.jsx)(i.code,{children:"remote::brave-search"}),", ",(0,r.jsx)(i.code,{children:"remote::tavily-search"}),", ",(0,r.jsx)(i.code,{children:"inline::rag-runtime"}),", ",(0,r.jsx)(i.code,{children:"remote::model-context-protocol"})]})]}),(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:"vector_io"}),(0,r.jsxs)(i.td,{children:[(0,r.jsx)(i.code,{children:"inline::faiss"}),", ",(0,r.jsx)(i.code,{children:"remote::chromadb"}),", ",(0,r.jsx)(i.code,{children:"remote::pgvector"})]})]})]})]}),"\n",(0,r.jsx)(i.admonition,{title:"GPU Requirements",type:"warning",children:(0,r.jsx)(i.p,{children:"You need access to NVIDIA GPUs to run this distribution. This distribution is not compatible with CPU-only machines or machines with AMD GPUs."})}),"\n",(0,r.jsx)(i.h2,{id:"environment-variables",children:"Environment Variables"}),"\n",(0,r.jsx)(i.p,{children:"The following environment variables can be configured:"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.code,{children:"LLAMA_STACK_PORT"}),": Port for the Llama Stack distribution server (default: ",(0,r.jsx)(i.code,{children:"8321"}),")"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.code,{children:"INFERENCE_MODEL"}),": Inference model loaded into the Meta Reference server (default: ",(0,r.jsx)(i.code,{children:"meta-llama/Llama-3.2-3B-Instruct"}),")"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.code,{children:"INFERENCE_CHECKPOINT_DIR"}),": Directory containing the Meta Reference model checkpoint (default: ",(0,r.jsx)(i.code,{children:"null"}),")"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.code,{children:"SAFETY_MODEL"}),": Name of the safety (Llama-Guard) model to use (default: ",(0,r.jsx)(i.code,{children:"meta-llama/Llama-Guard-3-1B"}),")"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.code,{children:"SAFETY_CHECKPOINT_DIR"}),": Directory containing the Llama-Guard model checkpoint (default: ",(0,r.jsx)(i.code,{children:"null"}),")"]}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsx)(i.h3,{id:"downloading-models",children:"Downloading Models"}),"\n",(0,r.jsxs)(i.p,{children:["Please use ",(0,r.jsx)(i.code,{children:"llama model list --downloaded"})," to check that you have llama model checkpoints downloaded in ",(0,r.jsx)(i.code,{children:"~/.llama"})," before proceeding. See ",(0,r.jsx)(i.a,{href:"/docs/references/llama-cli-reference/download-models",children:"installation guide"})," to download the models. Run ",(0,r.jsx)(i.code,{children:"llama model list"})," to see the available models to download, and ",(0,r.jsx)(i.code,{children:"llama model download"})," to download the checkpoints."]}),"\n",(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{className:"language-bash",children:"$ llama model list --downloaded\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Model                                   \u2503 Size     \u2503 Modified Time       \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 Llama3.2-1B-Instruct:int4-qlora-eo8     \u2502 1.53 GB  \u2502 2025-02-26 11:22:28 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Llama3.2-1B                             \u2502 2.31 GB  \u2502 2025-02-18 21:48:52 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Prompt-Guard-86M                        \u2502 0.02 GB  \u2502 2025-02-26 11:29:28 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Llama3.2-3B-Instruct:int4-spinquant-eo8 \u2502 3.69 GB  \u2502 2025-02-26 11:37:41 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Llama3.2-3B                             \u2502 5.99 GB  \u2502 2025-02-18 21:51:26 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Llama3.1-8B                             \u2502 14.97 GB \u2502 2025-02-16 10:36:37 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Llama3.2-1B-Instruct:int4-spinquant-eo8 \u2502 1.51 GB  \u2502 2025-02-26 11:35:02 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Llama-Guard-3-1B                        \u2502 2.80 GB  \u2502 2025-02-26 11:20:46 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Llama-Guard-3-1B:int4                   \u2502 0.43 GB  \u2502 2025-02-26 11:33:33 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,r.jsx)(i.h2,{id:"running-the-distribution",children:"Running the Distribution"}),"\n",(0,r.jsx)(i.p,{children:"You can do this via venv or Docker which has a pre-built image."}),"\n",(0,r.jsx)(i.h3,{id:"via-docker",children:"Via Docker"}),"\n",(0,r.jsx)(i.p,{children:"This method allows you to get started quickly without having to build the distribution code."}),"\n",(0,r.jsx)(i.h4,{id:"basic-setup",children:"Basic Setup"}),"\n",(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{className:"language-bash",children:"LLAMA_STACK_PORT=8321\ndocker run \\\n  -it \\\n  --pull always \\\n  --gpu all \\\n  -p $LLAMA_STACK_PORT:$LLAMA_STACK_PORT \\\n  -v ~/.llama:/root/.llama \\\n  llamastack/distribution-meta-reference-gpu \\\n  --port $LLAMA_STACK_PORT \\\n  --env INFERENCE_MODEL=meta-llama/Llama-3.2-3B-Instruct\n"})}),"\n",(0,r.jsx)(i.h4,{id:"with-safetyshield-apis",children:"With Safety/Shield APIs"}),"\n",(0,r.jsx)(i.p,{children:"If you are using Llama Stack Safety / Shield APIs, use:"}),"\n",(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{className:"language-bash",children:"docker run \\\n  -it \\\n  --pull always \\\n  --gpu all \\\n  -p $LLAMA_STACK_PORT:$LLAMA_STACK_PORT \\\n  -v ~/.llama:/root/.llama \\\n  llamastack/distribution-meta-reference-gpu \\\n  --port $LLAMA_STACK_PORT \\\n  --env INFERENCE_MODEL=meta-llama/Llama-3.2-3B-Instruct \\\n  --env SAFETY_MODEL=meta-llama/Llama-Guard-3-1B\n"})}),"\n",(0,r.jsx)(i.h3,{id:"via-venv",children:"Via venv"}),"\n",(0,r.jsxs)(i.p,{children:["Make sure you have done ",(0,r.jsx)(i.code,{children:"uv pip install llama-stack"})," and have the Llama Stack CLI available."]}),"\n",(0,r.jsx)(i.h4,{id:"basic-setup-1",children:"Basic Setup"}),"\n",(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{className:"language-bash",children:"llama stack build --distro meta-reference-gpu --image-type venv\nllama stack run distributions/meta-reference-gpu/run.yaml \\\n  --port 8321 \\\n  --env INFERENCE_MODEL=meta-llama/Llama-3.2-3B-Instruct\n"})}),"\n",(0,r.jsx)(i.h4,{id:"with-safetyshield-apis-1",children:"With Safety/Shield APIs"}),"\n",(0,r.jsx)(i.p,{children:"If you are using Llama Stack Safety / Shield APIs, use:"}),"\n",(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{className:"language-bash",children:"llama stack run distributions/meta-reference-gpu/run-with-safety.yaml \\\n  --port 8321 \\\n  --env INFERENCE_MODEL=meta-llama/Llama-3.2-3B-Instruct \\\n  --env SAFETY_MODEL=meta-llama/Llama-Guard-3-1B\n"})}),"\n",(0,r.jsx)(i.h2,{id:"use-cases",children:"Use Cases"}),"\n",(0,r.jsx)(i.p,{children:"The Meta Reference GPU distribution is ideal for:"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"High-performance inference"}),": Maximum performance with GPU acceleration"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Local development"}),": Full control over models and configurations"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Research and experimentation"}),": Access to Meta's reference implementations"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Production deployments"}),": When you need local GPU inference without external dependencies"]}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"performance-considerations",children:"Performance Considerations"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Memory Requirements"}),": Ensure sufficient GPU memory for your chosen models"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Model Size"}),": Larger models require more GPU memory and provide better performance"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Batch Processing"}),": Optimize batch sizes for your specific GPU configuration"]}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"related-guides",children:"Related Guides"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:(0,r.jsx)(i.a,{href:"../list-of-distributions",children:"Available Distributions"})})," - Compare with other distributions"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:(0,r.jsx)(i.a,{href:"../configuration",children:"Configuration Reference"})})," - Understanding configuration options"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:(0,r.jsx)(i.a,{href:"../building-distro",children:"Building Custom Distributions"})})," - Create your own distribution"]}),"\n"]})]})}function h(e={}){const{wrapper:i}={...(0,l.R)(),...e.components};return i?(0,r.jsx)(i,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}}}]);