"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[7148],{62691:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>d,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"advanced-apis/post-training","title":"Post-Training","description":"Post-training in Llama Stack allows you to fine-tune models using various providers and frameworks. This section covers all available post-training providers and how to use them effectively.","source":"@site/docs/advanced-apis/post-training.mdx","sourceDirName":"advanced-apis","slug":"/advanced-apis/post-training","permalink":"/llama-stack/docs/advanced-apis/post-training","draft":false,"unlisted":false,"editUrl":"https://github.com/meta-llama/llama-stack/tree/main/docs/docs/advanced-apis/post-training.mdx","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Playground","permalink":"/llama-stack/docs/building-applications/playground"},"next":{"title":"Evaluation","permalink":"/llama-stack/docs/advanced-apis/evaluation"}}');var r=i(74848),s=i(28453);const d={},a="Post-Training",l={},c=[{value:"Overview",id:"overview",level:2},{value:"HuggingFace SFTTrainer",id:"huggingface-sfttrainer",level:2},{value:"Features",id:"features",level:3},{value:"Configuration",id:"configuration",level:3},{value:"Sample Configuration",id:"sample-configuration",level:3},{value:"Setup",id:"setup",level:3},{value:"Usage Example",id:"usage-example",level:3},{value:"TorchTune",id:"torchtune",level:2},{value:"Features",id:"features-1",level:3},{value:"Configuration",id:"configuration-1",level:3},{value:"Sample Configuration",id:"sample-configuration-1",level:3},{value:"Setup",id:"setup-1",level:3},{value:"Usage Example",id:"usage-example-1",level:3},{value:"NVIDIA",id:"nvidia",level:2},{value:"Configuration",id:"configuration-2",level:3},{value:"Sample Configuration",id:"sample-configuration-2",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Next Steps",id:"next-steps",level:2}];function o(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"post-training",children:"Post-Training"})}),"\n",(0,r.jsx)(n.p,{children:"Post-training in Llama Stack allows you to fine-tune models using various providers and frameworks. This section covers all available post-training providers and how to use them effectively."}),"\n",(0,r.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsx)(n.p,{children:"Llama Stack provides multiple post-training providers:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"HuggingFace SFTTrainer"})," (",(0,r.jsx)(n.code,{children:"inline::huggingface"}),") - Fine-tuning using HuggingFace ecosystem"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"TorchTune"})," (",(0,r.jsx)(n.code,{children:"inline::torchtune"}),") - Fine-tuning using Meta's TorchTune framework"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"NVIDIA"})," (",(0,r.jsx)(n.code,{children:"remote::nvidia"}),") - Fine-tuning using NVIDIA's platform"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"huggingface-sfttrainer",children:"HuggingFace SFTTrainer"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/docs/trl/en/sft_trainer",children:"HuggingFace SFTTrainer"})," is an inline post training provider for Llama Stack. It allows you to run supervised fine tuning on a variety of models using many datasets."]}),"\n",(0,r.jsx)(n.h3,{id:"features",children:"Features"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Simple access through the post_training API"}),"\n",(0,r.jsx)(n.li,{children:"Fully integrated with Llama Stack"}),"\n",(0,r.jsx)(n.li,{children:"GPU support, CPU support, and MPS support (MacOS Metal Performance Shaders)"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"configuration",children:"Configuration"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Field"}),(0,r.jsx)(n.th,{children:"Type"}),(0,r.jsx)(n.th,{children:"Required"}),(0,r.jsx)(n.th,{children:"Default"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"device"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"str"})}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:"cuda"}),(0,r.jsx)(n.td,{})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"distributed_backend"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"Literal['fsdp', 'deepspeed']"})}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{}),(0,r.jsx)(n.td,{})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"checkpoint_format"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"Literal['full_state', 'huggingface']"})}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:"huggingface"}),(0,r.jsx)(n.td,{})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"chat_template"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"str"})}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{}),(0,r.jsx)(n.td,{})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"model_specific_config"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"dict"})}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"{'trust_remote_code': True, 'attn_implementation': 'sdpa'}"})}),(0,r.jsx)(n.td,{})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"max_seq_length"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"int"})}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:"2048"}),(0,r.jsx)(n.td,{})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"gradient_checkpointing"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"bool"})}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:"False"}),(0,r.jsx)(n.td,{})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"save_total_limit"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"int"})}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:"3"}),(0,r.jsx)(n.td,{})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"logging_steps"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"int"})}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:"10"}),(0,r.jsx)(n.td,{})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"warmup_ratio"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"float"})}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:"0.1"}),(0,r.jsx)(n.td,{})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"weight_decay"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"float"})}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:"0.01"}),(0,r.jsx)(n.td,{})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"dataloader_num_workers"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"int"})}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:"4"}),(0,r.jsx)(n.td,{})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"dataloader_pin_memory"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"bool"})}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:"True"}),(0,r.jsx)(n.td,{})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"sample-configuration",children:"Sample Configuration"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"checkpoint_format: huggingface\ndistributed_backend: null\ndevice: cpu\n"})}),"\n",(0,r.jsx)(n.h3,{id:"setup",children:"Setup"}),"\n",(0,r.jsxs)(n.p,{children:["You can access the HuggingFace trainer via the ",(0,r.jsx)(n.code,{children:"starter"})," distribution:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"llama stack build --distro starter --image-type venv\nllama stack run --image-type venv ~/.llama/distributions/starter/starter-run.yaml\n"})}),"\n",(0,r.jsx)(n.h3,{id:"usage-example",children:"Usage Example"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import time\nimport uuid\n\nfrom llama_stack_client.types import (\n    post_training_supervised_fine_tune_params,\n    algorithm_config_param,\n)\n\ndef create_http_client():\n    from llama_stack_client import LlamaStackClient\n    return LlamaStackClient(base_url="http://localhost:8321")\n\nclient = create_http_client()\n\n# Example Dataset\nclient.datasets.register(\n    purpose="post-training/messages",\n    source={\n        "type": "uri",\n        "uri": "huggingface://datasets/llamastack/simpleqa?split=train",\n    },\n    dataset_id="simpleqa",\n)\n\ntraining_config = post_training_supervised_fine_tune_params.TrainingConfig(\n    data_config=post_training_supervised_fine_tune_params.TrainingConfigDataConfig(\n        batch_size=32,\n        data_format="instruct",\n        dataset_id="simpleqa",\n        shuffle=True,\n    ),\n    gradient_accumulation_steps=1,\n    max_steps_per_epoch=0,\n    max_validation_steps=1,\n    n_epochs=4,\n)\n\nalgorithm_config = algorithm_config_param.LoraFinetuningConfig(\n    alpha=1,\n    apply_lora_to_mlp=True,\n    apply_lora_to_output=False,\n    lora_attn_modules=["q_proj"],\n    rank=1,\n    type="LoRA",\n)\n\njob_uuid = f"test-job{uuid.uuid4()}"\n\n# Example Model\ntraining_model = "ibm-granite/granite-3.3-8b-instruct"\n\nstart_time = time.time()\nresponse = client.post_training.supervised_fine_tune(\n    job_uuid=job_uuid,\n    logger_config={},\n    model=training_model,\n    hyperparam_search_config={},\n    training_config=training_config,\n    algorithm_config=algorithm_config,\n    checkpoint_dir="output",\n)\nprint("Job: ", job_uuid)\n\n# Wait for the job to complete!\nwhile True:\n    status = client.post_training.job.status(job_uuid=job_uuid)\n    if not status:\n        print("Job not found")\n        break\n\n    print(status)\n    if status.status == "completed":\n        break\n\n    print("Waiting for job to complete...")\n    time.sleep(5)\n\nend_time = time.time()\nprint("Job completed in", end_time - start_time, "seconds!")\n\nprint("Artifacts:")\nprint(client.post_training.job.artifacts(job_uuid=job_uuid))\n'})}),"\n",(0,r.jsx)(n.h2,{id:"torchtune",children:"TorchTune"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://github.com/pytorch/torchtune",children:"TorchTune"})," is an inline post training provider for Llama Stack. It provides a simple and efficient way to fine-tune language models using PyTorch."]}),"\n",(0,r.jsx)(n.h3,{id:"features-1",children:"Features"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Simple access through the post_training API"}),"\n",(0,r.jsx)(n.li,{children:"Fully integrated with Llama Stack"}),"\n",(0,r.jsx)(n.li,{children:"GPU support and single device capabilities"}),"\n",(0,r.jsx)(n.li,{children:"Support for LoRA"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"configuration-1",children:"Configuration"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Field"}),(0,r.jsx)(n.th,{children:"Type"}),(0,r.jsx)(n.th,{children:"Required"}),(0,r.jsx)(n.th,{children:"Default"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"torch_seed"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"int | None"})}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{}),(0,r.jsx)(n.td,{})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"checkpoint_format"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"Literal['meta', 'huggingface']"})}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:"meta"}),(0,r.jsx)(n.td,{})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"sample-configuration-1",children:"Sample Configuration"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"checkpoint_format: meta\n"})}),"\n",(0,r.jsx)(n.h3,{id:"setup-1",children:"Setup"}),"\n",(0,r.jsx)(n.p,{children:"You can access the TorchTune trainer by writing your own yaml pointing to the provider:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"post_training:\n  - provider_id: torchtune\n    provider_type: inline::torchtune\n    config: {}\n"})}),"\n",(0,r.jsx)(n.p,{children:"You can then build and run your own stack with this provider."}),"\n",(0,r.jsx)(n.h3,{id:"usage-example-1",children:"Usage Example"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import time\nimport uuid\n\nfrom llama_stack_client.types import (\n    post_training_supervised_fine_tune_params,\n    algorithm_config_param,\n)\n\ndef create_http_client():\n    from llama_stack_client import LlamaStackClient\n    return LlamaStackClient(base_url="http://localhost:8321")\n\nclient = create_http_client()\n\n# Example Dataset\nclient.datasets.register(\n    purpose="post-training/messages",\n    source={\n        "type": "uri",\n        "uri": "huggingface://datasets/llamastack/simpleqa?split=train",\n    },\n    dataset_id="simpleqa",\n)\n\ntraining_config = post_training_supervised_fine_tune_params.TrainingConfig(\n    data_config=post_training_supervised_fine_tune_params.TrainingConfigDataConfig(\n        batch_size=32,\n        data_format="instruct",\n        dataset_id="simpleqa",\n        shuffle=True,\n    ),\n    gradient_accumulation_steps=1,\n    max_steps_per_epoch=0,\n    max_validation_steps=1,\n    n_epochs=4,\n)\n\nalgorithm_config = algorithm_config_param.LoraFinetuningConfig(\n    alpha=1,\n    apply_lora_to_mlp=True,\n    apply_lora_to_output=False,\n    lora_attn_modules=["q_proj"],\n    rank=1,\n    type="LoRA",\n)\n\njob_uuid = f"test-job{uuid.uuid4()}"\n\n# Example Model\ntraining_model = "meta-llama/Llama-2-7b-hf"\n\nstart_time = time.time()\nresponse = client.post_training.supervised_fine_tune(\n    job_uuid=job_uuid,\n    logger_config={},\n    model=training_model,\n    hyperparam_search_config={},\n    training_config=training_config,\n    algorithm_config=algorithm_config,\n    checkpoint_dir="output",\n)\nprint("Job: ", job_uuid)\n\n# Wait for the job to complete!\nwhile True:\n    status = client.post_training.job.status(job_uuid=job_uuid)\n    if not status:\n        print("Job not found")\n        break\n\n    print(status)\n    if status.status == "completed":\n        break\n\n    print("Waiting for job to complete...")\n    time.sleep(5)\n\nend_time = time.time()\nprint("Job completed in", end_time - start_time, "seconds!")\n\nprint("Artifacts:")\nprint(client.post_training.job.artifacts(job_uuid=job_uuid))\n'})}),"\n",(0,r.jsx)(n.h2,{id:"nvidia",children:"NVIDIA"}),"\n",(0,r.jsx)(n.p,{children:"NVIDIA's post-training provider for fine-tuning models on NVIDIA's platform."}),"\n",(0,r.jsx)(n.h3,{id:"configuration-2",children:"Configuration"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Field"}),(0,r.jsx)(n.th,{children:"Type"}),(0,r.jsx)(n.th,{children:"Required"}),(0,r.jsx)(n.th,{children:"Default"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"api_key"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"str | None"})}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{}),(0,r.jsx)(n.td,{children:"The NVIDIA API key."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"dataset_namespace"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"str | None"})}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:"default"}),(0,r.jsx)(n.td,{children:"The NVIDIA dataset namespace."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"project_id"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"str | None"})}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:"test-example-model@v1"}),(0,r.jsx)(n.td,{children:"The NVIDIA project ID."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"customizer_url"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"str | None"})}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{}),(0,r.jsx)(n.td,{children:"Base URL for the NeMo Customizer API"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"timeout"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"int"})}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:"300"}),(0,r.jsx)(n.td,{children:"Timeout for the NVIDIA Post Training API"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"max_retries"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"int"})}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:"3"}),(0,r.jsx)(n.td,{children:"Maximum number of retries for the NVIDIA Post Training API"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"output_model_dir"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"str"})}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:"test-example-model@v1"}),(0,r.jsx)(n.td,{children:"Directory to save the output model"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"sample-configuration-2",children:"Sample Configuration"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"api_key: ${env.NVIDIA_API_KEY:=}\ndataset_namespace: ${env.NVIDIA_DATASET_NAMESPACE:=default}\nproject_id: ${env.NVIDIA_PROJECT_ID:=test-project}\ncustomizer_url: ${env.NVIDIA_CUSTOMIZER_URL:=http://nemo.test}\n"})}),"\n",(0,r.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Choose the right provider"}),": Use HuggingFace for broader compatibility, TorchTune for Meta models, or NVIDIA for their ecosystem"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Configure hardware appropriately"}),": Ensure your configuration matches your available hardware (CPU, GPU, MPS)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Monitor jobs"}),": Always monitor job status and handle completion appropriately"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Use appropriate datasets"}),": Ensure your dataset format matches the expected input format for your chosen provider"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Check out the ",(0,r.jsx)(n.a,{href:"/llama-stack/docs/building-applications/",children:"Building Applications - Fine-tuning"})," guide for application-level examples"]}),"\n",(0,r.jsxs)(n.li,{children:["See the ",(0,r.jsx)(n.a,{href:"/llama-stack/docs/providers/post_training/",children:"Providers"})," section for detailed provider documentation"]}),"\n",(0,r.jsxs)(n.li,{children:["Review the ",(0,r.jsx)(n.a,{href:"../api-reference/post-training.mdx",children:"API Reference"})," for complete API documentation"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(o,{...e})}):o(e)}}}]);