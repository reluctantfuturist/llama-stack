"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[1109],{264:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>o,default:()=>x,frontMatter:()=>c,metadata:()=>i,toc:()=>h});const i=JSON.parse('{"id":"distributions/ondevice-distro/ios-sdk","title":"iOS SDK","description":"Native iOS development with Llama Stack using Swift SDK for remote and on-device inference","source":"@site/docs/distributions/ondevice-distro/ios-sdk.mdx","sourceDirName":"distributions/ondevice-distro","slug":"/distributions/ondevice-distro/ios-sdk","permalink":"/docs/distributions/ondevice-distro/ios-sdk","draft":false,"unlisted":false,"editUrl":"https://github.com/meta-llama/llama-stack/tree/main/docs/docs/distributions/ondevice-distro/ios-sdk.mdx","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"iOS SDK","description":"Native iOS development with Llama Stack using Swift SDK for remote and on-device inference","sidebar_label":"iOS SDK","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"watsonx","permalink":"/docs/distributions/remote-hosted-distro/watsonx"},"next":{"title":"Android SDK","permalink":"/docs/distributions/ondevice-distro/android-sdk"}}');var s=t(74848),l=t(28453),r=t(4865),a=t(19365);const c={title:"iOS SDK",description:"Native iOS development with Llama Stack using Swift SDK for remote and on-device inference",sidebar_label:"iOS SDK",sidebar_position:1},o="iOS SDK",d={},h=[{value:"Remote Only",id:"remote-only",level:2},{value:"Setup",id:"setup",level:3},{value:"LocalInference",id:"localinference",level:2},{value:"Installation",id:"installation",level:3},{value:"Preparing a Model",id:"preparing-a-model",level:3},{value:"Using LocalInference",id:"using-localinference",level:3},{value:"Troubleshooting",id:"troubleshooting",level:3},{value:"Performance Considerations",id:"performance-considerations",level:2},{value:"Use Cases",id:"use-cases",level:2},{value:"Related Resources",id:"related-resources",level:2}];function u(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,l.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"ios-sdk",children:"iOS SDK"})}),"\n",(0,s.jsxs)(n.p,{children:["We offer both remote and on-device use of Llama Stack in Swift via a single SDK ",(0,s.jsx)(n.a,{href:"https://github.com/meta-llama/llama-stack-client-swift/",children:"llama-stack-client-swift"})," that contains two components:"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LlamaStackClient"})," for remote inference"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Local Inference"})," for on-device inference"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Seamlessly switching between local, on-device inference and remote hosted inference",src:t(91379).A+"",width:"412",height:"412"})}),"\n",(0,s.jsx)(n.h2,{id:"remote-only",children:"Remote Only"}),"\n",(0,s.jsx)(n.p,{children:"If you don't want to run inference on-device, then you can connect to any hosted Llama Stack distribution with the remote client."}),"\n",(0,s.jsx)(n.h3,{id:"setup",children:"Setup"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Add ",(0,s.jsx)(n.code,{children:"https://github.com/meta-llama/llama-stack-client-swift/"})," as a Package Dependency in Xcode"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Add ",(0,s.jsx)(n.code,{children:"LlamaStackClient"})," as a framework to your app target"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Call an API:"}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-swift",children:'import LlamaStackClient\n\nlet agents = RemoteAgents(url: URL(string: "http://localhost:8321")!)\nlet request = Components.Schemas.CreateAgentTurnRequest(\n        agent_id: agentId,\n        messages: [\n          .UserMessage(Components.Schemas.UserMessage(\n            content: .case1("Hello Llama!"),\n            role: .user\n          ))\n        ],\n        session_id: self.agenticSystemSessionId,\n        stream: true\n      )\n\n      for try await chunk in try await agents.createTurn(request: request) {\n        let payload = chunk.event.payload\n      // ...\n'})}),"\n",(0,s.jsxs)(n.p,{children:["Check out ",(0,s.jsx)(n.a,{href:"https://github.com/meta-llama/llama-stack-client-swift/tree/main/examples/ios_calendar_assistant",children:"iOSCalendarAssistant"})," for a complete app demo."]}),"\n",(0,s.jsx)(n.h2,{id:"localinference",children:"LocalInference"}),"\n",(0,s.jsxs)(n.p,{children:["LocalInference provides a local inference implementation powered by ",(0,s.jsx)(n.a,{href:"https://github.com/pytorch/executorch/",children:"executorch"}),"."]}),"\n",(0,s.jsxs)(n.p,{children:["Llama Stack currently supports on-device inference for iOS with Android coming soon. You can run on-device inference on Android today using ",(0,s.jsx)(n.a,{href:"https://github.com/pytorch/executorch/tree/main/examples/demo-apps/android/LlamaDemo",children:"executorch"}),", PyTorch's on-device inference library."]}),"\n",(0,s.jsxs)(n.p,{children:["The APIs ",(0,s.jsx)(n.em,{children:"work the same as remote"})," \u2013 the only difference is you'll instead use the ",(0,s.jsx)(n.code,{children:"LocalAgents"})," / ",(0,s.jsx)(n.code,{children:"LocalInference"})," classes and pass in a ",(0,s.jsx)(n.code,{children:"DispatchQueue"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-swift",children:'private let runnerQueue = DispatchQueue(label: "org.llamastack.stacksummary")\nlet inference = LocalInference(queue: runnerQueue)\nlet agents = LocalAgents(inference: self.inference)\n'})}),"\n",(0,s.jsxs)(n.p,{children:["Check out ",(0,s.jsx)(n.a,{href:"https://github.com/meta-llama/llama-stack-client-swift/tree/main/examples/ios_calendar_assistant",children:"iOSCalendarAssistantWithLocalInf"})," for a complete app demo."]}),"\n",(0,s.jsx)(n.h3,{id:"installation",children:"Installation"}),"\n",(0,s.jsx)(n.admonition,{title:"Development Status",type:"info",children:(0,s.jsxs)(n.p,{children:["We're working on making LocalInference easier to set up. For now, you'll need to import it via ",(0,s.jsx)(n.code,{children:".xcframework"}),"."]})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Clone the executorch submodule in this repo and its dependencies: ",(0,s.jsx)(n.code,{children:"git submodule update --init --recursive"})]}),"\n",(0,s.jsxs)(n.li,{children:["Install ",(0,s.jsx)(n.a,{href:"https://cmake.org/",children:"Cmake"})," for the executorch build"]}),"\n",(0,s.jsxs)(n.li,{children:["Drag ",(0,s.jsx)(n.code,{children:"LocalInference.xcodeproj"})," into your project"]}),"\n",(0,s.jsxs)(n.li,{children:["Add ",(0,s.jsx)(n.code,{children:"LocalInference"})," as a framework in your app target"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"preparing-a-model",children:"Preparing a Model"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Prepare a ",(0,s.jsx)(n.code,{children:".pte"})," file ",(0,s.jsx)(n.a,{href:"https://github.com/pytorch/executorch/blob/main/examples/models/llama/README.md#step-2-prepare-model",children:"following the executorch docs"})]}),"\n",(0,s.jsxs)(n.li,{children:["Bundle the ",(0,s.jsx)(n.code,{children:".pte"})," and ",(0,s.jsx)(n.code,{children:"tokenizer.model"})," file into your app"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"We now support models quantized using SpinQuant and QAT-LoRA which offer a significant performance boost (demo app on iPhone 13 Pro):"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{style:{textAlign:"left"},children:"Llama 3.2 1B"}),(0,s.jsx)(n.th,{style:{textAlign:"left"},children:"Tokens / Second (total)"}),(0,s.jsx)(n.th,{style:{textAlign:"left"}}),(0,s.jsx)(n.th,{style:{textAlign:"left"},children:"Time-to-First-Token (sec)"}),(0,s.jsx)(n.th,{style:{textAlign:"left"}})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{style:{textAlign:"left"}}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"Haiku"}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"Paragraph"}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"Haiku"}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"Paragraph"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"BF16"}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"2.2"}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"2.5"}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"2.3"}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"1.9"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"QAT+LoRA"}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"7.1"}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"3.3"}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"0.37"}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"0.24"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"SpinQuant"}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"10.1"}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"5.2"}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"0.2"}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"0.2"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"using-localinference",children:"Using LocalInference"}),"\n",(0,s.jsxs)(r.A,{children:[(0,s.jsxs)(a.default,{value:"init",label:"1. Initialize",children:[(0,s.jsx)(n.p,{children:"Instantiate LocalInference with a DispatchQueue. Optionally, pass it into your agents service:"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-swift",children:'init () {\n  runnerQueue = DispatchQueue(label: "org.meta.llamastack")\n  inferenceService = LocalInferenceService(queue: runnerQueue)\n  agentsService = LocalAgentsService(inference: inferenceService)\n}\n'})})]}),(0,s.jsxs)(a.default,{value:"load",label:"2. Load Model",children:[(0,s.jsx)(n.p,{children:"Before making any inference calls, load your model from your bundle:"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-swift",children:'let mainBundle = Bundle.main\ninferenceService.loadModel(\n    modelPath: mainBundle.url(forResource: "llama32_1b_spinquant", withExtension: "pte"),\n    tokenizerPath: mainBundle.url(forResource: "tokenizer", withExtension: "model"),\n    completion: {_ in } // use to handle load failures\n)\n'})})]}),(0,s.jsxs)(a.default,{value:"inference",label:"3. Make Inference Calls",children:[(0,s.jsx)(n.p,{children:"Make inference calls (or agents calls) as you normally would with LlamaStack:"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-swift",children:'for await chunk in try await agentsService.initAndCreateTurn(\n    messages: [\n    .UserMessage(Components.Schemas.UserMessage(\n        content: .case1("Call functions as needed to handle any actions in the following text:\\n\\n" + text),\n        role: .user))\n    ]\n) {\n'})})]})]}),"\n",(0,s.jsx)(n.h3,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,s.jsx)(n.p,{children:'If you receive errors like "missing package product" or "invalid checksum", try cleaning the build folder and resetting the Swift package cache:'}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"(Opt+Click) Product > Clean Build Folder Immediately"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"rm -rf \\\n  ~/Library/org.swift.swiftpm \\\n  ~/Library/Caches/org.swift.swiftpm \\\n  ~/Library/Caches/com.apple.dt.Xcode \\\n  ~/Library/Developer/Xcode/DerivedData\n"})}),"\n",(0,s.jsx)(n.h2,{id:"performance-considerations",children:"Performance Considerations"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Model Size"}),": Smaller models (1B-3B parameters) work best on mobile devices"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Quantization"}),": Use SpinQuant or QAT-LoRA for optimal performance"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Memory Usage"}),": Monitor app memory usage with larger models"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Battery Life"}),": On-device inference can impact battery performance"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"use-cases",children:"Use Cases"}),"\n",(0,s.jsx)(n.p,{children:"The iOS SDK is ideal for:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Native iOS applications"})," requiring AI capabilities"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Offline functionality"})," without internet dependency"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Privacy-focused"})," applications processing sensitive data locally"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Real-time inference"})," with low latency requirements"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Hybrid applications"})," switching between local and remote inference"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"related-resources",children:"Related Resources"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.a,{href:"https://github.com/meta-llama/llama-stack-client-swift/",children:"llama-stack-client-swift"})})," - Official Swift SDK repository"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.a,{href:"https://github.com/meta-llama/llama-stack-client-swift/tree/main/examples/ios_calendar_assistant",children:"iOS Calendar Assistant"})})," - Complete example app"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.a,{href:"https://github.com/pytorch/executorch/",children:"executorch"})})," - PyTorch on-device inference library"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.a,{href:"./android-sdk",children:"Android SDK"})})," - Android development guide"]}),"\n"]})]})}function x(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(u,{...e})}):u(e)}},4865:(e,n,t)=>{t.d(n,{A:()=>x});var i=t(96540),s=t(34164),l=t(23104),r=t(47751),a=t(92303);const c={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var o=t(74848);function d(e){var n=e.className,t=e.block,i=e.selectedValue,r=e.selectValue,a=e.tabValues,d=[],h=(0,l.a_)().blockElementScrollPositionUntilNextRender,u=function(e){var n=e.currentTarget,t=d.indexOf(n),s=a[t].value;s!==i&&(h(n),r(s))},x=function(e){var n,t=null;switch(e.key){case"Enter":u(e);break;case"ArrowRight":var i,s=d.indexOf(e.currentTarget)+1;t=null!=(i=d[s])?i:d[0];break;case"ArrowLeft":var l,r=d.indexOf(e.currentTarget)-1;t=null!=(l=d[r])?l:d[d.length-1]}null==(n=t)||n.focus()};return(0,o.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,s.A)("tabs",{"tabs--block":t},n),children:a.map(function(e){var n=e.value,t=e.label,l=e.attributes;return(0,o.jsx)("li",Object.assign({role:"tab",tabIndex:i===n?0:-1,"aria-selected":i===n,ref:function(e){d.push(e)},onKeyDown:x,onClick:u},l,{className:(0,s.A)("tabs__item",c.tabItem,null==l?void 0:l.className,{"tabs__item--active":i===n}),children:null!=t?t:n}),n)})})}function h(e){var n=e.lazy,t=e.children,l=e.selectedValue,r=(Array.isArray(t)?t:[t]).filter(Boolean);if(n){var a=r.find(function(e){return e.props.value===l});return a?(0,i.cloneElement)(a,{className:(0,s.A)("margin-top--md",a.props.className)}):null}return(0,o.jsx)("div",{className:"margin-top--md",children:r.map(function(e,n){return(0,i.cloneElement)(e,{key:n,hidden:e.props.value!==l})})})}function u(e){var n=(0,r.u)(e);return(0,o.jsxs)("div",{className:(0,s.A)("tabs-container",c.tabList),children:[(0,o.jsx)(d,Object.assign({},n,e)),(0,o.jsx)(h,Object.assign({},n,e))]})}function x(e){var n=(0,a.default)();return(0,o.jsx)(u,Object.assign({},e,{children:(0,r.v)(e.children)}),String(n))}},91379:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/remote_or_local-1ddf31143dd6f2bae3487e54ab3a6380.gif"}}]);