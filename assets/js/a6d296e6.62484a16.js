"use strict";(self.webpackChunkdocusaurus_template_openapi_docs=self.webpackChunkdocusaurus_template_openapi_docs||[]).push([[5348],{18570:(e,a,n)=>{n.r(a),n.d(a,{assets:()=>r,contentTitle:()=>t,default:()=>c,frontMatter:()=>d,metadata:()=>l,toc:()=>i});const l=JSON.parse('{"id":"references/llama-cli","title":"llama (server-side) CLI Reference","description":"The llama CLI tool helps you set up and use the Llama Stack. The CLI is available on your path after installing the llama-stack package.","source":"@site/docs/references/llama-cli.mdx","sourceDirName":"references","slug":"/references/llama-cli","permalink":"/llama-stack/docs/references/llama-cli","draft":false,"unlisted":false,"editUrl":"https://github.com/meta-llama/llama-stack/tree/main/docs/docs/references/llama-cli.mdx","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"References","permalink":"/llama-stack/docs/references/"},"next":{"title":"llama (client-side) CLI Reference","permalink":"/llama-stack/docs/references/client-cli"}}');var s=n(74848),o=n(28453);const d={},t="llama (server-side) CLI Reference",r={},i=[{value:"Installation",id:"installation",level:2},{value:"<code>llama</code> subcommands",id:"llama-subcommands",level:2},{value:"Sample Usage",id:"sample-usage",level:3},{value:"Downloading models",id:"downloading-models",level:2},{value:"Downloading from Meta",id:"downloading-from-meta",level:3},{value:"Downloading from Hugging Face",id:"downloading-from-hugging-face",level:3},{value:"List the downloaded models",id:"list-the-downloaded-models",level:2},{value:"Understand the models",id:"understand-the-models",level:2},{value:"Sample Usage",id:"sample-usage-1",level:3},{value:"Describe",id:"describe",level:3},{value:"Prompt Format",id:"prompt-format",level:3},{value:"Remove model",id:"remove-model",level:3}];function m(e){const a={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(a.header,{children:(0,s.jsx)(a.h1,{id:"llama-server-side-cli-reference",children:"llama (server-side) CLI Reference"})}),"\n",(0,s.jsxs)(a.p,{children:["The ",(0,s.jsx)(a.code,{children:"llama"})," CLI tool helps you set up and use the Llama Stack. The CLI is available on your path after installing the ",(0,s.jsx)(a.code,{children:"llama-stack"})," package."]}),"\n",(0,s.jsx)(a.h2,{id:"installation",children:"Installation"}),"\n",(0,s.jsx)(a.p,{children:"You have two ways to install Llama Stack:"}),"\n",(0,s.jsxs)(a.ol,{children:["\n",(0,s.jsxs)(a.li,{children:["\n",(0,s.jsxs)(a.p,{children:[(0,s.jsx)(a.strong,{children:"Install as a package"}),":\nYou can install the repository directly from ",(0,s.jsx)(a.a,{href:"https://pypi.org/project/llama-stack/",children:"PyPI"})," by running the following command:"]}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-bash",children:"pip install llama-stack\n"})}),"\n"]}),"\n",(0,s.jsxs)(a.li,{children:["\n",(0,s.jsxs)(a.p,{children:[(0,s.jsx)(a.strong,{children:"Install from source"}),":\nIf you prefer to install from the source code, follow these steps:"]}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-bash",children:" mkdir -p ~/local\n cd ~/local\n git clone git@github.com:meta-llama/llama-stack.git\n\n uv venv myenv --python 3.12\n source myenv/bin/activate  # On Windows: myenv\\Scripts\\activate\n\n cd llama-stack\n pip install -e .\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(a.h2,{id:"llama-subcommands",children:[(0,s.jsx)(a.code,{children:"llama"})," subcommands"]}),"\n",(0,s.jsxs)(a.ol,{children:["\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.code,{children:"download"}),": Supports downloading models from Meta or Hugging Face."]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.code,{children:"model"}),": Lists available models and their properties."]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.code,{children:"stack"}),": Allows you to build a stack using the ",(0,s.jsx)(a.code,{children:"llama stack"})," distribution and run a Llama Stack server. You can read more about how to build a Llama Stack distribution in the ",(0,s.jsx)(a.a,{href:"../distributions/building-distro",children:"Build your own Distribution"})," documentation."]}),"\n"]}),"\n",(0,s.jsx)(a.h3,{id:"sample-usage",children:"Sample Usage"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-bash",children:"llama --help\n"})}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{children:"usage: llama [-h] {download,model,stack} ...\n\nWelcome to the Llama CLI\n\noptions:\n  -h, --help            show this help message and exit\n\nsubcommands:\n  {download,model,stack}\n"})}),"\n",(0,s.jsx)(a.h2,{id:"downloading-models",children:"Downloading models"}),"\n",(0,s.jsx)(a.p,{children:"You first need to have models downloaded locally."}),"\n",(0,s.jsxs)(a.p,{children:["To download any model you need the ",(0,s.jsx)(a.strong,{children:"Model Descriptor"}),".\nThis can be obtained by running the command"]}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-bash",children:"llama model list\n"})}),"\n",(0,s.jsx)(a.p,{children:"You should see a table like this:"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{children:"+----------------------------------+------------------------------------------+----------------+\n| Model Descriptor(ID)             | Hugging Face Repo                        | Context Length |\n+----------------------------------+------------------------------------------+----------------+\n| Llama3.1-8B                      | meta-llama/Llama-3.1-8B                  | 128K           |\n+----------------------------------+------------------------------------------+----------------+\n| Llama3.1-70B                     | meta-llama/Llama-3.1-70B                 | 128K           |\n+----------------------------------+------------------------------------------+----------------+\n| Llama3.1-405B:bf16-mp8           | meta-llama/Llama-3.1-405B                | 128K           |\n+----------------------------------+------------------------------------------+----------------+\n| Llama3.1-405B                    | meta-llama/Llama-3.1-405B-FP8            | 128K           |\n+----------------------------------+------------------------------------------+----------------+\n| Llama3.1-405B:bf16-mp16          | meta-llama/Llama-3.1-405B                | 128K           |\n+----------------------------------+------------------------------------------+----------------+\n| Llama3.1-8B-Instruct             | meta-llama/Llama-3.1-8B-Instruct         | 128K           |\n+----------------------------------+------------------------------------------+----------------+\n| Llama3.1-70B-Instruct            | meta-llama/Llama-3.1-70B-Instruct        | 128K           |\n+----------------------------------+------------------------------------------+----------------+\n| Llama3.1-405B-Instruct:bf16-mp8  | meta-llama/Llama-3.1-405B-Instruct       | 128K           |\n+----------------------------------+------------------------------------------+----------------+\n| Llama3.1-405B-Instruct           | meta-llama/Llama-3.1-405B-Instruct-FP8   | 128K           |\n+----------------------------------+------------------------------------------+----------------+\n| Llama3.1-405B-Instruct:bf16-mp16 | meta-llama/Llama-3.1-405B-Instruct       | 128K           |\n+----------------------------------+------------------------------------------+----------------+\n| Llama3.2-1B                      | meta-llama/Llama-3.2-1B                  | 128K           |\n+----------------------------------+------------------------------------------+----------------+\n| Llama3.2-3B                      | meta-llama/Llama-3.2-3B                  | 128K           |\n+----------------------------------+------------------------------------------+----------------+\n| Llama3.2-11B-Vision              | meta-llama/Llama-3.2-11B-Vision          | 128K           |\n+----------------------------------+------------------------------------------+----------------+\n| Llama3.2-90B-Vision              | meta-llama/Llama-3.2-90B-Vision          | 128K           |\n+----------------------------------+------------------------------------------+----------------+\n| Llama3.2-1B-Instruct             | meta-llama/Llama-3.2-1B-Instruct         | 128K           |\n+----------------------------------+------------------------------------------+----------------+\n| Llama3.2-3B-Instruct             | meta-llama/Llama-3.2-3B-Instruct         | 128K           |\n+----------------------------------+------------------------------------------+----------------+\n| Llama3.2-11B-Vision-Instruct     | meta-llama/Llama-3.2-11B-Vision-Instruct | 128K           |\n+----------------------------------+------------------------------------------+----------------+\n| Llama3.2-90B-Vision-Instruct     | meta-llama/Llama-3.2-90B-Vision-Instruct | 128K           |\n+----------------------------------+------------------------------------------+----------------+\n| Llama-Guard-3-11B-Vision         | meta-llama/Llama-Guard-3-11B-Vision      | 128K           |\n+----------------------------------+------------------------------------------+----------------+\n| Llama-Guard-3-1B:int4-mp1        | meta-llama/Llama-Guard-3-1B-INT4         | 128K           |\n+----------------------------------+------------------------------------------+----------------+\n| Llama-Guard-3-1B                 | meta-llama/Llama-Guard-3-1B              | 128K           |\n+----------------------------------+------------------------------------------+----------------+\n| Llama-Guard-3-8B                 | meta-llama/Llama-Guard-3-8B              | 128K           |\n+----------------------------------+------------------------------------------+----------------+\n| Llama-Guard-3-8B:int8-mp1        | meta-llama/Llama-Guard-3-8B-INT8         | 128K           |\n+----------------------------------+------------------------------------------+----------------+\n| Prompt-Guard-86M                 | meta-llama/Prompt-Guard-86M              | 128K           |\n+----------------------------------+------------------------------------------+----------------+\n| Llama-Guard-2-8B                 | meta-llama/Llama-Guard-2-8B              | 4K             |\n+----------------------------------+------------------------------------------+----------------+\n"})}),"\n",(0,s.jsxs)(a.p,{children:["To download models, you can use the ",(0,s.jsx)(a.code,{children:"llama download"})," command."]}),"\n",(0,s.jsxs)(a.h3,{id:"downloading-from-meta",children:["Downloading from ",(0,s.jsx)(a.a,{href:"https://llama.meta.com/llama-downloads/",children:"Meta"})]}),"\n",(0,s.jsxs)(a.p,{children:["Here is an example download command to get the 3B-Instruct/11B-Vision-Instruct model. You will need META_URL which can be obtained from ",(0,s.jsx)(a.a,{href:"https://llama.meta.com/docs/getting_the_models/meta/",children:"here"})]}),"\n",(0,s.jsx)(a.p,{children:"Download the required checkpoints using the following commands:"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-bash",children:"# download the 8B model, this can be run on a single GPU\nllama download --source meta --model-id Llama3.2-3B-Instruct --meta-url META_URL\n\n# you can also get the 70B model, this will require 8 GPUs however\nllama download --source meta --model-id Llama3.2-11B-Vision-Instruct --meta-url META_URL\n\n# llama-agents have safety enabled by default. For this, you will need\n# safety models -- Llama-Guard and Prompt-Guard\nllama download --source meta --model-id Prompt-Guard-86M --meta-url META_URL\nllama download --source meta --model-id Llama-Guard-3-1B --meta-url META_URL\n"})}),"\n",(0,s.jsxs)(a.h3,{id:"downloading-from-hugging-face",children:["Downloading from ",(0,s.jsx)(a.a,{href:"https://huggingface.co/meta-llama",children:"Hugging Face"})]}),"\n",(0,s.jsxs)(a.p,{children:["Essentially, the same commands above work, just replace ",(0,s.jsx)(a.code,{children:"--source meta"})," with ",(0,s.jsx)(a.code,{children:"--source huggingface"}),"."]}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-bash",children:"llama download --source huggingface --model-id  Llama3.1-8B-Instruct --hf-token <HF_TOKEN>\n\nllama download --source huggingface --model-id Llama3.1-70B-Instruct --hf-token <HF_TOKEN>\n\nllama download --source huggingface --model-id Llama-Guard-3-1B --ignore-patterns *original*\nllama download --source huggingface --model-id Prompt-Guard-86M --ignore-patterns *original*\n"})}),"\n",(0,s.jsxs)(a.p,{children:[(0,s.jsx)(a.strong,{children:"Important:"})," Set your environment variable ",(0,s.jsx)(a.code,{children:"HF_TOKEN"})," or pass in ",(0,s.jsx)(a.code,{children:"--hf-token"})," to the command to validate your access. You can find your token at ",(0,s.jsx)(a.a,{href:"https://huggingface.co/settings/tokens",children:"https://huggingface.co/settings/tokens"}),"."]}),"\n",(0,s.jsx)(a.admonition,{type:"tip",children:(0,s.jsxs)(a.p,{children:["Default for ",(0,s.jsx)(a.code,{children:"llama download"})," is to run with ",(0,s.jsx)(a.code,{children:"--ignore-patterns *.safetensors"})," since we use the ",(0,s.jsx)(a.code,{children:".pth"})," files in the ",(0,s.jsx)(a.code,{children:"original"})," folder. For Llama Guard and Prompt Guard, however, we need safetensors. Hence, please run with ",(0,s.jsx)(a.code,{children:"--ignore-patterns original"})," so that safetensors are downloaded and ",(0,s.jsx)(a.code,{children:".pth"})," files are ignored."]})}),"\n",(0,s.jsx)(a.h2,{id:"list-the-downloaded-models",children:"List the downloaded models"}),"\n",(0,s.jsx)(a.p,{children:"To list the downloaded models with the following command:"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-bash",children:"llama model list --downloaded\n"})}),"\n",(0,s.jsx)(a.p,{children:"You should see a table like this:"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{children:"\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Model                                   \u2503 Size     \u2503 Modified Time       \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 Llama3.2-1B-Instruct:int4-qlora-eo8     \u2502 1.53 GB  \u2502 2025-02-26 11:22:28 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Llama3.2-1B                             \u2502 2.31 GB  \u2502 2025-02-18 21:48:52 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Prompt-Guard-86M                        \u2502 0.02 GB  \u2502 2025-02-26 11:29:28 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Llama3.2-3B-Instruct:int4-spinquant-eo8 \u2502 3.69 GB  \u2502 2025-02-26 11:37:41 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Llama3.2-3B                             \u2502 5.99 GB  \u2502 2025-02-18 21:51:26 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Llama3.1-8B                             \u2502 14.97 GB \u2502 2025-02-16 10:36:37 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Llama3.2-1B-Instruct:int4-spinquant-eo8 \u2502 1.51 GB  \u2502 2025-02-26 11:35:02 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Llama-Guard-3-1B                        \u2502 2.80 GB  \u2502 2025-02-26 11:20:46 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Llama-Guard-3-1B:int4                   \u2502 0.43 GB  \u2502 2025-02-26 11:33:33 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsx)(a.h2,{id:"understand-the-models",children:"Understand the models"}),"\n",(0,s.jsxs)(a.p,{children:["The ",(0,s.jsx)(a.code,{children:"llama model"})," command helps you explore the model's interface."]}),"\n",(0,s.jsxs)(a.ol,{children:["\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.code,{children:"download"}),": Download the model from different sources. (meta, huggingface)"]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.code,{children:"list"}),": Lists all the models available for download with hardware requirements for deploying the models."]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.code,{children:"prompt-format"}),": Show llama model message formats."]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.code,{children:"describe"}),": Describes all the properties of the model."]}),"\n"]}),"\n",(0,s.jsx)(a.h3,{id:"sample-usage-1",children:"Sample Usage"}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.code,{children:"llama model <subcommand> <options>"})}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-bash",children:"llama model --help\n"})}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{children:"usage: llama model [-h] {download,list,prompt-format,describe,verify-download,remove} ...\n\nWork with llama models\n\noptions:\n  -h, --help            show this help message and exit\n\nmodel_subcommands:\n  {download,list,prompt-format,describe,verify-download,remove}\n"})}),"\n",(0,s.jsx)(a.h3,{id:"describe",children:"Describe"}),"\n",(0,s.jsx)(a.p,{children:"You can use the describe command to know more about a model:"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-bash",children:"llama model describe -m Llama3.2-3B-Instruct\n"})}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{children:'+-----------------------------+----------------------------------+\n| Model                       | Llama3.2-3B-Instruct             |\n+-----------------------------+----------------------------------+\n| Hugging Face ID             | meta-llama/Llama-3.2-3B-Instruct |\n+-----------------------------+----------------------------------+\n| Description                 | Llama 3.2 3b instruct model      |\n+-----------------------------+----------------------------------+\n| Context Length              | 128K tokens                      |\n+-----------------------------+----------------------------------+\n| Weights format              | bf16                             |\n+-----------------------------+----------------------------------+\n| Model params.json           | {                                |\n|                             |     "dim": 3072,                 |\n|                             |     "n_layers": 28,              |\n|                             |     "n_heads": 24,               |\n|                             |     "n_kv_heads": 8,             |\n|                             |     "vocab_size": 128256,        |\n|                             |     "ffn_dim_multiplier": 1.0,   |\n|                             |     "multiple_of": 256,          |\n|                             |     "norm_eps": 1e-05,           |\n|                             |     "rope_theta": 500000.0,      |\n|                             |     "use_scaled_rope": true      |\n|                             | }                                |\n+-----------------------------+----------------------------------+\n| Recommended sampling params | {                                |\n|                             |     "temperature": 1.0,          |\n|                             |     "top_p": 0.9,                |\n|                             |     "top_k": 0                   |\n|                             | }                                |\n+-----------------------------+----------------------------------+\n'})}),"\n",(0,s.jsx)(a.h3,{id:"prompt-format",children:"Prompt Format"}),"\n",(0,s.jsxs)(a.p,{children:["You can even run ",(0,s.jsx)(a.code,{children:"llama model prompt-format"})," see all of the templates and their tokens:"]}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-bash",children:"llama model prompt-format -m Llama3.2-3B-Instruct\n"})}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.img,{alt:"Prompt Format Example",src:n(68988).A+"",width:"1438",height:"1024"})}),"\n",(0,s.jsx)(a.p,{children:"You will be shown a Markdown formatted description of the model interface and how prompts / messages are formatted for various scenarios."}),"\n",(0,s.jsxs)(a.p,{children:[(0,s.jsx)(a.strong,{children:"NOTE"}),": Outputs in terminal are color printed to show special tokens."]}),"\n",(0,s.jsx)(a.h3,{id:"remove-model",children:"Remove model"}),"\n",(0,s.jsxs)(a.p,{children:["You can run ",(0,s.jsx)(a.code,{children:"llama model remove"})," to remove an unnecessary model:"]}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-bash",children:"llama model remove -m Llama-Guard-3-8B-int8\n"})})]})}function c(e={}){const{wrapper:a}={...(0,o.R)(),...e.components};return a?(0,s.jsx)(a,{...e,children:(0,s.jsx)(m,{...e})}):m(e)}},28453:(e,a,n)=>{n.d(a,{R:()=>d,x:()=>t});var l=n(96540);const s={},o=l.createContext(s);function d(e){const a=l.useContext(o);return l.useMemo(function(){return"function"==typeof e?e(a):{...a,...e}},[a,e])}function t(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:d(e.components),l.createElement(o.Provider,{value:a},e.children)}},68988:(e,a,n)=>{n.d(a,{A:()=>l});const l=n.p+"assets/images/prompt-format-8ff638c395ccfa264adcfaa25403a985.png"}}]);