"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[4098],{53418:(e,r,i)=>{i.r(r),i.d(r,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>n,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"concepts/api-providers","title":"API Providers","description":"Understanding remote vs inline provider implementations","source":"@site/docs/concepts/api-providers.mdx","sourceDirName":"concepts","slug":"/concepts/api-providers","permalink":"/llama-stack/docs/concepts/api-providers","draft":false,"unlisted":false,"editUrl":"https://github.com/meta-llama/llama-stack/tree/main/docs/docs/concepts/api-providers.mdx","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"API Providers","description":"Understanding remote vs inline provider implementations","sidebar_label":"API Providers","sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"APIs","permalink":"/llama-stack/docs/concepts/apis"},"next":{"title":"Distributions","permalink":"/llama-stack/docs/concepts/distributions"}}');var t=i(74848),a=i(28453);const n={title:"API Providers",description:"Understanding remote vs inline provider implementations",sidebar_label:"API Providers",sidebar_position:4},o="API Providers",l={},d=[];function c(e){const r={h1:"h1",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(r.header,{children:(0,t.jsx)(r.h1,{id:"api-providers",children:"API Providers"})}),"\n",(0,t.jsx)(r.p,{children:"The goal of Llama Stack is to build an ecosystem where users can easily swap out different implementations for the same API. Examples for these include:"}),"\n",(0,t.jsxs)(r.ul,{children:["\n",(0,t.jsx)(r.li,{children:"LLM inference providers (e.g., Fireworks, Together, AWS Bedrock, Groq, Cerebras, SambaNova, vLLM, etc.),"}),"\n",(0,t.jsx)(r.li,{children:"Vector databases (e.g., ChromaDB, Weaviate, Qdrant, Milvus, FAISS, PGVector, etc.),"}),"\n",(0,t.jsx)(r.li,{children:"Safety providers (e.g., Meta's Llama Guard, AWS Bedrock Guardrails, etc.)"}),"\n"]}),"\n",(0,t.jsx)(r.p,{children:"Providers come in two flavors:"}),"\n",(0,t.jsxs)(r.ul,{children:["\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Remote"}),": the provider runs as a separate service external to the Llama Stack codebase. Llama Stack contains a small amount of adapter code."]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Inline"}),": the provider is fully specified and implemented within the Llama Stack codebase. It may be a simple wrapper around an existing library, or a full fledged implementation within Llama Stack."]}),"\n"]}),"\n",(0,t.jsx)(r.p,{children:"Most importantly, Llama Stack always strives to provide at least one fully inline provider for each API so you can iterate on a fully featured environment locally."})]})}function p(e={}){const{wrapper:r}={...(0,a.R)(),...e.components};return r?(0,t.jsx)(r,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}}}]);