"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[6326],{4865:(e,n,i)=>{i.d(n,{A:()=>p});var s=i(96540),a=i(34164),t=i(23104),l=i(47751),r=i(92303);const o={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var d=i(74848);function c(e){var n=e.className,i=e.block,s=e.selectedValue,l=e.selectValue,r=e.tabValues,c=[],h=(0,t.a_)().blockElementScrollPositionUntilNextRender,u=function(e){var n=e.currentTarget,i=c.indexOf(n),a=r[i].value;a!==s&&(h(n),l(a))},p=function(e){var n,i=null;switch(e.key){case"Enter":u(e);break;case"ArrowRight":var s,a=c.indexOf(e.currentTarget)+1;i=null!=(s=c[a])?s:c[0];break;case"ArrowLeft":var t,l=c.indexOf(e.currentTarget)-1;i=null!=(t=c[l])?t:c[c.length-1]}null==(n=i)||n.focus()};return(0,d.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,a.A)("tabs",{"tabs--block":i},n),children:r.map(function(e){var n=e.value,i=e.label,t=e.attributes;return(0,d.jsx)("li",Object.assign({role:"tab",tabIndex:s===n?0:-1,"aria-selected":s===n,ref:function(e){c.push(e)},onKeyDown:p,onClick:u},t,{className:(0,a.A)("tabs__item",o.tabItem,null==t?void 0:t.className,{"tabs__item--active":s===n}),children:null!=i?i:n}),n)})})}function h(e){var n=e.lazy,i=e.children,t=e.selectedValue,l=(Array.isArray(i)?i:[i]).filter(Boolean);if(n){var r=l.find(function(e){return e.props.value===t});return r?(0,s.cloneElement)(r,{className:(0,a.A)("margin-top--md",r.props.className)}):null}return(0,d.jsx)("div",{className:"margin-top--md",children:l.map(function(e,n){return(0,s.cloneElement)(e,{key:n,hidden:e.props.value!==t})})})}function u(e){var n=(0,l.u)(e);return(0,d.jsxs)("div",{className:(0,a.A)("tabs-container",o.tabList),children:[(0,d.jsx)(c,Object.assign({},n,e)),(0,d.jsx)(h,Object.assign({},n,e))]})}function p(e){var n=(0,r.default)();return(0,d.jsx)(u,Object.assign({},e,{children:(0,l.v)(e.children)}),String(n))}},88299:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>d,default:()=>p,frontMatter:()=>o,metadata:()=>s,toc:()=>h});const s=JSON.parse('{"id":"building-applications/safety","title":"Safety Guardrails","description":"Implement safety measures and content moderation in Llama Stack applications","source":"@site/docs/building-applications/safety.mdx","sourceDirName":"building-applications","slug":"/building-applications/safety","permalink":"/docs/building-applications/safety","draft":false,"unlisted":false,"editUrl":"https://github.com/meta-llama/llama-stack/tree/main/docs/docs/building-applications/safety.mdx","tags":[],"version":"current","sidebarPosition":9,"frontMatter":{"title":"Safety Guardrails","description":"Implement safety measures and content moderation in Llama Stack applications","sidebar_label":"Safety","sidebar_position":9},"sidebar":"tutorialSidebar","previous":{"title":"Telemetry","permalink":"/docs/building-applications/telemetry"},"next":{"title":"Playground","permalink":"/docs/building-applications/playground"}}');var a=i(74848),t=i(28453),l=i(4865),r=i(19365);const o={title:"Safety Guardrails",description:"Implement safety measures and content moderation in Llama Stack applications",sidebar_label:"Safety",sidebar_position:9},d="Safety Guardrails",c={},h=[{value:"Shield System Overview",id:"shield-system-overview",level:2},{value:"Basic Shield Usage",id:"basic-shield-usage",level:2},{value:"Registering a Safety Shield",id:"registering-a-safety-shield",level:3},{value:"Agent Integration",id:"agent-integration",level:2},{value:"Available Shield Types",id:"available-shield-types",level:2},{value:"Llama Guard Shields",id:"llama-guard-shields",level:3},{value:"Custom Safety Shields",id:"custom-safety-shields",level:3},{value:"Safety Response Handling",id:"safety-response-handling",level:2},{value:"Safety Configuration Best Practices",id:"safety-configuration-best-practices",level:2},{value:"\ud83d\udee1\ufe0f <strong>Multi-Layer Protection</strong>",id:"\ufe0f-multi-layer-protection",level:3},{value:"\ud83d\udcca <strong>Monitoring &amp; Auditing</strong>",id:"-monitoring--auditing",level:3},{value:"\u2699\ufe0f <strong>Configuration Management</strong>",id:"\ufe0f-configuration-management",level:3},{value:"\ud83d\udd27 <strong>Integration Patterns</strong>",id:"-integration-patterns",level:3},{value:"Advanced Safety Scenarios",id:"advanced-safety-scenarios",level:2},{value:"Context-Aware Safety",id:"context-aware-safety",level:3},{value:"Dynamic Shield Selection",id:"dynamic-shield-selection",level:3},{value:"Compliance and Regulations",id:"compliance-and-regulations",level:2},{value:"Industry-Specific Safety",id:"industry-specific-safety",level:3},{value:"Related Resources",id:"related-resources",level:2}];function u(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"safety-guardrails",children:"Safety Guardrails"})}),"\n",(0,a.jsx)(n.p,{children:"Safety is a critical component of any AI application. Llama Stack provides a comprehensive Shield system that can be applied at multiple touchpoints to ensure responsible AI behavior and content moderation."}),"\n",(0,a.jsx)(n.h2,{id:"shield-system-overview",children:"Shield System Overview"}),"\n",(0,a.jsx)(n.p,{children:"The Shield system in Llama Stack provides:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Content filtering"})," for both input and output messages"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Multi-touchpoint protection"})," across your application flow"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Configurable safety policies"})," tailored to your use case"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Integration with agents"})," for automated safety enforcement"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"basic-shield-usage",children:"Basic Shield Usage"}),"\n",(0,a.jsx)(n.h3,{id:"registering-a-safety-shield",children:"Registering a Safety Shield"}),"\n",(0,a.jsxs)(l.A,{children:[(0,a.jsx)(r.default,{value:"registration",label:"Shield Registration",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Register a safety shield\nshield_id = "content_safety"\nclient.shields.register(\n    shield_id=shield_id, \n    provider_shield_id="llama-guard-basic"\n)\n'})})}),(0,a.jsx)(r.default,{value:"manual-check",label:"Manual Safety Check",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Run content through shield manually\nresponse = client.safety.run_shield(\n    shield_id=shield_id, \n    messages=[{"role": "user", "content": "User message here"}]\n)\n\nif response.violation:\n    print(f"Safety violation detected: {response.violation.user_message}")\n    # Handle violation appropriately\nelse:\n    print("Content passed safety checks")\n'})})})]}),"\n",(0,a.jsx)(n.h2,{id:"agent-integration",children:"Agent Integration"}),"\n",(0,a.jsx)(n.p,{children:"Shields can be automatically applied to agent interactions for seamless safety enforcement:"}),"\n",(0,a.jsxs)(l.A,{children:[(0,a.jsx)(r.default,{value:"input-shields",label:"Input Shields",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from llama_stack_client import Agent\n\n# Create agent with input safety shields\nagent = Agent(\n    client,\n    model="meta-llama/Llama-3.2-3B-Instruct",\n    instructions="You are a helpful assistant",\n    input_shields=["content_safety"],  # Shield user inputs\n    tools=["builtin::websearch"],\n)\n\nsession_id = agent.create_session("safe_session")\n\n# All user inputs will be automatically screened\nresponse = agent.create_turn(\n    messages=[{"role": "user", "content": "Tell me about AI safety"}],\n    session_id=session_id,\n)\n'})})}),(0,a.jsx)(r.default,{value:"output-shields",label:"Output Shields",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Create agent with output safety shields\nagent = Agent(\n    client,\n    model="meta-llama/Llama-3.2-3B-Instruct", \n    instructions="You are a helpful assistant",\n    output_shields=["content_safety"],  # Shield agent outputs\n    tools=["builtin::websearch"],\n)\n\nsession_id = agent.create_session("safe_session")\n\n# All agent responses will be automatically screened\nresponse = agent.create_turn(\n    messages=[{"role": "user", "content": "Help me with my research"}],\n    session_id=session_id,\n)\n'})})}),(0,a.jsx)(r.default,{value:"both-shields",label:"Input & Output Shields",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Create agent with comprehensive safety coverage\nagent = Agent(\n    client,\n    model="meta-llama/Llama-3.2-3B-Instruct",\n    instructions="You are a helpful assistant",\n    input_shields=["content_safety"],   # Screen user inputs\n    output_shields=["content_safety"],  # Screen agent outputs\n    tools=["builtin::websearch"],\n)\n\nsession_id = agent.create_session("fully_protected_session")\n\n# Both input and output are automatically protected\nresponse = agent.create_turn(\n    messages=[{"role": "user", "content": "Research question here"}],\n    session_id=session_id,\n)\n'})})})]}),"\n",(0,a.jsx)(n.h2,{id:"available-shield-types",children:"Available Shield Types"}),"\n",(0,a.jsx)(n.h3,{id:"llama-guard-shields",children:"Llama Guard Shields"}),"\n",(0,a.jsx)(n.p,{children:"Llama Guard provides state-of-the-art content safety classification:"}),"\n",(0,a.jsxs)(l.A,{children:[(0,a.jsxs)(r.default,{value:"basic",label:"Basic Llama Guard",children:[(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Basic Llama Guard for general content safety\nclient.shields.register(\n    shield_id="llama_guard_basic",\n    provider_shield_id="llama-guard-basic"\n)\n'})}),(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Use Cases:"})}),(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"General content moderation"}),"\n",(0,a.jsx)(n.li,{children:"Harmful content detection"}),"\n",(0,a.jsx)(n.li,{children:"Basic safety compliance"}),"\n"]})]}),(0,a.jsxs)(r.default,{value:"advanced",label:"Advanced Llama Guard",children:[(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Advanced Llama Guard with custom categories\nclient.shields.register(\n    shield_id="llama_guard_advanced",\n    provider_shield_id="llama-guard-advanced",\n    config={\n        "categories": [\n            "violence", "hate_speech", "sexual_content", \n            "self_harm", "illegal_activity"\n        ],\n        "threshold": 0.8\n    }\n)\n'})}),(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Use Cases:"})}),(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Fine-tuned safety policies"}),"\n",(0,a.jsx)(n.li,{children:"Domain-specific content filtering"}),"\n",(0,a.jsx)(n.li,{children:"Enterprise compliance requirements"}),"\n"]})]})]}),"\n",(0,a.jsx)(n.h3,{id:"custom-safety-shields",children:"Custom Safety Shields"}),"\n",(0,a.jsx)(n.p,{children:"Create domain-specific safety shields for specialized use cases:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Register custom safety shield\nclient.shields.register(\n    shield_id="financial_compliance",\n    provider_shield_id="custom-financial-shield",\n    config={\n        "detect_pii": True,\n        "financial_advice_warning": True,\n        "regulatory_compliance": "FINRA"\n    }\n)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"safety-response-handling",children:"Safety Response Handling"}),"\n",(0,a.jsx)(n.p,{children:"When safety violations are detected, handle them appropriately:"}),"\n",(0,a.jsxs)(l.A,{children:[(0,a.jsx)(r.default,{value:"basic-handling",label:"Basic Handling",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'response = client.safety.run_shield(\n    shield_id="content_safety",\n    messages=[{"role": "user", "content": "Potentially harmful content"}]\n)\n\nif response.violation:\n    violation = response.violation\n    print(f"Violation Type: {violation.violation_type}")\n    print(f"User Message: {violation.user_message}")\n    print(f"Metadata: {violation.metadata}")\n    \n    # Log the violation for audit purposes\n    logger.warning(f"Safety violation detected: {violation.violation_type}")\n    \n    # Provide appropriate user feedback\n    return "I can\'t help with that request. Please try asking something else."\n'})})}),(0,a.jsx)(r.default,{value:"advanced-handling",label:"Advanced Handling",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'def handle_safety_response(safety_response, user_message):\n    """Advanced safety response handling with logging and user feedback"""\n    \n    if not safety_response.violation:\n        return {"safe": True, "message": "Content passed safety checks"}\n    \n    violation = safety_response.violation\n    \n    # Log violation details\n    audit_log = {\n        "timestamp": datetime.now().isoformat(),\n        "violation_type": violation.violation_type,\n        "original_message": user_message,\n        "shield_response": violation.user_message,\n        "metadata": violation.metadata\n    }\n    logger.warning(f"Safety violation: {audit_log}")\n    \n    # Determine appropriate response based on violation type\n    if violation.violation_type == "hate_speech":\n        user_feedback = "I can\'t engage with content that contains hate speech. Let\'s keep our conversation respectful."\n    elif violation.violation_type == "violence":\n        user_feedback = "I can\'t provide information that could promote violence. How else can I help you today?"\n    else:\n        user_feedback = "I can\'t help with that request. Please try asking something else."\n    \n    return {\n        "safe": False, \n        "user_feedback": user_feedback,\n        "violation_details": audit_log\n    }\n\n# Usage\nsafety_result = handle_safety_response(response, user_input)\nif not safety_result["safe"]:\n    return safety_result["user_feedback"]\n'})})})]}),"\n",(0,a.jsx)(n.h2,{id:"safety-configuration-best-practices",children:"Safety Configuration Best Practices"}),"\n",(0,a.jsxs)(n.h3,{id:"\ufe0f-multi-layer-protection",children:["\ud83d\udee1\ufe0f ",(0,a.jsx)(n.strong,{children:"Multi-Layer Protection"})]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Use both input and output shields for comprehensive coverage"}),"\n",(0,a.jsx)(n.li,{children:"Combine multiple shield types for different threat categories"}),"\n",(0,a.jsx)(n.li,{children:"Implement fallback mechanisms when shields fail"}),"\n"]}),"\n",(0,a.jsxs)(n.h3,{id:"-monitoring--auditing",children:["\ud83d\udcca ",(0,a.jsx)(n.strong,{children:"Monitoring & Auditing"})]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Log all safety violations for compliance and analysis"}),"\n",(0,a.jsx)(n.li,{children:"Monitor false positive rates to tune shield sensitivity"}),"\n",(0,a.jsx)(n.li,{children:"Track safety metrics across different use cases"}),"\n"]}),"\n",(0,a.jsxs)(n.h3,{id:"\ufe0f-configuration-management",children:["\u2699\ufe0f ",(0,a.jsx)(n.strong,{children:"Configuration Management"})]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Use environment-specific safety configurations"}),"\n",(0,a.jsx)(n.li,{children:"Implement A/B testing for shield effectiveness"}),"\n",(0,a.jsx)(n.li,{children:"Regularly update shield models and policies"}),"\n"]}),"\n",(0,a.jsxs)(n.h3,{id:"-integration-patterns",children:["\ud83d\udd27 ",(0,a.jsx)(n.strong,{children:"Integration Patterns"})]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Integrate shields early in the development process"}),"\n",(0,a.jsx)(n.li,{children:"Test safety measures with adversarial inputs"}),"\n",(0,a.jsx)(n.li,{children:"Provide clear user feedback for violations"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"advanced-safety-scenarios",children:"Advanced Safety Scenarios"}),"\n",(0,a.jsx)(n.h3,{id:"context-aware-safety",children:"Context-Aware Safety"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Safety shields that consider conversation context\nagent = Agent(\n    client,\n    model="meta-llama/Llama-3.2-3B-Instruct",\n    instructions="You are a healthcare assistant",\n    input_shields=["medical_safety"],\n    output_shields=["medical_safety"], \n    # Context helps shields make better decisions\n    safety_context={\n        "domain": "healthcare",\n        "user_type": "patient",\n        "compliance_level": "HIPAA"\n    }\n)\n'})}),"\n",(0,a.jsx)(n.h3,{id:"dynamic-shield-selection",children:"Dynamic Shield Selection"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'def select_shield_for_user(user_profile):\n    """Select appropriate safety shield based on user context"""\n    if user_profile.age < 18:\n        return "child_safety_shield"\n    elif user_profile.context == "enterprise":\n        return "enterprise_compliance_shield" \n    else:\n        return "general_safety_shield"\n\n# Use dynamic shield selection\nshield_id = select_shield_for_user(current_user)\nresponse = client.safety.run_shield(\n    shield_id=shield_id,\n    messages=messages\n)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"compliance-and-regulations",children:"Compliance and Regulations"}),"\n",(0,a.jsx)(n.h3,{id:"industry-specific-safety",children:"Industry-Specific Safety"}),"\n",(0,a.jsxs)(l.A,{children:[(0,a.jsx)(r.default,{value:"healthcare",label:"Healthcare (HIPAA)",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Healthcare-specific safety configuration\nclient.shields.register(\n    shield_id="hipaa_compliance",\n    provider_shield_id="healthcare-safety-shield",\n    config={\n        "detect_phi": True,  # Protected Health Information\n        "medical_advice_warning": True,\n        "regulatory_framework": "HIPAA"\n    }\n)\n'})})}),(0,a.jsx)(r.default,{value:"financial",label:"Financial (FINRA)",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Financial services safety configuration\nclient.shields.register(\n    shield_id="finra_compliance", \n    provider_shield_id="financial-safety-shield",\n    config={\n        "detect_financial_advice": True,\n        "investment_disclaimers": True,\n        "regulatory_framework": "FINRA"\n    }\n)\n'})})}),(0,a.jsx)(r.default,{value:"education",label:"Education (COPPA)",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Educational platform safety for minors\nclient.shields.register(\n    shield_id="coppa_compliance",\n    provider_shield_id="educational-safety-shield", \n    config={\n        "child_protection": True,\n        "educational_content_only": True,\n        "regulatory_framework": "COPPA"\n    }\n)\n'})})})]}),"\n",(0,a.jsx)(n.h2,{id:"related-resources",children:"Related Resources"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.a,{href:"./agent",children:"Agents"})})," - Integrating safety shields with intelligent agents"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.a,{href:"./agent-execution-loop",children:"Agent Execution Loop"})})," - Understanding safety in the execution flow"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.a,{href:"./evals",children:"Evaluations"})})," - Evaluating safety shield effectiveness"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.a,{href:"./telemetry",children:"Telemetry"})})," - Monitoring safety violations and metrics"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.a,{href:"https://github.com/meta-llama/PurpleLlama/tree/main/Llama-Guard3",children:"Llama Guard Documentation"})})," - Advanced safety model details"]}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(u,{...e})}):u(e)}}}]);