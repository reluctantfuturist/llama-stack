"use strict";(self.webpackChunkdocusaurus_template_openapi_docs=self.webpackChunkdocusaurus_template_openapi_docs||[]).push([[4098],{20062:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>a,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"concepts/api-providers","title":"API Providers","description":"Understanding remote vs inline provider implementations","source":"@site/docs/concepts/api-providers.mdx","sourceDirName":"concepts","slug":"/concepts/api-providers","permalink":"/llama-stack/docs/concepts/api-providers","draft":false,"unlisted":false,"editUrl":"https://github.com/meta-llama/llama-stack/tree/main/docs/docs/concepts/api-providers.mdx","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"API Providers","description":"Understanding remote vs inline provider implementations","sidebar_label":"API Providers","sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"APIs","permalink":"/llama-stack/docs/concepts/apis"},"next":{"title":"Distributions","permalink":"/llama-stack/docs/concepts/distributions"}}');var n=r(74848),i=r(28453);const a={title:"API Providers",description:"Understanding remote vs inline provider implementations",sidebar_label:"API Providers",sidebar_position:4},o="API Providers",l={},c=[];function d(e){const t={h1:"h1",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.header,{children:(0,n.jsx)(t.h1,{id:"api-providers",children:"API Providers"})}),"\n",(0,n.jsx)(t.p,{children:"The goal of Llama Stack is to build an ecosystem where users can easily swap out different implementations for the same API. Examples for these include:"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"LLM inference providers (e.g., Fireworks, Together, AWS Bedrock, Groq, Cerebras, SambaNova, vLLM, etc.),"}),"\n",(0,n.jsx)(t.li,{children:"Vector databases (e.g., ChromaDB, Weaviate, Qdrant, Milvus, FAISS, PGVector, etc.),"}),"\n",(0,n.jsx)(t.li,{children:"Safety providers (e.g., Meta's Llama Guard, AWS Bedrock Guardrails, etc.)"}),"\n"]}),"\n",(0,n.jsx)(t.p,{children:"Providers come in two flavors:"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:"Remote"}),": the provider runs as a separate service external to the Llama Stack codebase. Llama Stack contains a small amount of adapter code."]}),"\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:"Inline"}),": the provider is fully specified and implemented within the Llama Stack codebase. It may be a simple wrapper around an existing library, or a full fledged implementation within Llama Stack."]}),"\n"]}),"\n",(0,n.jsx)(t.p,{children:"Most importantly, Llama Stack always strives to provide at least one fully inline provider for each API so you can iterate on a fully featured environment locally."})]})}function p(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(d,{...e})}):d(e)}},28453:(e,t,r)=>{r.d(t,{R:()=>a,x:()=>o});var s=r(96540);const n={},i=s.createContext(n);function a(e){const t=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:a(e.components),s.createElement(i.Provider,{value:t},e.children)}}}]);