"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[3142],{64212:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>o});const s=JSON.parse('{"id":"distributions/self-hosted-distro/dell","title":"Dell Distribution","description":"Dell\'s distribution of Llama Stack using custom TGI containers via Dell Enterprise Hub","source":"@site/docs/distributions/self-hosted-distro/dell.mdx","sourceDirName":"distributions/self-hosted-distro","slug":"/distributions/self-hosted-distro/dell","permalink":"/llama-stack/docs/distributions/self-hosted-distro/dell","draft":false,"unlisted":false,"editUrl":"https://github.com/meta-llama/llama-stack/tree/main/docs/docs/distributions/self-hosted-distro/dell.mdx","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Dell Distribution","description":"Dell\'s distribution of Llama Stack using custom TGI containers via Dell Enterprise Hub","sidebar_label":"Dell","sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Starter","permalink":"/llama-stack/docs/distributions/self-hosted-distro/starter"},"next":{"title":"Dell-TGI","permalink":"/llama-stack/docs/distributions/self-hosted-distro/dell-tgi"}}');var l=i(74848),t=i(28453);const r={title:"Dell Distribution",description:"Dell's distribution of Llama Stack using custom TGI containers via Dell Enterprise Hub",sidebar_label:"Dell",sidebar_position:2},a="Dell Distribution of Llama Stack",d={},o=[{value:"Provider Configuration",id:"provider-configuration",level:2},{value:"Environment Variables",id:"environment-variables",level:2},{value:"Setting up Inference Server",id:"setting-up-inference-server",level:2},{value:"Dell Enterprise Hub&#39;s Custom TGI Container",id:"dell-enterprise-hubs-custom-tgi-container",level:3},{value:"Safety Model Setup (Optional)",id:"safety-model-setup-optional",level:3},{value:"ChromaDB Setup",id:"chromadb-setup",level:3},{value:"Running Llama Stack",id:"running-llama-stack",level:2},{value:"Via Docker",id:"via-docker",level:3},{value:"Basic Setup",id:"basic-setup",level:4},{value:"With Safety/Shield APIs",id:"with-safetyshield-apis",level:4},{value:"Via venv",id:"via-venv",level:3},{value:"Basic Setup",id:"basic-setup-1",level:4},{value:"With Safety/Shield APIs",id:"with-safetyshield-apis-1",level:4},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Use Cases",id:"use-cases",level:2},{value:"Related Guides",id:"related-guides",level:2}];function c(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(n.header,{children:(0,l.jsx)(n.h1,{id:"dell-distribution-of-llama-stack",children:"Dell Distribution of Llama Stack"})}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.code,{children:"llamastack/distribution-dell"})," distribution consists of the following provider configurations."]}),"\n",(0,l.jsx)(n.h2,{id:"provider-configuration",children:"Provider Configuration"}),"\n",(0,l.jsxs)(n.table,{children:[(0,l.jsx)(n.thead,{children:(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.th,{children:"API"}),(0,l.jsx)(n.th,{children:"Provider(s)"})]})}),(0,l.jsxs)(n.tbody,{children:[(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"agents"}),(0,l.jsx)(n.td,{children:(0,l.jsx)(n.code,{children:"inline::meta-reference"})})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"datasetio"}),(0,l.jsxs)(n.td,{children:[(0,l.jsx)(n.code,{children:"remote::huggingface"}),", ",(0,l.jsx)(n.code,{children:"inline::localfs"})]})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"eval"}),(0,l.jsx)(n.td,{children:(0,l.jsx)(n.code,{children:"inline::meta-reference"})})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"inference"}),(0,l.jsxs)(n.td,{children:[(0,l.jsx)(n.code,{children:"remote::tgi"}),", ",(0,l.jsx)(n.code,{children:"inline::sentence-transformers"})]})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"safety"}),(0,l.jsx)(n.td,{children:(0,l.jsx)(n.code,{children:"inline::llama-guard"})})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"scoring"}),(0,l.jsxs)(n.td,{children:[(0,l.jsx)(n.code,{children:"inline::basic"}),", ",(0,l.jsx)(n.code,{children:"inline::llm-as-judge"}),", ",(0,l.jsx)(n.code,{children:"inline::braintrust"})]})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"telemetry"}),(0,l.jsx)(n.td,{children:(0,l.jsx)(n.code,{children:"inline::meta-reference"})})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"tool_runtime"}),(0,l.jsxs)(n.td,{children:[(0,l.jsx)(n.code,{children:"remote::brave-search"}),", ",(0,l.jsx)(n.code,{children:"remote::tavily-search"}),", ",(0,l.jsx)(n.code,{children:"inline::rag-runtime"})]})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"vector_io"}),(0,l.jsxs)(n.td,{children:[(0,l.jsx)(n.code,{children:"inline::faiss"}),", ",(0,l.jsx)(n.code,{children:"remote::chromadb"}),", ",(0,l.jsx)(n.code,{children:"remote::pgvector"})]})]})]})]}),"\n",(0,l.jsx)(n.p,{children:"You can use this distribution if you have GPUs and want to run an independent TGI or Dell Enterprise Hub container for running inference."}),"\n",(0,l.jsx)(n.h2,{id:"environment-variables",children:"Environment Variables"}),"\n",(0,l.jsx)(n.p,{children:"The following environment variables can be configured:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"DEH_URL"}),": URL for the Dell inference server (default: ",(0,l.jsx)(n.code,{children:"http://0.0.0.0:8181"}),")"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"DEH_SAFETY_URL"}),": URL for the Dell safety inference server (default: ",(0,l.jsx)(n.code,{children:"http://0.0.0.0:8282"}),")"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"CHROMA_URL"}),": URL for the Chroma server (default: ",(0,l.jsx)(n.code,{children:"http://localhost:6601"}),")"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"INFERENCE_MODEL"}),": Inference model loaded into the TGI server (default: ",(0,l.jsx)(n.code,{children:"meta-llama/Llama-3.2-3B-Instruct"}),")"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"SAFETY_MODEL"}),": Name of the safety (Llama-Guard) model to use (default: ",(0,l.jsx)(n.code,{children:"meta-llama/Llama-Guard-3-1B"}),")"]}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"setting-up-inference-server",children:"Setting up Inference Server"}),"\n",(0,l.jsx)(n.h3,{id:"dell-enterprise-hubs-custom-tgi-container",children:"Dell Enterprise Hub's Custom TGI Container"}),"\n",(0,l.jsx)(n.admonition,{title:"Development Status",type:"note",children:(0,l.jsxs)(n.p,{children:["This is a placeholder to run inference with TGI. This will be updated to use ",(0,l.jsx)(n.a,{href:"https://dell.huggingface.co/authenticated/models",children:"Dell Enterprise Hub's containers"})," once verified."]})}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"export INFERENCE_PORT=8181\nexport DEH_URL=http://0.0.0.0:$INFERENCE_PORT\nexport INFERENCE_MODEL=meta-llama/Llama-3.1-8B-Instruct\nexport CHROMADB_HOST=localhost\nexport CHROMADB_PORT=6601\nexport CHROMA_URL=http://$CHROMADB_HOST:$CHROMADB_PORT\nexport CUDA_VISIBLE_DEVICES=0\nexport LLAMA_STACK_PORT=8321\n\ndocker run --rm -it \\\n  --pull always \\\n  --network host \\\n  -v $HOME/.cache/huggingface:/data \\\n  -e HF_TOKEN=$HF_TOKEN \\\n  -p $INFERENCE_PORT:$INFERENCE_PORT \\\n  --gpus $CUDA_VISIBLE_DEVICES \\\n  ghcr.io/huggingface/text-generation-inference \\\n  --dtype bfloat16 \\\n  --usage-stats off \\\n  --sharded false \\\n  --cuda-memory-fraction 0.7 \\\n  --model-id $INFERENCE_MODEL \\\n  --port $INFERENCE_PORT --hostname 0.0.0.0\n"})}),"\n",(0,l.jsx)(n.h3,{id:"safety-model-setup-optional",children:"Safety Model Setup (Optional)"}),"\n",(0,l.jsxs)(n.p,{children:["If you are using Llama Stack Safety / Shield APIs, then you will need to also run another instance of a TGI with a corresponding safety model like ",(0,l.jsx)(n.code,{children:"meta-llama/Llama-Guard-3-1B"}),":"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"export SAFETY_INFERENCE_PORT=8282\nexport DEH_SAFETY_URL=http://0.0.0.0:$SAFETY_INFERENCE_PORT\nexport SAFETY_MODEL=meta-llama/Llama-Guard-3-1B\nexport CUDA_VISIBLE_DEVICES=1\n\ndocker run --rm -it \\\n  --pull always \\\n  --network host \\\n  -v $HOME/.cache/huggingface:/data \\\n  -e HF_TOKEN=$HF_TOKEN \\\n  -p $SAFETY_INFERENCE_PORT:$SAFETY_INFERENCE_PORT \\\n  --gpus $CUDA_VISIBLE_DEVICES \\\n  ghcr.io/huggingface/text-generation-inference \\\n  --dtype bfloat16 \\\n  --usage-stats off \\\n  --sharded false \\\n  --cuda-memory-fraction 0.7 \\\n  --model-id $SAFETY_MODEL \\\n  --hostname 0.0.0.0 \\\n  --port $SAFETY_INFERENCE_PORT\n"})}),"\n",(0,l.jsx)(n.h3,{id:"chromadb-setup",children:"ChromaDB Setup"}),"\n",(0,l.jsx)(n.p,{children:"Dell distribution relies on ChromaDB for vector database usage. You can start a ChromaDB easily using Docker:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"# This is where the indices are persisted\nmkdir -p $HOME/chromadb\n\npodman run --rm -it \\\n  --network host \\\n  --name chromadb \\\n  -v $HOME/chromadb:/chroma/chroma \\\n  -e IS_PERSISTENT=TRUE \\\n  chromadb/chroma:latest \\\n  --port $CHROMADB_PORT \\\n  --host $CHROMADB_HOST\n"})}),"\n",(0,l.jsx)(n.h2,{id:"running-llama-stack",children:"Running Llama Stack"}),"\n",(0,l.jsx)(n.p,{children:"Now you are ready to run Llama Stack with TGI as the inference provider. You can do this via venv or Docker which has a pre-built image."}),"\n",(0,l.jsx)(n.h3,{id:"via-docker",children:"Via Docker"}),"\n",(0,l.jsx)(n.p,{children:"This method allows you to get started quickly without having to build the distribution code."}),"\n",(0,l.jsx)(n.h4,{id:"basic-setup",children:"Basic Setup"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"docker run -it \\\n  --pull always \\\n  --network host \\\n  -p $LLAMA_STACK_PORT:$LLAMA_STACK_PORT \\\n  -v $HOME/.llama:/root/.llama \\\n  # NOTE: mount the llama-stack / llama-model directories if testing local changes else not needed\n  -v /home/hjshah/git/llama-stack:/app/llama-stack-source -v /home/hjshah/git/llama-models:/app/llama-models-source \\\n  # localhost/distribution-dell:dev if building / testing locally\n  llamastack/distribution-dell\\\n  --port $LLAMA_STACK_PORT  \\\n  --env INFERENCE_MODEL=$INFERENCE_MODEL \\\n  --env DEH_URL=$DEH_URL \\\n  --env CHROMA_URL=$CHROMA_URL\n"})}),"\n",(0,l.jsx)(n.h4,{id:"with-safetyshield-apis",children:"With Safety/Shield APIs"}),"\n",(0,l.jsx)(n.p,{children:"If you are using Llama Stack Safety / Shield APIs:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"# You need a local checkout of llama-stack to run this, get it using\n# git clone https://github.com/meta-llama/llama-stack.git\ncd /path/to/llama-stack\n\nexport SAFETY_INFERENCE_PORT=8282\nexport DEH_SAFETY_URL=http://0.0.0.0:$SAFETY_INFERENCE_PORT\nexport SAFETY_MODEL=meta-llama/Llama-Guard-3-1B\n\ndocker run \\\n  -it \\\n  --pull always \\\n  -p $LLAMA_STACK_PORT:$LLAMA_STACK_PORT \\\n  -v $HOME/.llama:/root/.llama \\\n  -v ./llama_stack/distributions/tgi/run-with-safety.yaml:/root/my-run.yaml \\\n  llamastack/distribution-dell \\\n  --config /root/my-run.yaml \\\n  --port $LLAMA_STACK_PORT \\\n  --env INFERENCE_MODEL=$INFERENCE_MODEL \\\n  --env DEH_URL=$DEH_URL \\\n  --env SAFETY_MODEL=$SAFETY_MODEL \\\n  --env DEH_SAFETY_URL=$DEH_SAFETY_URL \\\n  --env CHROMA_URL=$CHROMA_URL\n"})}),"\n",(0,l.jsx)(n.h3,{id:"via-venv",children:"Via venv"}),"\n",(0,l.jsxs)(n.p,{children:["Make sure you have done ",(0,l.jsx)(n.code,{children:"pip install llama-stack"})," and have the Llama Stack CLI available."]}),"\n",(0,l.jsx)(n.h4,{id:"basic-setup-1",children:"Basic Setup"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"llama stack build --distro dell --image-type venv\nllama stack run dell\n  --port $LLAMA_STACK_PORT \\\n  --env INFERENCE_MODEL=$INFERENCE_MODEL \\\n  --env DEH_URL=$DEH_URL \\\n  --env CHROMA_URL=$CHROMA_URL\n"})}),"\n",(0,l.jsx)(n.h4,{id:"with-safetyshield-apis-1",children:"With Safety/Shield APIs"}),"\n",(0,l.jsx)(n.p,{children:"If you are using Llama Stack Safety / Shield APIs:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"llama stack run ./run-with-safety.yaml \\\n  --port $LLAMA_STACK_PORT \\\n  --env INFERENCE_MODEL=$INFERENCE_MODEL \\\n  --env DEH_URL=$DEH_URL \\\n  --env SAFETY_MODEL=$SAFETY_MODEL \\\n  --env DEH_SAFETY_URL=$DEH_SAFETY_URL \\\n  --env CHROMA_URL=$CHROMA_URL\n"})}),"\n",(0,l.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Hardware"}),": GPU access required for TGI containers"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Software"}),": Docker with GPU support, Podman (optional)"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Models"}),": Access to Hugging Face models and Dell Enterprise Hub containers"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Storage"}),": Sufficient disk space for model caches and ChromaDB indices"]}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"use-cases",children:"Use Cases"}),"\n",(0,l.jsx)(n.p,{children:"The Dell distribution is ideal for:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Enterprise deployments"})," with Dell infrastructure"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Custom TGI containers"})," via Dell Enterprise Hub"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Local GPU inference"})," with high performance requirements"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Vector database integration"})," with ChromaDB"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Safety-enabled"})," applications with Llama Guard"]}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"related-guides",children:"Related Guides"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:(0,l.jsx)(n.a,{href:"./dell-tgi",children:"Dell-TGI Distribution"})})," - Dell's TGI-specific distribution"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:(0,l.jsx)(n.a,{href:"../configuration",children:"Configuration Reference"})})," - Understanding configuration options"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:(0,l.jsx)(n.a,{href:"../building-distro",children:"Building Custom Distributions"})})," - Create your own distribution"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,l.jsx)(n,{...e,children:(0,l.jsx)(c,{...e})}):c(e)}}}]);