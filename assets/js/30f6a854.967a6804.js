"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[6598],{4865:(e,n,i)=>{i.d(n,{A:()=>m});var t=i(96540),s=i(34164),l=i(23104),a=i(47751),r=i(92303);const o={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var c=i(74848);function d(e){var n=e.className,i=e.block,t=e.selectedValue,a=e.selectValue,r=e.tabValues,d=[],p=(0,l.a_)().blockElementScrollPositionUntilNextRender,h=function(e){var n=e.currentTarget,i=d.indexOf(n),s=r[i].value;s!==t&&(p(n),a(s))},m=function(e){var n,i=null;switch(e.key){case"Enter":h(e);break;case"ArrowRight":var t,s=d.indexOf(e.currentTarget)+1;i=null!=(t=d[s])?t:d[0];break;case"ArrowLeft":var l,a=d.indexOf(e.currentTarget)-1;i=null!=(l=d[a])?l:d[d.length-1]}null==(n=i)||n.focus()};return(0,c.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,s.A)("tabs",{"tabs--block":i},n),children:r.map(function(e){var n=e.value,i=e.label,l=e.attributes;return(0,c.jsx)("li",Object.assign({role:"tab",tabIndex:t===n?0:-1,"aria-selected":t===n,ref:function(e){d.push(e)},onKeyDown:m,onClick:h},l,{className:(0,s.A)("tabs__item",o.tabItem,null==l?void 0:l.className,{"tabs__item--active":t===n}),children:null!=i?i:n}),n)})})}function p(e){var n=e.lazy,i=e.children,l=e.selectedValue,a=(Array.isArray(i)?i:[i]).filter(Boolean);if(n){var r=a.find(function(e){return e.props.value===l});return r?(0,t.cloneElement)(r,{className:(0,s.A)("margin-top--md",r.props.className)}):null}return(0,c.jsx)("div",{className:"margin-top--md",children:a.map(function(e,n){return(0,t.cloneElement)(e,{key:n,hidden:e.props.value!==l})})})}function h(e){var n=(0,a.u)(e);return(0,c.jsxs)("div",{className:(0,s.A)("tabs-container",o.tabList),children:[(0,c.jsx)(d,Object.assign({},n,e)),(0,c.jsx)(p,Object.assign({},n,e))]})}function m(e){var n=(0,r.default)();return(0,c.jsx)(h,Object.assign({},e,{children:(0,a.v)(e.children)}),String(n))}},18227:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>c,default:()=>m,frontMatter:()=>o,metadata:()=>t,toc:()=>p});const t=JSON.parse('{"id":"providers/openai-compatibility","title":"OpenAI API Compatibility","description":"Use OpenAI clients and libraries with Llama Stack for seamless integration","source":"@site/docs/providers/openai-compatibility.mdx","sourceDirName":"providers","slug":"/providers/openai-compatibility","permalink":"/llama-stack/docs/providers/openai-compatibility","draft":false,"unlisted":false,"editUrl":"https://github.com/meta-llama/llama-stack/tree/main/docs/docs/providers/openai-compatibility.mdx","tags":[],"version":"current","sidebarPosition":11,"frontMatter":{"title":"OpenAI API Compatibility","description":"Use OpenAI clients and libraries with Llama Stack for seamless integration","sidebar_label":"OpenAI Compatibility","sidebar_position":11},"sidebar":"tutorialSidebar","previous":{"title":"Community Providers","permalink":"/llama-stack/docs/providers/external/external-providers-list"},"next":{"title":"Overview","permalink":"/llama-stack/docs/building-applications/"}}');var s=i(74848),l=i(28453),a=i(4865),r=i(19365);const o={title:"OpenAI API Compatibility",description:"Use OpenAI clients and libraries with Llama Stack for seamless integration",sidebar_label:"OpenAI Compatibility",sidebar_position:11},c="OpenAI API Compatibility",d={},p=[{value:"Server Path",id:"server-path",level:2},{value:"Client Configuration",id:"client-configuration",level:2},{value:"Available APIs",id:"available-apis",level:2},{value:"Models",id:"models",level:3},{value:"Responses",id:"responses",level:3},{value:"Simple Inference",id:"simple-inference",level:4},{value:"Structured Output",id:"structured-output",level:4},{value:"Chat Completions",id:"chat-completions",level:3},{value:"Simple Inference",id:"simple-inference-1",level:4},{value:"Structured Output",id:"structured-output-1",level:4},{value:"Streaming Responses",id:"streaming-responses",level:4},{value:"Completions",id:"completions",level:3},{value:"Simple Inference",id:"simple-inference-2",level:4},{value:"Migration from OpenAI",id:"migration-from-openai",level:2},{value:"Quick Migration Steps",id:"quick-migration-steps",level:3},{value:"Migration Example",id:"migration-example",level:3},{value:"Benefits",id:"benefits",level:2},{value:"\ud83d\udd04 <strong>Seamless Integration</strong>",id:"-seamless-integration",level:3},{value:"\ud83c\udfe0 <strong>Local Control</strong>",id:"-local-control",level:3},{value:"\ud83d\udcda <strong>Library Compatibility</strong>",id:"-library-compatibility",level:3},{value:"\ud83d\udd27 <strong>Easy Migration</strong>",id:"-easy-migration",level:3},{value:"Limitations",id:"limitations",level:2},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues",id:"common-issues",level:3},{value:"Next Steps",id:"next-steps",level:2},{value:"Related Resources",id:"related-resources",level:2}];function h(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,l.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"openai-api-compatibility",children:"OpenAI API Compatibility"})}),"\n",(0,s.jsx)(n.p,{children:"Llama Stack provides OpenAI-compatible API endpoints, allowing you to use existing OpenAI clients and libraries with Llama Stack servers."}),"\n",(0,s.jsx)(n.h2,{id:"server-path",children:"Server Path"}),"\n",(0,s.jsxs)(n.p,{children:["Llama Stack exposes an OpenAI-compatible API endpoint at ",(0,s.jsx)(n.code,{children:"/v1/openai/v1"}),"."]}),"\n",(0,s.jsxs)(n.p,{children:["For a Llama Stack server running locally on port ",(0,s.jsx)(n.code,{children:"8321"}),", the full URL to the OpenAI-compatible API endpoint is:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"http://localhost:8321/v1/openai/v1\n"})}),"\n",(0,s.jsx)(n.h2,{id:"client-configuration",children:"Client Configuration"}),"\n",(0,s.jsx)(n.p,{children:"You can use any client that speaks OpenAI APIs with Llama Stack. We regularly test with the official Llama Stack clients as well as OpenAI's official Python client."}),"\n",(0,s.jsxs)(a.A,{children:[(0,s.jsxs)(r.default,{value:"llamastack",label:"Llama Stack Client",children:[(0,s.jsxs)(n.p,{children:["When using the Llama Stack client, set the ",(0,s.jsx)(n.code,{children:"base_url"})," to the root of your Llama Stack server. It will automatically route OpenAI-compatible requests to the right server endpoint for you."]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from llama_stack_client import LlamaStackClient\n\nclient = LlamaStackClient(base_url="http://localhost:8321")\n'})})]}),(0,s.jsxs)(r.default,{value:"openai",label:"OpenAI Client",children:[(0,s.jsxs)(n.p,{children:["When using an OpenAI client, set the ",(0,s.jsx)(n.code,{children:"base_url"})," to the ",(0,s.jsx)(n.code,{children:"/v1/openai/v1"})," path on your Llama Stack server."]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from openai import OpenAI\n\nclient = OpenAI(\n    base_url="http://localhost:8321/v1/openai/v1", \n    api_key="none"  # API key not required for local Llama Stack\n)\n'})})]})]}),"\n",(0,s.jsx)(n.p,{children:"Regardless of the client you choose, the following code examples should all work the same."}),"\n",(0,s.jsx)(n.h2,{id:"available-apis",children:"Available APIs"}),"\n",(0,s.jsx)(n.h3,{id:"models",children:"Models"}),"\n",(0,s.jsx)(n.p,{children:"Many of the APIs require you to pass in a model parameter. To see the list of models available in your Llama Stack server:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'models = client.models.list()\nfor model in models:\n    print(f"Model ID: {model.id}")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"responses",children:"Responses"}),"\n",(0,s.jsx)(n.admonition,{title:"Development Status",type:"info",children:(0,s.jsxs)(n.p,{children:["The Responses API implementation is still in active development. While it is quite usable, there are still unimplemented parts of the API. We'd love feedback on any use-cases you try that do not work to help prioritize the pieces left to implement. Please open issues in the ",(0,s.jsx)(n.a,{href:"https://github.com/meta-llama/llama-stack",children:"meta-llama/llama-stack"})," GitHub repository with details of anything that does not work."]})}),"\n",(0,s.jsx)(n.h4,{id:"simple-inference",children:"Simple Inference"}),"\n",(0,s.jsx)(n.p,{children:"Request:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'response = client.responses.create(\n    model="meta-llama/Llama-3.2-3B-Instruct",\n    input="Write a haiku about coding."\n)\n\nprint(response.output_text)\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Example output:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"Pixels dancing slow\nSyntax whispers secrets sweet\nCode's gentle silence\n"})}),"\n",(0,s.jsx)(n.h4,{id:"structured-output",children:"Structured Output"}),"\n",(0,s.jsx)(n.p,{children:"Request:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'response = client.responses.create(\n    model="meta-llama/Llama-3.2-3B-Instruct",\n    input=[\n        {\n            "role": "system",\n            "content": "Extract the participants from the event information.",\n        },\n        {\n            "role": "user",\n            "content": "Alice and Bob are going to a science fair on Friday.",\n        },\n    ],\n    text={\n        "format": {\n            "type": "json_schema",\n            "name": "participants",\n            "schema": {\n                "type": "object",\n                "properties": {\n                    "participants": {"type": "array", "items": {"type": "string"}}\n                },\n                "required": ["participants"],\n            },\n        }\n    },\n)\nprint(response.output_text)\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Example output:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json",children:'{ "participants": ["Alice", "Bob"] }\n'})}),"\n",(0,s.jsx)(n.h3,{id:"chat-completions",children:"Chat Completions"}),"\n",(0,s.jsx)(n.h4,{id:"simple-inference-1",children:"Simple Inference"}),"\n",(0,s.jsx)(n.p,{children:"Request:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'chat_completion = client.chat.completions.create(\n    model="meta-llama/Llama-3.2-3B-Instruct",\n    messages=[{"role": "user", "content": "Write a haiku about coding."}],\n)\n\nprint(chat_completion.choices[0].message.content)\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Example output:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"Lines of code unfold\nLogic flows like a river\nCode's gentle beauty\n"})}),"\n",(0,s.jsx)(n.h4,{id:"structured-output-1",children:"Structured Output"}),"\n",(0,s.jsx)(n.p,{children:"Request:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'chat_completion = client.chat.completions.create(\n    model="meta-llama/Llama-3.2-3B-Instruct",\n    messages=[\n        {\n            "role": "system",\n            "content": "Extract the participants from the event information.",\n        },\n        {\n            "role": "user",\n            "content": "Alice and Bob are going to a science fair on Friday.",\n        },\n    ],\n    response_format={\n        "type": "json_schema",  \n        "json_schema": {\n            "name": "participants",\n            "schema": {\n                "type": "object",\n                "properties": {\n                    "participants": {"type": "array", "items": {"type": "string"}}\n                },\n                "required": ["participants"],\n            },\n        },\n    },\n)\n\nprint(chat_completion.choices[0].message.content)\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Example output:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json",children:'{ "participants": ["Alice", "Bob"] }\n'})}),"\n",(0,s.jsx)(n.h4,{id:"streaming-responses",children:"Streaming Responses"}),"\n",(0,s.jsx)(n.p,{children:"Request:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'stream = client.chat.completions.create(\n    model="meta-llama/Llama-3.2-3B-Instruct",\n    messages=[{"role": "user", "content": "Count from 1 to 10."}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content is not None:\n        print(chunk.choices[0].delta.content, end="")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"completions",children:"Completions"}),"\n",(0,s.jsx)(n.h4,{id:"simple-inference-2",children:"Simple Inference"}),"\n",(0,s.jsx)(n.p,{children:"Request:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'completion = client.completions.create(\n    model="meta-llama/Llama-3.2-3B-Instruct", \n    prompt="Write a haiku about coding."\n)\n\nprint(completion.choices[0].text)\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Example output:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"Lines of code unfurl\nLogic whispers in the dark\nArt in hidden form\n"})}),"\n",(0,s.jsx)(n.h2,{id:"migration-from-openai",children:"Migration from OpenAI"}),"\n",(0,s.jsx)(n.h3,{id:"quick-migration-steps",children:"Quick Migration Steps"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Update base URL"}),": Change from OpenAI's API endpoint to your Llama Stack server"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Remove API key requirement"}),": Local Llama Stack doesn't require authentication by default"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Update model names"}),": Use Llama Stack model identifiers"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Test functionality"}),": Verify that your existing code works as expected"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"migration-example",children:"Migration Example"}),"\n",(0,s.jsxs)(a.A,{children:[(0,s.jsx)(r.default,{value:"before",label:"Before (OpenAI)",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from openai import OpenAI\n\nclient = OpenAI(api_key="your-openai-api-key")\n\nresponse = client.chat.completions.create(\n    model="gpt-4",\n    messages=[{"role": "user", "content": "Hello!"}]\n)\n'})})}),(0,s.jsx)(r.default,{value:"after",label:"After (Llama Stack)",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from openai import OpenAI\n\nclient = OpenAI(\n    base_url="http://localhost:8321/v1/openai/v1",\n    api_key="none"  # Not required for local instance\n)\n\nresponse = client.chat.completions.create(\n    model="meta-llama/Llama-3.2-3B-Instruct",  # Use Llama model\n    messages=[{"role": "user", "content": "Hello!"}]\n)\n'})})})]}),"\n",(0,s.jsx)(n.h2,{id:"benefits",children:"Benefits"}),"\n",(0,s.jsxs)(n.h3,{id:"-seamless-integration",children:["\ud83d\udd04 ",(0,s.jsx)(n.strong,{children:"Seamless Integration"})]}),"\n",(0,s.jsx)(n.p,{children:"Use existing OpenAI-compatible code with minimal changes"}),"\n",(0,s.jsxs)(n.h3,{id:"-local-control",children:["\ud83c\udfe0 ",(0,s.jsx)(n.strong,{children:"Local Control"})]}),"\n",(0,s.jsx)(n.p,{children:"Run models locally while keeping familiar API patterns"}),"\n",(0,s.jsxs)(n.h3,{id:"-library-compatibility",children:["\ud83d\udcda ",(0,s.jsx)(n.strong,{children:"Library Compatibility"})]}),"\n",(0,s.jsx)(n.p,{children:"Works with existing tools and libraries built for OpenAI APIs"}),"\n",(0,s.jsxs)(n.h3,{id:"-easy-migration",children:["\ud83d\udd27 ",(0,s.jsx)(n.strong,{children:"Easy Migration"})]}),"\n",(0,s.jsx)(n.p,{children:"Gradually migrate from OpenAI to Llama Stack without rewriting applications"}),"\n",(0,s.jsx)(n.h2,{id:"limitations",children:"Limitations"}),"\n",(0,s.jsx)(n.p,{children:"While Llama Stack strives for full OpenAI compatibility, some features may have differences:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Model-specific capabilities"}),": Different models may support different features"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Response format variations"}),": Some response fields may differ slightly"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Rate limiting"}),": Different rate limiting behavior compared to OpenAI"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Authentication"}),": Local instances may not require authentication"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,s.jsx)(n.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Connection Errors:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Verify the Llama Stack server is running"}),"\n",(0,s.jsxs)(n.li,{children:["Check the base URL includes the correct path (",(0,s.jsx)(n.code,{children:"/v1/openai/v1"}),")"]}),"\n",(0,s.jsx)(n.li,{children:"Ensure the port number is correct"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Model Not Found:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Use ",(0,s.jsx)(n.code,{children:"client.models.list()"})," to see available models"]}),"\n",(0,s.jsx)(n.li,{children:"Verify the model is loaded in your Llama Stack configuration"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"API Differences:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Check the Llama Stack logs for detailed error information"}),"\n",(0,s.jsxs)(n.li,{children:["Refer to the ",(0,s.jsx)(n.a,{href:"/docs/api/",children:"API documentation"})," for Llama Stack-specific details"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsxs)(n.strong,{children:["\ud83d\ude80 ",(0,s.jsx)(n.a,{href:"/docs/distributions/",children:"Set up a Llama Stack server"})]})," if you haven't already"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsxs)(n.strong,{children:["\ud83d\udcd6 ",(0,s.jsx)(n.a,{href:"/docs/api/",children:"Explore the full API reference"})]})," for advanced features"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsxs)(n.strong,{children:["\ud83d\udd27 ",(0,s.jsx)(n.a,{href:"/docs/distributions/configuration",children:"Configure your models"})]})," for optimal performance"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsxs)(n.strong,{children:["\ud83d\udcac ",(0,s.jsx)(n.a,{href:"https://github.com/meta-llama/llama-stack/discussions",children:"Join the community"})]})," for support and tips"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"related-resources",children:"Related Resources"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.a,{href:"/docs/getting-started/libraries",children:"Llama Stack Client"})})," - Official Python client"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.a,{href:"/docs/providers/",children:"Provider Documentation"})})," - Understanding the provider system"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.a,{href:"/docs/distributions/configuration",children:"Configuration Guide"})})," - Server configuration options"]}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}}}]);