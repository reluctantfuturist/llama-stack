---
sidebar_position: 1
tags:
  - test
---

# Welcome to Llama Stack

Llama Stack is the open-source framework for building generative AI applications.

:::tip Llama 4 is here!

Check out [Getting Started with Llama 4](https://colab.research.google.com/github/meta-llama/llama-stack/blob/main/docs/getting_started_llama4.ipynb)

:::

## What is Llama Stack?

Llama Stack defines and standardizes the core building blocks needed to bring generative AI applications to market. It provides a unified set of APIs with implementations from leading service providers, enabling seamless transitions between development and production environments. More specifically, it provides:

- **Unified API layer** for Inference, RAG, Agents, Tools, Safety, Evals, and Telemetry.
- **Plugin architecture** to support the rich ecosystem of implementations of the different APIs in different environments like local development, on-premises, cloud, and mobile.
- **Prepackaged verified distributions** which offer a one-stop solution for developers to get started quickly and reliably in any environment
- **Multiple developer interfaces** like CLI and SDKs for Python, Node, iOS, and Android
- **Standalone applications** as examples for how to build production-grade AI applications with Llama Stack

Our goal is to provide pre-packaged implementations (aka "distributions") which can be run in a variety of deployment environments. LlamaStack can assist you in your entire app development lifecycle - start iterating on local, mobile or desktop and seamlessly transition to on-prem or public cloud deployments. At every point in this transition, the same set of APIs and the same developer experience is available.

## How does Llama Stack work?

Llama Stack consists of a server (with multiple pluggable API providers) and Client SDKs meant to be used in your applications. The server can be run in a variety of environments, including local (inline) development, on-premises, and cloud. The client SDKs are available for Python, Swift, Node, and Kotlin.

## Quick Links

- Ready to build? Check out the [Getting Started](/docs/getting-started) guide.
- Explore the [API Reference](/docs/category/llama-stack-api) for detailed endpoint documentation.
- Learn about core [Concepts](/docs/concepts) and architecture.

## Supported Implementations

A number of "adapters" are available for popular Inference and Vector Store providers. For other APIs (particularly Safety and Agents), we provide *reference implementations* you can use to get started.

### Inference API Providers
- Meta Reference, Ollama, Fireworks, Together
- NVIDIA NIM, vLLM, TGI, AWS Bedrock
- Cerebras, Groq, SambaNova, OpenAI
- Anthropic, Gemini, WatsonX
- PyTorch ExecuTorch (On-device iOS, Android)

### Vector IO API Providers
- FAISS, SQLite-Vec, Chroma, Milvus
- Postgres (PGVector), Weaviate, Qdrant

### Additional APIs
- **Agents**: Meta Reference, Fireworks, Together, PyTorch ExecuTorch
- **Safety**: Llama Guard, Prompt Guard, Code Scanner, AWS Bedrock
- **Post Training**: Meta Reference, HuggingFace, TorchTune, NVIDIA NEMO
- **Eval**: Meta Reference, NVIDIA NEMO
- **Telemetry**: Meta Reference
- **Tool Runtime**: Brave Search, RAG Runtime
